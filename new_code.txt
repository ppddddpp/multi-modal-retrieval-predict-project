src\data_run.py
from pathlib import Path
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
from Evaluate import dataPhraseCheck
from Evaluate import edaLabeledCheck
from DataHandler import label2CSV, run_gemini_label_verifier, get_final_ouput_data, train_val_test_split
from Evaluate import get_eda_before_split, get_eda_after_split

try:
    BASE_DIR = Path(__file__).resolve().parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent
XML_DIR    = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
MODEL_PLACE = BASE_DIR / "models"
EDA_DIR = BASE_DIR / 'eda_data'
CHECK_RUN_DIR = BASE_DIR / "check_run"
OUTPUT_DIR = BASE_DIR / 'outputs'
SPLIT_DIR = BASE_DIR / 'splited_data'
EDA_DIR.mkdir(parents=True, exist_ok=True)
CHECK_RUN_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
SPLIT_DIR.mkdir(parents=True, exist_ok=True)

if __name__ == "__main__":
    combined_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
    }
    dataPhraseCheck(xml_path=XML_DIR, dicom_path=DICOM_ROOT, 
                    model_path=MODEL_PLACE, check_run_dir=CHECK_RUN_DIR, 
                    combined_groups=combined_groups)
    
    edaLabeledCheck(xml_dir=XML_DIR, 
                    save_dir=EDA_DIR)
    
    label2CSV(xml_dir=XML_DIR, 
                dicom_dir=DICOM_ROOT, 
                out_path=OUTPUT_DIR / "openi_labels.csv", 
                combined_groups=combined_groups)

    """
    run_gemini_label_verifier(csv_in_path=OUTPUT_DIR / "openi_labels.csv", 
                                csv_out_path=OUTPUT_DIR / "openi_labels_verified.csv", 
                                batch_size=5, combined_groups=combined_groups)
    """

    get_final_ouput_data(validated_data_path=OUTPUT_DIR / "openi_labels_verified.csv", 
                            out_path=OUTPUT_DIR / "openi_labels_final.csv", 
                            combined=combined_groups)

    get_eda_before_split(xml_dir=XML_DIR, dicom_root=DICOM_ROOT, 
                        eda_dir=EDA_DIR, output_file=OUTPUT_DIR / "openi_labels_final.csv", 
                        combine_groups=combined_groups, drop_zero=True, 
                        save_cleaned=True, max_show=10, 
                        output_drop_zero=OUTPUT_DIR)
    
    train_val_test_split(xml_dir=XML_DIR, dicom_dir=DICOM_ROOT, 
                        combined_groups=combined_groups,label_csv=OUTPUT_DIR / "openi_labels_final_cleaned.csv",
                        split_dir=SPLIT_DIR, seed=2709, split_ratio=[0.8, 0.1, 0.1])
    
    get_eda_after_split(xml_dir=XML_DIR, dicom_root=DICOM_ROOT,split_dir=SPLIT_DIR, 
                        label_csv=OUTPUT_DIR / "openi_labels_final_cleaned.csv",combined_groups=combined_groups)
end src\data_run.py

src\DataHandler\ChestXRDataset.py
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from .tensorDICOM import DICOMImagePreprocessor
from pathlib import Path
import torch

BASE_DIR = Path(__file__).resolve().parent.parent.parent
MODEL_PLACE = BASE_DIR / "models"

def tokenize_report(text, tokenizer, max_length=128):
    """
    Tokenize report using a Hugging Face tokenizer.

    Args:
        text (str): The report text to be tokenized.
        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use for tokenization.
        max_length (int, optional): The maximum length for tokenization. Defaults to 128.

    Returns:
        Tuple[torch.Tensor, torch.Tensor]: A tuple containing the tokenized input IDs and attention mask.
    """
    tokens = tokenizer(
        text or "",
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )
    return tokens.input_ids.squeeze(0), tokens.attention_mask.squeeze(0)

class ChestXRDataset(Dataset):
    """
    Dataset contructor class for ChestXR dataset.
    """
    def __init__(self, records, image_preprocessor=None, tokenizer=None, max_length=128, clinic_model_path=None):
        """
        Args:
            records (list of dicts): List of records with keys:
                - 'dicom_path'
                - 'report_text'
                - optional 'mesh_labels'
            image_preprocessor (DICOMImagePreprocessor, optional): Image preprocessor.
                Defaults to None.
            tokenizer (transformers.AutoTokenizer, optional): Tokenizer for text.
                Defaults to None.
            max_length (int, optional): Maximum length of text tokenization.
                Defaults to 128.
            model_path (str, optional): Path to model directory. Defaults to None.
        """
        self.records = records
        self.image_preprocessor = image_preprocessor or DICOMImagePreprocessor()
        cache_path = str(MODEL_PLACE / "clinicalbert") if clinic_model_path is None else clinic_model_path
        if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(
                "emilyalsentzer/Bio_ClinicalBERT",
                cache_dir=cache_path
        )
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):    
        """
        Retrieve the dataset sample at the specified index.

        Args:
            idx (int): Index of the dataset sample to retrieve.

        Returns:
            dict: A dictionary containing the following keys:
                - 'image': Processed image tensor from the DICOM file.
                - 'input_ids': Tokenized input IDs of the report text.
                - 'attn_mask': Attention mask for the tokenized report text.
                - 'labels' (optional): Labels associated with the sample, if available.
        """
        rec = self.records[idx]
        # Process image
        img = self.image_preprocessor(rec['dicom_path'])        
        # Tokenize report
        input_ids, attn_mask = tokenize_report(
            rec.get('report_text', ''),
            self.tokenizer,
            self.max_length
        )
        sample = {
            'image': img,
            'input_ids': input_ids,
            'attn_mask': attn_mask,
            'id': rec['id'] 
        }
        # Optional labels
        if 'labels' in rec:
            sample['labels'] = torch.tensor(rec['labels'], dtype=torch.float32)

        return sample

end src\DataHandler\ChestXRDataset.py

src\DataHandler\dataLoader.py
from .tensorDICOM import DICOMImagePreprocessor
from transformers import AutoTokenizer
from .ChestXRDataset import ChestXRDataset
from torch.utils.data import DataLoader, WeightedRandomSampler
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent.parent
MODEL_PLACE = BASE_DIR / "models"

def build_dataloader(
        records, 
        batch_size=4, 
        shuffle=True, 
        num_workers=4,
        mean=0.5, 
        std=0.5, 
        tokenizer=None, 
        augment=False,
        max_length=128,
        clinic_model_path=None,
        sampler: WeightedRandomSampler = None
):
    """
    Convenience function to create DataLoader for ChestXRDataset.

    Args:
        records (List[dict]): List of records parsed from OpenI XML.
        batch_size (int, optional): Defaults to 4.
        shuffle (bool, optional): Defaults to True.
        num_workers (int, optional): Defaults to 4.
        mean (float, optional): Image normalization mean. Defaults to 0.5.
        std (float, optional): Image normalization std. Defaults to 0.5.
        tokenizer (transformers.AutoTokenizer, optional): Defaults to None.
        augment (bool, optional): Defaults to False.
        max_length (int, optional): Maximum length of text tokenization. Defaults to 128.
        clinic_model_path (str, optional): Path to clinic model directory. Defaults to None.
        sampler (torch.utils.data.WeightedRandomSampler, optional): Defaults to None.

    Returns:
        torch.utils.data.DataLoader
    """
    preprocessor = DICOMImagePreprocessor(mean=mean, std=std, augment=augment)
    if tokenizer is None:
        cache_dir = str(MODEL_PLACE / "clinicalbert") if clinic_model_path is None else clinic_model_path
        tokenizer = AutoTokenizer.from_pretrained(
            "emilyalsentzer/Bio_ClinicalBERT",
            cache_dir=cache_dir
        )
    dataset = ChestXRDataset(records, image_preprocessor=preprocessor,
                                tokenizer=tokenizer, max_length=max_length)
    if sampler is not None:
        return DataLoader(dataset,
                            batch_size=batch_size,
                            sampler=sampler,
                            num_workers=num_workers,
                            pin_memory=True)
    else:
        return DataLoader(dataset,
                            batch_size=batch_size,
                            shuffle=shuffle,
                            num_workers=num_workers,
                            pin_memory=True)

end src\DataHandler\dataLoader.py

src\DataHandler\dataParser.py
import sys
import os
import glob
import xml.etree.ElementTree as ET
import spacy
from spacy.matcher import PhraseMatcher
from pathlib import Path
import subprocess

BASE_DIR    = Path(__file__).resolve().parent.parent.parent
MODEL_PLACE = BASE_DIR / "models"
MODEL_PLACE.mkdir(exist_ok=True)

# ---------------- GLOBALS ----------------
_nlp = None  # cache

def get_nlp():
    global _nlp
    if _nlp is not None:
        return _nlp

    # Load SciSpaCy model (download if missing)
    try:
        nlp = spacy.load("en_core_sci_sm")
    except OSError:
        print("[INFO] en_core_sci_sm not found, downloading...")
        subprocess.run([
            sys.executable, "-m", "pip", "install",
            "https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz"
        ], check=True)
        nlp = spacy.load("en_core_sci_sm")

    print("[INFO] Loaded model from:", nlp.path)

    # Try negspacy
    try:
        from negspacy.negation import Negex
        nlp.add_pipe("negex", config={"ent_types": ["MATCH"]})
        print("[INFO] Added Negex from negspacy")
    except Exception as e:
        print("[WARN] negspacy not available:", e)

    _nlp = nlp
    return _nlp

def label_report_diseases(text, combined_groups=None):
    if combined_groups is None:
        raise ValueError("Please provide disease groups.")

    nlp = get_nlp()

    # Build PhraseMatcher for all keywords
    matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    for group, kws in combined_groups.items():
        for kw in kws:
            matcher.add(group, [nlp.make_doc(kw)])

    doc = nlp(text)
    labels = {group: 0 for group in combined_groups}

    for match_id, start, end in matcher(doc):
        span = doc[start:end]
        group = nlp.vocab.strings[match_id]
        if not hasattr(span._, "negex") or not span._.negex:
            labels[group] = 1

    return labels

def label_vector(text, combined_groups=None):
    """
    Return a binary label vector corresponding to the disease groups in `text`

    The vector is ordered by the sorted keys of `disease_groups` and `normal_groups` and contains
    binary values (0 or 1) indicating the presence or absence of each group in
    the text.

    Parameters
    ----------
    text : str
        The text to be analyzed

    Returns
    -------
    list of int
        A binary label vector
    """
    ordered = sorted(combined_groups.keys())
    lbls = label_report_diseases(text, combined_groups)
    return [lbls[g] for g in ordered]

def parse_openi_xml(xml_dir=None, dicom_root=None, combined_groups=None):
    """
    Parse OpenI XML reports and match to corresponding DICOM files

    Parameters
    ----------
    xml_dir : str
        Path to folder containing individual .xml report files
    dicom_root : str
        Root folder where .dcm files live (possibly nested)
    combined_groups : dict of str to list of str
        Dictionary where keys are disease/normal group names and values are lists of labels
    
    Returns
    -------
    records : list of dicts
        Each dict has: 'id', 'dicom_path', 'report_text', 'labels' (14-dim vector)
    """
    if xml_dir is None or dicom_root is None:
        raise ValueError("Please provide a path to the OpenI XML reports and DICOM files.")
    if combined_groups is None:
        raise ValueError("Please provide a least a list of disease groups and normal groups to label the report with.")

    all_dcms = glob.glob(os.path.join(dicom_root, '**', '*.dcm'), recursive=True)
    dcm_map = {os.path.splitext(os.path.basename(p))[0]: p for p in all_dcms}

    print(f"[DataParser] [INFO] Found {len(os.listdir(xml_dir))} XML files in {xml_dir}")
    print(f"[DataParser] [INFO] Found {len(all_dcms)} DICOM files in {dicom_root}")

    records = []

    for fname in os.listdir(xml_dir):
        if not fname.endswith('.xml'):
            continue

        xml_path = os.path.join(xml_dir, fname)
        tree = ET.parse(xml_path)
        root = tree.getroot()

        for img_tag in root.findall('parentImage'):
            raw_id = img_tag.attrib.get('id')  # e.g. CXR3_1_IM-1384-2001
            if not raw_id:
                continue

            # Normalize ID and match to DICOM
            if raw_id.startswith("CXR") and "_" in raw_id:
                parts = raw_id[3:].split("_", 1)
                if len(parts) == 2:
                    image_id = parts[0] + "_" + parts[1]
                else:
                    continue
            else:
                continue

            dcm_path = dcm_map.get(image_id)
            if not dcm_path:
                continue

            # Extract full report text
            abstract_parts = [n.text.strip() for n in root.findall('.//AbstractText') if n.text]
            if not abstract_parts:
                title = root.findtext('.//ArticleTitle') or ""
                abstract_parts = [title.strip()]
            report = " ".join(abstract_parts)

            # Label diseases
            vec = label_vector(text=report, combined_groups=combined_groups)

            ordered_labels = sorted(combined_groups.keys())
            normal_idx = ordered_labels.index("Normal")

            is_normal = vec[normal_idx] == 1 and sum(vec) == 1
            is_abnormal = any(vec[i] for i in range(len(vec)) if i != normal_idx)

            records.append({
                'id': image_id,
                'dicom_path': dcm_path,
                'report_text': report,
                'labels': vec,
                'is_normal': is_normal,
                'is_abnormal': is_abnormal
            })

    print(f"[DataParser] [INFO] Loaded {len(records)} records.")
    return records

end src\DataHandler\dataParser.py

src\DataHandler\finalOutputData.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import pandas as pd
import ast
from pathlib import Path
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
# Resolve paths...
try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent

def get_final_ouput_data(validated_data_path = None, out_path = None, combined = None):
    """
    Apply manual label changes from a verified CSV to a final output CSV.

    Given a verified CSV, apply remove/add label changes and save the result to a final output CSV.

    Args:
        combined (dict): A dictionary of label names to lists of IDs to remove/add.

    Returns:
        None
    """
    vdata_path = BASE_DIR / "outputs" / "openi_labels_verified_final.csv" if validated_data_path is None else validated_data_path
    out_path   = BASE_DIR / "outputs" / "openi_labels_final.csv" if out_path is None else out_path

    if combined is None:
        raise Exception("Must provide a dictionary of label names to lists of IDs to remove/add.")

    # Build your one‑hot label columns
    label_cols = list(combined.keys())

    # Load the CSV
    df = pd.read_csv(vdata_path)

    # If there's a duplicate ID column like "id.1", drop it
    for dup in [c for c in df.columns if c.startswith("id.")]:
        df = df.drop(columns=dup)

    def safe_parse_list(val):
        """Parse a Python‑style or JSON list literal; return [] on error/NaN."""
        if pd.isna(val) or str(val).strip() == "":
            return []
        try:
            parsed = ast.literal_eval(val)
            return [str(x).strip() for x in parsed] if isinstance(parsed, (list, tuple)) else []
        except Exception:
            return []

    # Make sure all label columns are ints to start
    df[label_cols] = df[label_cols].fillna(0).astype(int)

    # Now apply remove/add via an explicit loop
    for idx, row in df.iterrows():
        to_remove = safe_parse_list(row.get("remove", "[]"))
        to_add    = safe_parse_list(row.get("add",    "[]"))

        # Remove any labels
        for lbl in to_remove:
            if lbl in label_cols:
                df.at[idx, lbl] = 0

        # Add any labels
        for lbl in to_add:
            if lbl in label_cols:
                df.at[idx, lbl] = 1

    # Collect final labels into a list
    df["final_labels"] = df[label_cols].apply(
        lambda r: [lbl for lbl, v in r.items() if v == 1],
        axis=1
    )

    # Save
    df.to_csv(out_path, index=False)
    print(f"Wrote cleaned labels to {out_path}")

if __name__ == "__main__":
    combined_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
    }
    get_final_ouput_data(combined=combined_groups)

end src\DataHandler\finalOutputData.py

src\DataHandler\labeledData2CSV.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import pandas as pd
from .dataParser import parse_openi_xml
from pathlib import Path
import numpy as np
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups

# Resolve paths...
try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent

XML_DIR    = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
MODEL_PLACE = BASE_DIR / "models"

def label2CSV(xml_dir=None, dicom_dir=None, out_path=None, combined_groups=None):
    """
    Convert parsed OpenI XML reports to a CSV file with labels.

    Parameters
    ----------
    xml_dir : str
        Path to folder containing individual .xml report files
    dicom_dir : str
        Root folder where .dcm files live (possibly nested)
    out_path : str
        Path to save the output CSV file to
    combined_groups : dict of str to list of str
        Dictionary where keys are disease/normal group names and values are lists of labels

    Returns
    -------
    None

    Saves a CSV file with the following columns: ['id', 'report_text'] + label_names
    """
    if xml_dir is None:
        xml_dir = XML_DIR
    if dicom_dir is None:
        dicom_dir = DICOM_ROOT
    if combined_groups is None:
        raise ValueError("Please provide a least a list of disease groups and normal groups to label the report with.")
    
    label_names  = sorted(combined_groups.keys())
    records      = parse_openi_xml(xml_dir=xml_dir, dicom_root=dicom_dir, combined_groups=combined_groups)
    
    # Build label matrix
    label_matrix = np.array([rec['labels'] for rec in records])

    # Build a pandas DataFrame
    df = pd.DataFrame(label_matrix, columns=label_names)
    df.insert(0, 'report_text', [rec['report_text'] for rec in records])
    df.insert(0, 'id',          [rec['id']          for rec in records])

    # Save to CSV
    out_path = BASE_DIR / 'outputs' / 'openi_labels.csv' if out_path is None else out_path
    out_path.parent.mkdir(exist_ok=True)
    df.to_csv(out_path, index=False, encoding='utf-8')

    print(f"Wrote {len(df)} rows with {len(label_names)} labels to {out_path}")

if __name__ == "__main__":
    combined_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
    }
    label2CSV(combined_groups=combined_groups)
end src\DataHandler\labeledData2CSV.py

src\DataHandler\stat_utils.py
from .tensorDICOM import DICOMImagePreprocessor
from torch.utils.data import Dataset
import torch
import cv2
import numpy as np

class RawStatDataset(Dataset):
    """
    Dataset child class for raw DICOM images preprocessed.
    """
    def __init__(self, records, size=(224, 224)):
        """
        Constructor for RawStatDataset.

        Parameters:
        records (List[dict]): List of records parsed from OpenI XML.
        size (Tuple[int, int], optional): Target size for resizing raw DICOM images.
            Defaults to (224, 224).
        """
        self.records = records
        self.size = size  # target (width, height)

    def __len__(self):
        return len(self.records)

    def __getitem__(self, idx):
        """
        Retrieve the dataset sample at the specified index.

        Parameters:
        idx (int): Index of the dataset sample to retrieve.

        Returns:
        tensor (torch.Tensor): A normalized tensor of shape (3, H, W) ready for
            input to a neural network.
        """
        rec = self.records[idx]
        try:
            # Load raw DICOM image
            pre = DICOMImagePreprocessor(augment=False)
            arr = pre.load_raw_array(rec['dicom_path'])  # shape: (H, W)

            # Resize to (H, W) = (224, 224)
            resized = cv2.resize(arr, self.size, interpolation=cv2.INTER_AREA)

            # Normalize to [0, 1]
            if resized.max() > 1:
                resized = resized.astype(np.float32) / 255.0

            # Convert to 3-channel when using Swin (by duplicating channels)
            resized = np.stack([resized] * 3, axis=0)  # shape: (3, 224, 224)

            return torch.from_numpy(resized).float()

        except Exception as e:
            print(f"[ERROR] Failed at idx={idx} to {rec['dicom_path']}: {e}")
            return torch.zeros((3, *self.size), dtype=torch.float32)  # fallback


end src\DataHandler\stat_utils.py

src\DataHandler\tensorDICOM.py
import io
import torch
import pydicom
import numpy as np
from torchvision import transforms
from pathlib import Path

class DICOMImagePreprocessor:
    """
    Object-oriented DICOM loader and preprocessor.
    Applies windowing, normalization, and torchvision transforms.
    """
    def __init__(self, mean=0.5, std=0.5,
                    default_window_center=40.0, default_window_width=400.0,
                    output_size=(224, 224), augment=False):
        self.mean = mean
        self.std = std
        self.default_center = default_window_center
        self.default_width = default_window_width
        # Define torchvision pipeline
        ops = [
            transforms.ToPILImage(),
            transforms.Resize(output_size)
        ]
        if augment:
            ops += [
                transforms.RandomRotation(5),
                transforms.RandomHorizontalFlip(0.1)
            ]
        ops += [
            transforms.ToTensor(),
            transforms.Normalize([mean], [std])
        ]
        self.transform = transforms.Compose(ops)

    def window_image(self, pixel_array, window_center, window_width):
        """
        Apply windowing transformation to raw pixel data based on the specified
        window center and width. This operation maps the pixel values to a 
        normalized range [0, 1] by clipping the pixel array within the specified 
        window and then scaling it.

        Parameters:
        pixel_array (np.ndarray): Raw pixel data to be transformed.
        window_center (float): The center value of the window.
        window_width (float): The total width of the window.

        Returns:
        np.ndarray: The windowed and normalized pixel data.
        """
        lower = window_center - window_width / 2
        upper = window_center + window_width / 2
        img = np.clip(pixel_array, lower, upper)
        return (img - lower) / (upper - lower)

    @staticmethod
    def load_raw_array(dicom_path):
        """
        Load a single DICOM file and return a normalized array.

        Parameters:
        dicom_path (str): Path to the DICOM file.

        Returns:
        array (np.ndarray): A normalized array of shape (H, W) ready for
            input to a neural network.
        """
        if isinstance(dicom_path, (str, Path)):
            dcm = pydicom.dcmread(dicom_path)
        elif isinstance(dicom_path, (bytes, bytearray)):
            dcm = pydicom.dcmread(io.BytesIO(dicom_path))
        else:
            raise TypeError(f"Unsupported type for dicom_path: {type(dicom_path)}")

        # Raw pixel values
        raw = dcm.pixel_array.astype(np.float32)

        # Rescale
        slope     = float(getattr(dcm, 'RescaleSlope', 1.0))
        intercept = float(getattr(dcm, 'RescaleIntercept', 0.0))
        scaled = raw * slope + intercept

        # WindowCenter / WindowWidth
        pmin, pmax = np.percentile(scaled, [0.5, 99.5])
        wc = (pmin + pmax) / 2
        ww = pmax - pmin

        # DEBUG LOGGING
        print(f"[DEBUG] {dicom_path}")
        print(f"  raw min/max    = {raw.min():.1f}/{raw.max():.1f}")
        print(f"  slope,intercpt = {slope:.3f}, {intercept:.3f}")
        print(f"  scaled min/max = {scaled.min():.1f}/{scaled.max():.1f}")
        print(f"  window center  = {wc:.1f}, width = {ww:.1f}")

        # Window + normalize
        lower, upper = wc - ww/2, wc + ww/2
        win = np.clip(scaled, lower, upper)
        norm = (scaled - scaled.min()) / (scaled.max() - scaled.min())

        print(f"  post-win min/max = {norm.min():.3f}/{norm.max():.3f}\n")

        return norm

    def load(self, dicom_path):
        """
        Load a single DICOM file and return a normalized tensor.

        Parameters:
        dicom_path (str): Path to the DICOM file.

        Returns:
        tensor (torch.Tensor): A normalized tensor of shape (1, H, W) ready for
            input to a neural network.
        """
        if isinstance(dicom_path, (bytes, bytearray)):
            dcm = pydicom.dcmread(io.BytesIO(dicom_path))
        else:
            dcm = pydicom.dcmread(dicom_path)
        # Read window values (handle MultiValue)
        if 'WindowCenter' in dcm:
            wc = float(dcm.WindowCenter[0] if isinstance(dcm.WindowCenter, pydicom.multival.MultiValue) else dcm.WindowCenter)
        else:
            wc = self.default_center
        if 'WindowWidth' in dcm:
            ww = float(dcm.WindowWidth[0] if isinstance(dcm.WindowWidth, pydicom.multival.MultiValue) else dcm.WindowWidth)
        else:
            ww = self.default_width
        pixel_array = dcm.pixel_array.astype(np.float32)
        windowed = self.window_image(pixel_array, wc, ww)
        # expand channels
        img = np.expand_dims(windowed, axis=0)  # (1, H, W)
        tensor = torch.from_numpy(img)
        # apply torchvision transforms
        return self.transform(tensor)

    def __call__(self, dicom_path):
        return self.load(dicom_path)
end src\DataHandler\tensorDICOM.py

src\DataHandler\train_val_split.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))

import json
from collections import defaultdict
import numpy as np
import pandas as pd
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit

from .dataParser import parse_openi_xml
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups

# Resolve paths...
try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

XML_DIR    = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
LABEL_CSV  = BASE_DIR / 'outputs' / 'openi_labels_final_cleaned.csv'
SPLIT_DIR  = BASE_DIR / 'splited_data'
SPLIT_DIR.mkdir(parents=True, exist_ok=True)


def train_val_test_split(xml_dir=None, dicom_dir=None, combined_groups=None,
                         label_csv=None, split_dir=None, seed=42,
                         split_ratio=(0.8, 0.1, 0.1)):
    """Create report-level multilabel stratified train/val/test splits.
       Saves train/val/test id json files and labeled CSVs (using cleaned label CSV).
    """
    if xml_dir is None: xml_dir = XML_DIR
    if dicom_dir is None: dicom_dir = DICOM_ROOT
    if combined_groups is None:
        raise ValueError("Provide combined_groups")
    if label_csv is None: label_csv = LABEL_CSV
    if split_dir is None: split_dir = SPLIT_DIR

    if not Path(label_csv).exists():
        raise FileNotFoundError(f"Label CSV not found: {label_csv}")

    # --- Parse records grouped by report_text ---
    records = parse_openi_xml(xml_dir, dicom_dir, combined_groups)
    report_to_records = defaultdict(list)
    for rec in records:
        report_to_records[rec["report_text"]].append(rec)
    reports = list(report_to_records.keys())
    if len(reports) == 0:
        raise RuntimeError("No reports parsed. Check parse_openi_xml output.")

    # --- Load labels CSV ---
    labels_df = pd.read_csv(label_csv).set_index("id")
    label_cols = (list(disease_groups.keys()) +
                  list(normal_groups.keys()) +
                  list(finding_groups.keys()) +
                  list(symptom_groups.keys()))

    # --- Build per-report label vectors ---
    report_label_vecs = []
    report_id_lists = []
    missing_ids = 0
    for text in reports:
        recs = report_to_records[text]
        ids = [r["id"] for r in recs]
        report_id_lists.append(ids)
        rows = labels_df.reindex(ids)  # safe: missing ids -> NaN
        if rows.isnull().any(axis=None):
            missing_ids += int(rows.isnull().any(axis=1).sum())
        vec = (rows[label_cols].fillna(0).sum(axis=0) > 0).astype(int).values
        report_label_vecs.append(vec)
    if missing_ids:
        print(f"[WARN] {missing_ids} image IDs from parsed records not found in {label_csv} (treated as zeros).")

    report_label_vecs = np.vstack(report_label_vecs)  # (R, C)

    # --- Stratified splitting ---
    train_frac, val_frac, test_frac = split_ratio
    if not np.isclose(train_frac + val_frac + test_frac, 1.0):
        raise ValueError("split_ratio must sum to 1.0")

    # train vs temp
    msss1 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=(1 - train_frac), random_state=seed)
    train_idx, temp_idx = next(msss1.split(np.zeros(len(report_label_vecs)), report_label_vecs))

    # temp -> val/test
    temp_labels = report_label_vecs[temp_idx]
    val_rel = val_frac / (val_frac + test_frac)
    msss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=(1 - val_rel), random_state=seed+1)
    val_rel_idx, test_rel_idx = next(msss2.split(np.zeros(len(temp_idx)), temp_labels))
    val_idx = temp_idx[val_rel_idx]
    test_idx = temp_idx[test_rel_idx]

    train_reports = [reports[i] for i in train_idx]
    val_reports   = [reports[i] for i in val_idx]
    test_reports  = [reports[i] for i in test_idx]

    # flatten to record ids
    def flatten_ids(report_list):
        return [r["id"] for rpt in report_list for r in report_to_records[rpt]]
    train_ids = [str(x) for x in flatten_ids(train_reports)]
    val_ids   = [str(x) for x in flatten_ids(val_reports)]
    test_ids  = [str(x) for x in flatten_ids(test_reports)]

    # --- Save ID splits ---
    split_dir = Path(split_dir)
    with open(split_dir / "train_split_ids.json", "w") as f: json.dump(train_ids, f)
    with open(split_dir / "val_split_ids.json", "w") as f: json.dump(val_ids, f)
    with open(split_dir / "test_split_ids.json", "w") as f: json.dump(test_ids, f)

    # --- Save labeled CSVs (only IDs present in CSV) ---
    df = pd.read_csv(label_csv)
    available_ids = set(df["id"].astype(str))
    def keep_present(ids): return [i for i in ids if i in available_ids]
    train_ids_present = keep_present(train_ids)
    val_ids_present   = keep_present(val_ids)
    test_ids_present  = keep_present(test_ids)

    df[df["id"].astype(str).isin(train_ids_present)].to_csv(split_dir / "openi_train_labeled.csv", index=False)
    df[df["id"].astype(str).isin(val_ids_present)].to_csv(split_dir / "openi_val_labeled.csv", index=False)
    df[df["id"].astype(str).isin(test_ids_present)].to_csv(split_dir / "openi_test_labeled.csv", index=False)

    # --- Diagnostics ---
    df_indexed = df.set_index("id")
    def per_label_counts(ids):
        arr = df_indexed.reindex(ids)[label_cols].fillna(0).values if len(ids) > 0 else np.zeros((0, len(label_cols)))
        return arr.sum(axis=0).astype(int)

    train_counts = per_label_counts(train_ids_present)
    val_counts   = per_label_counts(val_ids_present)
    test_counts  = per_label_counts(test_ids_present)

    diag = pd.DataFrame({
        "label": label_cols,
        "train_pos": train_counts,
        "val_pos": val_counts,
        "test_pos": test_counts
    }).sort_values("val_pos")

    print("Split sizes (records):", len(train_ids_present), len(val_ids_present), len(test_ids_present))
    print("Per-label positives in validation (bottom 40):")
    print(diag[["label", "val_pos"]].head(40).to_string(index=False))
    zeros = diag[diag.val_pos == 0].label.tolist()
    if zeros:
        print(f"[WARN] {len(zeros)} labels have 0 positives in validation. Consider larger val, k-fold, or merging very-rare labels.")

    print("Saved stratified report-level splits and CSVs to:", split_dir)
    return train_ids_present, val_ids_present, test_ids_present


if __name__ == "__main__":
    combined_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
    }
    train_ids, val_ids, test_ids = train_val_test_split(
        xml_dir=XML_DIR,
        dicom_dir=DICOM_ROOT,
        combined_groups=combined_groups,
        label_csv=LABEL_CSV,
        split_dir=SPLIT_DIR,
        seed=42,
        split_ratio=(0.8, 0.1, 0.1)
    )

end src\DataHandler\train_val_split.py

src\DataHandler\verify_labels_with_gemini.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import pandas as pd
import json
import random
import time
import logging
from google import genai
import os
from pathlib import Path
from dotenv import load_dotenv
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups

# Resolve paths
try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent

XML_DIR    = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
MODEL_PLACE = BASE_DIR / "models"
os.environ['TRANSFORMERS_CACHE'] = str(MODEL_PLACE)

class OpenIChecker:
    """
    Class to verify labels with Gemini
    """
    def __init__(self, gemini_api_keys):
        self.gemini_api_keys = gemini_api_keys

    def verify_labels_with_gemini(self, csv_path, output_path, batch_size=10, combined_groups=None):
        """
        Verify labels with Gemini.

        This function will take a CSV file of labels and verify them
        using the Gemini AI. It will output a new CSV file with the
        verified labels.

        Parameters
        ----------
        csv_path : str
            The path to the CSV file to verify.
        output_path : str
            The path to write the verified labels CSV file.
        batch_size : int, optional
            The number of labels to verify in each batch. Defaults to 10.
        combined_groups : dict, optional
            A dictionary of label names to lists of IDs to remove/add.

        Returns
        -------
        None

        Notes
        -----
        The function will write a merged CSV file to
        outputs/openi_labels_verified_final.csv, which includes all
        verified labels and retried Unknown rows.
        """
        df = pd.read_csv(csv_path)
        verified = []

        api_keys = self.gemini_api_keys.copy()
        print(f"Loaded {len(api_keys)} API keys")
        if len(api_keys) < 20:
            raise ValueError("Not enough API keys")

        random.shuffle(api_keys)

        for start in range(0, len(df), batch_size):
            time.sleep(5)
            batch = df.iloc[start : start + batch_size]
            prompt = self._build_verify_prompt(batch, combined_groups)

            response_text = None
            for api_key in api_keys:
                try:
                    client = genai.Client(api_key=api_key)
                    resp = client.models.generate_content(
                        model="gemini-2.5-flash",
                        contents=prompt
                    )
                    response_text = resp.text
                    break
                except Exception as e:
                    logging.error(f"[Verifier] Key {api_key[:4]} failed: {e}")
                    time.sleep(1)

            if response_text is None:
                for _ in batch.itertuples():
                    verified.append({
                        "llm_status":       "Unknown",
                        "llm_suggestions":  "{}"
                    })
            else:
                try:
                    if response_text.startswith("```json"):
                        response_text = response_text.strip()[7:-3].strip()
                    elif response_text.startswith("```"):
                        response_text = response_text.strip()[3:-3].strip()
                    results = json.loads(response_text)
                    print(f"Verifier returned {len(results)} results")
                except json.JSONDecodeError:
                    logging.error("Verifier returned invalid JSON; marking batch Unknown")
                    for _ in batch.itertuples():
                        verified.append({
                            "llm_status":       "Unknown",
                            "llm_suggestions":  "{}"
                        })
                else:
                    verified.extend(results)
            time.sleep(5)

        print(f"Verified {len(verified)} labels")
        verified_df = pd.DataFrame(verified)
        out = pd.concat([df.reset_index(drop=True), verified_df], axis=1)
        out.to_csv(output_path, index=False, encoding='utf-8')
        print(f"Wrote verified labels to {output_path}")

    def _build_verify_prompt(self, batch_df, combined_groups=None):
        """
        Function for building a prompt for the LLM context to verify labels.

        Parameters
        ----------
        batch_df : pd.DataFrame
            A DataFrame of labels to verify.
        combined_groups : dict, optional
            A dictionary of label names to lists of IDs to remove/add.
        combined_groups : list, optional
            A list of label groups to combine for verification.
        Returns
        -------
        str
            A JSON string to be used as the prompt for the LLM context.

        Notes
        -----
        The function will raise a ValueError if no combined groups are provided.
        The function will provide label descriptions for LLM context by
        serializing the combined groups to a JSON string.
        The function will generate a prompt string by concatenating the
        instructions and the JSON string of the batch entries.
        """
        if combined_groups is None:
            raise ValueError("Please provide a least a list of disease groups and normal groups to label the report with.")
        
        # Provide label descriptions for LLM context
        label_info = json.dumps(combined_groups, indent=2)

        instructions = (
            "You are a clinical-label verifier. For each chest X‑ray report and its"
            " assigned one‑hot disease/normal labels, check whether the labels are:"
            "\n  • Correct: no changes needed"
            "\n  • Incorrect: remove any labels that don’t apply"
            "\n  • Missing: add any labels that were missed"
            "\nRespond *only* in JSON, as a list of objects with these keys:\n"
            "  {\n"
            "    \"id\": <the report’s id>,\n"
            "    \"llm_status\": \"Correct\"|\"Incorrect\"|\"Missing\",\n"
            "    \"remove\": [<labels to remove>],\n"
            "    \"add\":    [<labels to add>]\n"
            "  }\n"
            "\nHere are the label categories and their associated terms:\n"
            f"{label_info}\n"
            "\nNow review these records:\n\n"
        )

        entries = []
        for _, row in batch_df.iterrows():
            assigned = [c for c in batch_df.columns
                        if c not in ("id", "report_text")
                        and row[c] == 1]
            entries.append({
                "id":          row["id"],
                "report_text": row["report_text"],
                "labels":      assigned
            })

        return instructions + json.dumps(entries, indent=2)

    def _run_pass(self, csv_path, batch_size, output_path, combined_groups):
        """
        Internal helper to re-use verify_labels_with_gemini for retrying small batches.
        """
        self.verify_labels_with_gemini(csv_path=csv_path,
                                       output_path=output_path,
                                       batch_size=batch_size, 
                                       combined_groups=combined_groups)
        return pd.read_csv(output_path)

    def retry_unknowns(self, verified_csv_path, output_path, retry_batch_size=1, combined_groups=None):
        """
        Retry verifying labels on Unknown rows.

        Parameters
        ----------
        verified_csv_path : str
            The path to the CSV file of already verified labels.
        output_path : str
            The path to write the retried + merged results CSV file.
        retry_batch_size : int, optional
            The number of labels to retry in each batch. Defaults to 1.
        combined_groups : dict, optional
            A dictionary of label names to lists of IDs to remove/add.

        Returns
        -------
        pd.DataFrame
            The DataFrame of retried + merged results.

        Notes
        -----
        The function will write a merged CSV file to
        outputs/openi_labels_verified_final.csv, which includes all
        verified labels and retried Unknown rows.
        """
        # Load the already‑verified CSV
        verified_df = pd.read_csv(verified_csv_path)

        # Select only the Unknown rows
        unknown_df = verified_df[verified_df["llm_status"] == "Unknown"].copy()
        if unknown_df.empty:
            print("No Unknown rows to retry.")
            return verified_df

        print(f"Retrying {len(unknown_df)} Unknown rows…")

        # Run a single‑pass on just the unknowns
        temp_in  = verified_csv_path.parent / "temp_unknowns.csv"
        temp_out = verified_csv_path.parent / "temp_retry_results.csv"
        unknown_df.to_csv(temp_in, index=False)

        retry_df = self._run_pass(
            csv_path    = temp_in,
            batch_size  = retry_batch_size,
            output_path = temp_out,
            combined_groups=combined_groups
        )

        # Merge: drop old Unknowns, append new rows
        kept = verified_df[verified_df["llm_status"] != "Unknown"]
        merged = pd.concat([kept, retry_df], axis=0, ignore_index=True)

        #  Write final merged file
        merged.to_csv(output_path, index=False, encoding="utf-8")
        print(f"Wrote retried + merged results to {output_path}")

        return merged

def run_gemini_label_verifier(csv_in_path=None, csv_out_path=None, batch_size=5, combined_groups=None):
    """
    Run the Gemini label verifier on a CSV file.

    This function loads Gemini API keys from a .env file,
    creates an OpenIChecker instance with those keys,
    runs the verifier on a CSV file, and then retries
    any Unknown rows with a single pass of Gemini.

    Parameters
    ----------
    csv_in_path : str, optional
        The path to the CSV file to verify. Defaults to
        outputs/openi_labels.csv.
    csv_out_path : str, optional
        The path to write the verified labels CSV file.
        Defaults to outputs/openi_labels_verified.csv.
    batch_size : int, optional
        The number of labels to verify in each batch. Defaults to 5.
    combined_groups : dict, optional
        A dictionary of label groups to combine for verification.

    Returns
    -------
    None

    Notes
    -----
    The final verified CSV file will be written to
    outputs/openi_labels_verified_final.csv, which includes all
    verified labels and retried Unknown rows.
    """
    env_path = Path(__file__).resolve().parent.parent.parent / ".env"
    load_dotenv(dotenv_path=env_path)

    raw = os.getenv("GEMINI_KEYS", "")
    if not raw:
        raise ValueError("No GEMINI_KEYS found in .env file")
    gemini_api_keys = [k.strip() for k in raw.split(",") if k.strip()]

    checker = OpenIChecker(gemini_api_keys)

    csv_in  = BASE_DIR / "outputs" / "openi_labels.csv" if csv_in_path is None else csv_in_path
    csv_out = BASE_DIR / "outputs" / "openi_labels_verified.csv" if csv_out_path is None else csv_out_path

    checker.verify_labels_with_gemini(csv_path=csv_in,
                                        output_path=csv_out,
                                        batch_size=batch_size,
                                        combined_groups=combined_groups)

    csv_out_final = BASE_DIR / "outputs" / "openi_labels_verified_final.csv"

    checker.retry_unknowns(
    verified_csv_path = csv_out,
    output_path       = csv_out_final,
    retry_batch_size  = 1,
    combined_groups=combined_groups
    )
    
if __name__ == "__main__":
    combined_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
    }
    run_gemini_label_verifier(combined_groups=combined_groups)
end src\DataHandler\verify_labels_with_gemini.py

src\DataHandler\__init__.py
from .ChestXRDataset import ChestXRDataset, tokenize_report
from .dataLoader import build_dataloader
from .dataParser import parse_openi_xml
from .finalOutputData import get_final_ouput_data
from .labeledData2CSV import label2CSV
from .stat_utils import RawStatDataset
from .tensorDICOM import DICOMImagePreprocessor
from .train_val_split import train_val_test_split
from .verify_labels_with_gemini import run_gemini_label_verifier, OpenIChecker

__all__ = [
    "ChestXRDataset",
    "tokenize_report",
    "build_dataloader",
    "parse_openi_xml",
    "get_final_ouput_data",
    "label2CSV",
    "RawStatDataset",
    "DICOMImagePreprocessor",
    "train_val_test_split",
    "run_gemini_label_verifier",
    "OpenIChecker"
]
end src\DataHandler\__init__.py

src\Evaluate\dataEDAnLabeledCheck.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
from collections import Counter, defaultdict
import xml.etree.ElementTree as ET
from LabelData import disease_groups, device_groups, finding_groups, symptom_groups, technical_groups, normal_groups, anatomy_groups
import pandas as pd
import matplotlib.pyplot as plt

try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

def edaLabeledCheck(xml_dir=None, 
                    save_dir=None, 
                    min_count_threshold=100):
    """
    This function checks the XML files in the given directory for MeSH labels,
    and outputs statistics on the number of MeSH labels, the number of
    unmapped MeSH labels, and the number of duplicate MeSH labels.

    Args:
        xml_dir (str): The directory containing the XML files to check.
        save_dir (str): The directory to save the output CSV file.
        min_count_threshold (int): The minimum count for a MeSH label to be plotted.

    Returns:
        None
    """
    # Point to the real directory (relative to project root)
    xml_dir = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology' if xml_dir is None else xml_dir

    files = list(xml_dir.glob("*.xml"))
    print("\nFound XML files:", len(files))

    existing_ids = {int(p.stem) for p in xml_dir.glob("*.xml")}
    expected_ids = set(range(1, 4000))
    missing_ids = sorted(expected_ids - existing_ids)
    print("\nMissing XML files:", missing_ids)
    print("\nCount missing:", len(missing_ids))

    mesh_counter = Counter()
    for fn in files:
        root = ET.parse(fn).getroot()
        for term_node in root.findall('.//MeSH/*'):
            raw = term_node.text or ""
            label = raw.split('/')[0].strip().lower()
            if label:
                mesh_counter[label] += 1

    print("\nUnique MeSH labels:", len(mesh_counter))
    print(mesh_counter)

    mesh_terms = list(mesh_counter.keys())
    print("\nUnique MeSH labels:", len(mesh_terms))

    print("\nMeSH terms:")
    print(mesh_terms)

    # Put all of individual group‑dicts into one single dict
    all_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **device_groups,
        **technical_groups,
        **normal_groups,
        **anatomy_groups
    }

    # Normalize both mesh_terms and all group terms to lowercase
    mesh_terms_lc = [t.strip().lower() for t in mesh_terms]
    all_groups_lc = {
        grp: [t.strip().lower() for t in terms]
        for grp, terms in all_groups.items()
    }

    # Build reverse map: term → list of groups it belongs to
    reverse_map = defaultdict(list)
    for grp, terms in all_groups_lc.items():
        for term in terms:
            reverse_map[term].append(grp)

    # Find unmapped MeSH terms
    unmapped = [t for t in mesh_terms_lc if t not in reverse_map]

    # Find duplicates in the original mesh_terms
    mesh_dups = {t for t in mesh_terms_lc if mesh_terms_lc.count(t) > 1}

    # Find duplicates across all labels (terms assigned more than once to different groups)
    labeled_dups = {t for t, grps in reverse_map.items() if len(grps) > 1}

    # Find “extra” labels (terms in your groups that aren’t in mesh_terms)
    extra_labeled = [t for t in reverse_map if t not in mesh_terms_lc]

    # Summary printout
    print(f"\nTotal MeSH terms           : {len(set(mesh_terms_lc))}")
    print(f"\nLabeled MeSH terms         : {len(set(mesh_terms_lc) - set(unmapped))}")
    print(f"\nUnmapped MeSH terms        : {len(unmapped)}")
    print(f"{unmapped}")
    print(f"\nDuplicate in mesh_terms    : {mesh_dups or 'None'}")
    print(f"\nTerms assigned to multiple groups: {labeled_dups or 'None'}")
    print(f"\nExtra labeled terms not in mesh_terms: {extra_labeled or 'None'}")

    # Detailed per-group term‑counts
    print(f"\nTotal group: {len(all_groups_lc)}")
    print("\nGroup term counts:")
    for grp, terms in all_groups_lc.items():
        print(f" - {grp}: {len(terms)}")

    # Detailed per-diseases term‑counts
    print(f"\nTotal diseases group: {len(disease_groups)}")
    print("\nGroup term counts:")
    for grp, terms in disease_groups.items():
        print(f" - {grp}: {len(terms)}")

    # Detailed per-findings term‐counts
    print(f"\nTotal finding group: {len(finding_groups)}")
    print("\nGroup term counts:")
    for grp, terms in finding_groups.items():
        print(f" - {grp}: {len(terms)}")

    # Detailed per-symptoms term‐counts
    print(f"\nTotal symptoms group: {len(symptom_groups)}")
    print("\nGroup term counts:")
    for grp, terms in symptom_groups.items():
        print(f" - {grp}: {len(terms)}")

    df = pd.DataFrame(mesh_counter.items(), columns=['MeSH_Label', 'Count'])

    # Filter for labels with count >= min_count_threshold
    df_above_threshold = df[df['Count'] >= min_count_threshold].sort_values(by='Count', ascending=False)

    # Save counts to CSV
    save_dir = BASE_DIR / 'eda_data' if save_dir is None else save_dir
    csv_path = save_dir / f'mesh_labels_count_gte_{min_count_threshold}.csv' 
    csv_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(csv_path, index=False)
    print(f"Saved MeSH label counts to {csv_path}")

    # Save bar plot
    if not df[df['Count'] >= min_count_threshold].empty:
        plt.figure(figsize=(12, 8))
        plt.barh(df_above_threshold['MeSH_Label'], df_above_threshold['Count'], color='purple')
        plt.xlabel('Count')
        plt.ylabel('MeSH Label')
        plt.title(f'MeSH Labels with Count >= {min_count_threshold}')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        
        img_path = save_dir / f'mesh_labels_count_gte_{min_count_threshold}.png'
        plt.savefig(img_path)
        plt.close()
        print(f"Saved bar chart to {img_path}")

end src\Evaluate\dataEDAnLabeledCheck.py

src\Evaluate\data_phrase_check.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import os
import matplotlib.pyplot as plt
import torch
import pydicom
import numpy as np
from collections import Counter
from pathlib import Path
from torch.utils.data import DataLoader
from DataHandler import parse_openi_xml, DICOMImagePreprocessor, RawStatDataset, build_dataloader
from Model import Backbones
from Helpers import load_hf_model_or_local
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
from Helpers import log_and_print

# Resolve paths...
try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

XML_DIR    = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
MODEL_PLACE = BASE_DIR / "models"
CHECK_RUN_DIR = BASE_DIR / "check_run"
if not os.path.exists(CHECK_RUN_DIR):
    os.makedirs(CHECK_RUN_DIR)
os.environ['TRANSFORMERS_CACHE'] = str(MODEL_PLACE)

def analyze_label_distribution(records,label_field='labels', label_names=None):
    """
    Count how often each label (from label vectors) appears in the dataset.
    
    Parameters
    ----------
    records : list of dicts
        Each record must have a 'labels' field that is a 14-dim list
    label_names : list of str
        List of names for the labels (optional but recommended)

    Returns
    -------
    Counter with label name (or index) as key and count as value
    """
    counter = Counter()
    for rec in records:
        vec = rec[label_field]
        for i, v in enumerate(vec):
            if v == 1:
                label = label_names[i] if label_names else i
                counter[label] += 1
    return counter

def plot_dicom_debug(dicom_path, check_run_dir=CHECK_RUN_DIR, log_file=None):
    """
    Plot debugging information for a single DICOM image.

    Plots the original raw image, the scaled image, the windowed image, and the
    normalized image. Prints out the min/max values of each.

    Args:
        dicom_path (str): path to the DICOM image file
    """
    dcm = pydicom.dcmread(dicom_path)
    raw = dcm.pixel_array.astype(np.float32)
    slope = float(getattr(dcm, 'RescaleSlope', 1.0))
    intercept = float(getattr(dcm, 'RescaleIntercept', 0.0))
    scaled = raw * slope + intercept

    wc_val = dcm.get('WindowCenter', 40.0)
    ww_val = dcm.get('WindowWidth', 400.0)
    wc = float(wc_val[0] if isinstance(wc_val, pydicom.multival.MultiValue) else wc_val)
    ww = float(ww_val[0] if isinstance(ww_val, pydicom.multival.MultiValue) else ww_val)

    lower, upper = wc - ww / 2, wc + ww / 2
    win = np.clip(scaled, lower, upper)
    norm = (win - lower) / (upper - lower + 1e-5)

    log_and_print(f"[DEBUG] raw min/max       = {raw.min():.2f} / {raw.max():.2f}", log_file=log_file)
    log_and_print(f"        scaled min/max    = {scaled.min():.2f} / {scaled.max():.2f}", log_file=log_file)
    log_and_print(f"        window center/wid = {wc} / {ww}", log_file=log_file)
    log_and_print(f"        clip min/max      = {win.min():.2f} / {win.max():.2f}", log_file=log_file)
    log_and_print(f"        norm min/max      = {norm.min():.4f} / {norm.max():.4f}", log_file=log_file)

    fig, axs = plt.subplots(1, 4, figsize=(20, 4))
    axs[0].hist(raw.ravel(), bins=100)
    axs[0].set_title("Raw")
    axs[1].hist(scaled.ravel(), bins=100)
    axs[1].set_title("Scaled")
    axs[2].hist(win.ravel(), bins=100)
    axs[2].set_title("Windowed")
    axs[3].hist(norm.ravel(), bins=100)
    axs[3].set_title("Normalized")
    plt.tight_layout()
    plt.savefig(check_run_dir / "dicom_debug.png")
    plt.close()

    plt.imshow(norm, cmap='gray', vmin=0, vmax=1)
    plt.title("Final Normalized Image")
    plt.axis('off')
    plt.savefig(check_run_dir / "final_norm.png")
    plt.close()

def dataPhraseCheck(xml_path=XML_DIR, dicom_path=DICOM_ROOT, 
                    model_path=MODEL_PLACE, check_run_dir=CHECK_RUN_DIR,
                    combined_groups=None, ):
    """
    Generate a report on the data distribution and check if the framework is working as expected.

    Args:
        xml_path (Path): path to the XML directory containing NLMCXR reports
        dicom_path (Path): path to the DICOM root directory
        model_path (Path): path to the models directory
        check_run_dir (Path): path to the check run directory
        combined_groups (dict): dictionary containing the combined groups of labels

    Returns:
        None

    Notes:
        This function is used to debug the data distribution and check the mean/std of the image features.
        It will generate a report on the number of normal and abnormal cases, as well as the average number of labels per image.
        It will also generate plots of the image features and label distributions.
    """
    log_file = check_run_dir / "log.txt"
    open(log_file, "w").close()
    # Prepare a tokenizer for decoding
    message_xml = f"XML DIR exists: {xml_path.exists()} -> {xml_path}"
    message_dcm = f"DICOM DIR exists: {dicom_path.exists()} -> {dicom_path}"

    log_and_print(message_xml, log_file=log_file)
    log_and_print(message_dcm, log_file=log_file)
    tokenizer = load_hf_model_or_local("emilyalsentzer/Bio_ClinicalBERT", local_dir=model_path, is_tokenizer=True)

    if combined_groups is None:
        combined_groups = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
        }

    label_names = sorted(combined_groups.keys())
    normal_idx = label_names.index("Normal")

    # Parse records
    records = parse_openi_xml(str(xml_path), str(dicom_path), combined_groups=combined_groups)
    log_and_print("Loaded records:", len(records), log_file=log_file)

    for r in records:
        vec = r["labels"]
        r["is_normal"] = vec[normal_idx] == 1 and sum(vec) == 1
        r["is_abnormal"] = any(vec[i] for i in range(len(vec)) if i != normal_idx)

    label_counts = analyze_label_distribution(
        records,
        label_field='labels',
        label_names=label_names
    )

    log_and_print(f"Found {len(label_counts)} active labels:", log_file=log_file)
    for label, count in label_counts.most_common():
        log_and_print(f"  • {label:20s} — {count} cases", log_file=log_file)

    # Compute mean/std with RawStatDataset
    ds = RawStatDataset(records[:100])
    dl = DataLoader(ds, batch_size=16, num_workers=4, pin_memory=True)
    sum_, sum_sq, count = 0.0, 0.0, 0
    for batch in dl:
        b = batch.float()
        sum_   += b.sum().item()
        sum_sq += (b*b).sum().item()
        count  += b.numel()
    mean = sum_ / count
    std  = ((sum_sq/count) - mean**2)**0.5
    log_and_print(f"Computed mean={mean:.4f}, std={std:.4f}", log_file=log_file)

    # Build your multimodal DataLoader
    loader = build_dataloader(records, batch_size=4, mean=mean, std=std)

    # Get one batch
    batch = next(iter(loader))
    imgs, ids, masks = batch['image'], batch['input_ids'], batch['attn_mask']

    # Instantiate Backbones
    SWIN_CKPT = BASE_DIR / 'models' / 'swin_checkpoint.safetensors'
    BERT_DIR  = BASE_DIR / 'models' / 'clinicalbert_local/'
    backbones = Backbones(
        pretrained=True,
        swin_checkpoint_path=SWIN_CKPT,
        bert_local_dir=BERT_DIR
    )

    # Forward‐pass through Backbones
    (img_global, img_region), txt_feats = backbones(imgs, ids, masks)

    log_and_print(f"Global Image feats:", img_global.shape,  " NaNs?", torch.isnan(img_global).any(), log_file=log_file)
    log_and_print(f"Patch Image feats: ", img_region.shape, " NaNs?", torch.isnan(img_region).any(), log_file=log_file)
    log_and_print(f"Pooled Text feats: ", txt_feats.shape,  " NaNs?", torch.isnan(txt_feats).any(), log_file=log_file)

    # See some norm mean 
    log_and_print("Global‑img norm mean:", img_global.norm(dim=1).mean().item(), log_file=log_file)
    log_and_print("Patch‑img  norm mean:", img_region.norm(dim=2).mean().item(), log_file=log_file)   # norm over C, then mean over patches&batch
    log_and_print("Text‑feat  norm mean:", txt_feats.norm(dim=1).mean().item(), log_file=log_file)

    # Debug DICOM
    plot_dicom_debug(records[0]['dicom_path'], log_file=log_file)

    log_and_print(f"\n--- Report Text ---", log_file=log_file)
    log_and_print(f"{records[0]['report_text']}", log_file=log_file)
    log_and_print(f"\n--- Label Vector ---", log_file=log_file)
    log_and_print(f"{records[0]['labels']}", log_file=log_file)  
    log_and_print(f"Total records parsed: {len(records)}", log_file=log_file)
    total_label_hits = sum(sum(r['labels']) for r in records)
    log_and_print(f"Total labels across all records: {total_label_hits}" , log_file=log_file)
    avg_per_image = total_label_hits / len(records)
    log_and_print(f"Average labels per image: {avg_per_image:.2f}", log_file=log_file)
    n_normals = sum(r['is_normal'] for r in records)
    n_abnormals = sum(r['is_abnormal'] for r in records)
    log_and_print(f"Normal cases: {n_normals}", log_file=log_file)
    log_and_print(f"Abnormal cases: {n_abnormals}", log_file=log_file)

    # Debug DICOM ranges
    B = imgs.size(0)
    dp = DICOMImagePreprocessor(augment=False)
    for i in range(B):
        arr = dp.load_raw_array(records[i]['dicom_path'])
        log_and_print(f"[WIN] img {i}  min={arr.min():.4f}, max={arr.max():.4f}", log_file=log_file)
        plt.imshow(arr, cmap='gray', vmin=arr.min(), vmax=arr.max())
        plt.axis('off')
        plt.savefig(check_run_dir /f"img{i}.png")
        plt.close()
end src\Evaluate\data_phrase_check.py

src\Evaluate\diversity_retrieval_report.py
import json
from pathlib import Path
import numpy as np
import torch
from transformers import AutoTokenizer
from Helpers import (
    compare_maps,
    to_numpy,
    safe_unpack_topk,
    heatmap_to_base64_overlay,
    resize_to_match,
    save_b64_map,
    find_dicom_file,
    load_report_lookup_via_parser,
)
from Retrieval.retrieval import make_retrieval_engine
from Helpers import Config
from Model import MultiModalRetrievalModel
from DataHandler import DICOMImagePreprocessor
from LabelData import normal_groups, disease_groups, finding_groups, symptom_groups

BASE_DIR = Path(__file__).resolve().parent.parent.parent
OUT_DIR = BASE_DIR / "retrieval_diversity_score" / "retrieval_reports"
OUT_DIR.mkdir(parents=True, exist_ok=True)
OVERLAY_DIR = OUT_DIR / "overlays"
OVERLAY_DIR.mkdir(parents=True, exist_ok=True)
CONFIG_DIR = BASE_DIR / "config"
CKPT_PATH = BASE_DIR / "checkpoints" / "model_best.pt"
EMBEDDINGS_DIR = BASE_DIR / "embeddings"
MODEL_DIR = BASE_DIR / "models"
XML_DIR = BASE_DIR / "data" / "openi" / "xml" / "NLMCXR_reports" / "ecgen-radiology"
DICOM_ROOT = BASE_DIR / "data" / "openi" / "dicom"
OUTPUT_DIR = BASE_DIR / "outputs"
LABEL_CSV = BASE_DIR / "outputs" / "openi_labels_final.csv"

cfg = Config.load(CONFIG_DIR / "config.yaml")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[loader] device = {device}")

# report lookup
report_lookup = load_report_lookup_via_parser(XML_DIR, DICOM_ROOT)

# retrieval engine
embeddings_feat = EMBEDDINGS_DIR / "train_joint_embeddings.npy"
embeddings_ids  = EMBEDDINGS_DIR / "train_ids.json"
if not embeddings_feat.exists() or not embeddings_ids.exists():
    raise FileNotFoundError(f"Embeddings or ids not found at {embeddings_feat} / {embeddings_ids}")

retriever = make_retrieval_engine(
    features_path=str(embeddings_feat),
    ids_path=str(embeddings_ids),
    method="dls",
    link_threshold=0.5,
    max_links=10
)

# image preprocessor & tokenizer
preproc = DICOMImagePreprocessor()
tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT", cache_dir=str(MODEL_DIR / "clinicalbert"))
combined_groups = {
    **disease_groups,
    **finding_groups,
    **symptom_groups,
    **normal_groups
    }
label_cols = sorted(combined_groups.keys())

model = MultiModalRetrievalModel(
    joint_dim=cfg.joint_dim,
    num_heads=cfg.num_heads,
    num_fusion_layers=cfg.num_fusion_layers,
    num_classes=len(label_cols),
    fusion_type=cfg.fusion_type,
    swin_ckpt_path=MODEL_DIR / "swin_checkpoint.safetensors",
    bert_local_dir= MODEL_DIR / "clinicalbert_local",
    checkpoint_path=str(CKPT_PATH),
    use_shared_ffn=cfg.use_shared_ffn,
    use_cls_only=cfg.use_cls_only,
    device=device,
    retriever=retriever
).to(device)
model.eval()

# config
TOPK = 5
TOPK_FRACS = [0.05, 0.20]  # for IoU comparisons

# Which query ids to use? e.g., test split
with open(BASE_DIR / "splited_data" / "test_split_ids.json") as f:
    query_ids = json.load(f)

# (re)load report lookup to ensure availability
report_lookup = load_report_lookup_via_parser(
    BASE_DIR / "data" / "openi" / "xml" / "NLMCXR_reports" / "ecgen-radiology",
    BASE_DIR / "data" / "openi" / "dicom",
)

results = []
device = None
try:
    device = next(model.parameters()).device
except Exception:
    device = torch.device("cpu")
model.eval()

# canonical attention keys
ATT_VARIANTS = [
    "txt2img", "img2txt",
    "comb", "comb_img", "comb_txt",
    "final_patch_map", "final_token_map",
    "att_txt_tensor", "att_img_tensor", "att_comb_tensor"
]

def _to_numpy_map(d: dict):
    """Convert all tensor-like values in dict to numpy arrays (leaves None alone)."""
    if not d:
        return {}
    out = {}
    for k, v in d.items():
        if v is None:
            out[k] = None
        else:
            try:
                out[k] = to_numpy(v)
            except Exception:
                try:
                    out[k] = np.array(v)
                except Exception:
                    out[k] = None
    return out

count = 0
for qid in query_ids:
    try:
        # get query report and dcm
        q_report = report_lookup.get(qid, "")
        if q_report == "":
            print(f"[WARN] query {qid} no report, skipping")
            continue

        q_dcm_path = find_dicom_file(qid)
        if q_dcm_path is None:
            print(f"[WARN] query {qid} missing DICOM, skipping")
            continue

        q_tensor = preproc(q_dcm_path).unsqueeze(0).to(device)

        # tokenize query text
        tokens = tokenizer(
            q_report or "",
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=cfg.text_dim,
        )
        txt_ids = tokens.input_ids.to(device)
        txt_mask = tokens.attention_mask.to(device)

        # call model.predict (ask for explanations)
        out = model.predict(q_tensor, txt_ids, txt_mask, K=TOPK, explain=True)

        # query maps
        att_maps_q = _to_numpy_map(out.get("attention_map", {}) or {})
        ig_maps_q = out.get("ig_maps", {}) or {}
        ig_maps_q = {int(k): to_numpy(v) for k, v in ig_maps_q.items()} if ig_maps_q else {}
        gradcam_maps_q = out.get("gradcam_maps", {}) or {}
        gradcam_maps_q = {int(k): to_numpy(v) for k, v in gradcam_maps_q.items()} if gradcam_maps_q else {}

        # quick debug
        present_att_keys = [k for k in ATT_VARIANTS if k in att_maps_q and att_maps_q[k] is not None]
        if not present_att_keys:
            print(f"[WARN] Query {qid}: no usable attention maps; keys found: {list(att_maps_q.keys())}")
        else:
            print(f"[DEBUG] Query {qid}: attention keys: {present_att_keys}")

        # determine main target
        main_target = None
        topk_idx = safe_unpack_topk(out.get("topk_idx", []))
        if topk_idx:
            try:
                main_target = int(topk_idx[0])
            except Exception:
                main_target = None
        if main_target is None and len(ig_maps_q) > 0:
            main_target = list(ig_maps_q.keys())[0]

        # prepare query image vis
        q_arr = q_tensor.squeeze().cpu().numpy().astype(np.float32)
        arr_max, arr_min = np.nanmax(q_arr), np.nanmin(q_arr)
        if np.isfinite(arr_max - arr_min) and (arr_max - arr_min) > 0:
            img_vis = ((q_arr - arr_min) / (arr_max - arr_min)) * 255.0
        else:
            img_vis = np.clip(q_arr, 0, 1) * 255.0
        img_vis = img_vis.astype(np.uint8)

        # helper to save overlay
        def _maybe_overlay_and_save(img, amap, qid, rid, tag):
            if amap is None:
                return None
            try:
                b64 = heatmap_to_base64_overlay(img, amap, alpha=0.45)
                return save_b64_map(b64, qid, rid, tag)
            except Exception as e:
                print(f"[WARN] overlay save failed for {qid}/{rid}/{tag}: {e}")
                return None

        # save query overlays
        attn_q_txt_path = _maybe_overlay_and_save(img_vis, att_maps_q.get("txt2img"), qid, None, "attn_txt")
        attn_q_img_path = _maybe_overlay_and_save(img_vis, att_maps_q.get("img2txt"), qid, None, "attn_img")

        comb_map = None
        for k in ("comb", "comb_img", "comb_txt", "att_comb_tensor", "final_patch_map"):
            if att_maps_q.get(k) is not None:
                comb_map = att_maps_q.get(k)
                break
        attn_q_comb_path = _maybe_overlay_and_save(img_vis, comb_map, qid, None, "attn_comb")

        query_ig_path = _maybe_overlay_and_save(img_vis, ig_maps_q.get(main_target) if main_target is not None else None, qid, None, "ig")
        query_gc_path = _maybe_overlay_and_save(img_vis, gradcam_maps_q.get(main_target) if main_target is not None else None, qid, None, "gradcam")

        # retrieval results
        retrieval_ids = out.get("retrieval_ids", [])[:TOPK]
        retrieval_dists = out.get("retrieval_dists", [])[:TOPK]

        count += 1
        print(f"Query: {qid}, Target: {main_target}, Current: {count}/{len(query_ids)}")

        per_retrieved = []
        retrieved_final_patch_maps = []  # collect maps for retrieval→retrieval
        for rid, dist in zip(retrieval_ids, retrieval_dists):
            rec = {"qid": qid, "rid": rid, "dist": float(dist)}
            try:
                r_report = report_lookup.get(rid, "")
                r_dcm_path = find_dicom_file(rid)
                if r_dcm_path is None:
                    rec["error"] = "missing dcm"
                    per_retrieved.append(rec)
                    continue

                r_tensor = preproc(r_dcm_path).unsqueeze(0).to(device)

                out_ret = model.get_explain_score(r_tensor, txt_ids, txt_mask, main_target)

                att_maps_r = _to_numpy_map(out_ret.get("attention_map", {}) or {})
                ig_maps_r = out_ret.get("ig_maps", {}) or {}
                ig_maps_r = {int(k): to_numpy(v) for k, v in ig_maps_r.items()} if ig_maps_r else {}
                gradcam_maps_r = out_ret.get("gradcam_maps", {}) or {}
                gradcam_maps_r = {int(k): to_numpy(v) for k, v in gradcam_maps_r.items()} if gradcam_maps_r else {}

                img_arr = r_tensor.squeeze().cpu().numpy().astype(np.float32)
                arr_max, arr_min = np.nanmax(img_arr), np.nanmin(img_arr)
                if np.isfinite(arr_max - arr_min) and (arr_max - arr_min) > 0:
                    img_vis_r = ((img_arr - arr_min) / (arr_max - arr_min)) * 255.0
                else:
                    img_vis_r = np.clip(img_arr, 0, 1) * 255.0
                img_vis_r = img_vis_r.astype(np.uint8)

                attn_txt_path = _maybe_overlay_and_save(img_vis_r, att_maps_r.get("txt2img"), qid, rid, "attn_txt")
                attn_img_path = _maybe_overlay_and_save(img_vis_r, att_maps_r.get("img2txt"), qid, rid, "attn_img")

                comb_map_r = None
                for k in ("comb", "comb_img", "comb_txt", "att_comb_tensor", "final_patch_map"):
                    if att_maps_r.get(k) is not None:
                        comb_map_r = att_maps_r.get(k)
                        break
                attn_comb_path = _maybe_overlay_and_save(img_vis_r, comb_map_r, qid, rid, "attn_comb")

                ig_path = _maybe_overlay_and_save(img_vis_r, ig_maps_r.get(main_target) if main_target is not None else None, qid, rid, "ig")
                gc_path = _maybe_overlay_and_save(img_vis_r, gradcam_maps_r.get(main_target) if main_target is not None else None, qid, rid, "gradcam")

                # compute query→retrieval metrics
                cross = {}
                for key in ["txt2img", "img2txt", "comb", "comb_img", "comb_txt", "final_patch_map"]:
                    qmap = att_maps_q.get(key)
                    rmap = att_maps_r.get(key)
                    if qmap is not None and rmap is not None:
                        if qmap.shape == rmap.shape:
                            cm5 = compare_maps(qmap, rmap, topk_frac=TOPK_FRACS[0])
                            cm20 = compare_maps(qmap, rmap, topk_frac=TOPK_FRACS[1])
                            cross.update({
                                f"{key}_pearson": float(cm5.get("pearson", 0.0)),
                                f"{key}_spearman": float(cm5.get("spearman", 0.0)),
                                f"{key}_iou_5": float(cm5.get("iou_top5pct", 0.0)),
                                f"{key}_iou_20": float(cm20.get("iou_top20pct", 0.0)),
                            })
                        else:
                            try:
                                qmap_resized = resize_to_match(qmap, rmap)
                                # Normalize
                                def _norm01(x):
                                    xm, xM = np.nanmin(x), np.nanmax(x)
                                    if np.isfinite(xM - xm) and (xM - xm) > 0:
                                        return (x - xm) / (xM - xm)
                                    else:
                                        return x - xm
                                qn = _norm01(qmap_resized)
                                rn = _norm01(rmap.astype(np.float32))

                                cm5 = compare_maps(qn, rn, topk_frac=TOPK_FRACS[0])
                                cm20 = compare_maps(qn, rn, topk_frac=TOPK_FRACS[1])
                                cross.update({
                                    f"{key}_pearson": float(cm5.get("pearson", 0.0)),
                                    f"{key}_spearman": float(cm5.get("spearman", 0.0)),
                                    f"{key}_iou_5": float(cm5.get("iou_top5pct", 0.0)),
                                    f"{key}_iou_20": float(cm20.get("iou_top20pct", 0.0)),
                                })
                            except Exception as e:
                                print(f"[WARN] compare_maps resize/compare failed for {qid}/{rid}/{key}: {e}")
                                cross[f"{key}_error"] = str(e)

                # IG vs IG
                if ig_maps_q and ig_maps_r and main_target in ig_maps_q and main_target in ig_maps_r:
                    if ig_maps_q[main_target].shape == ig_maps_r[main_target].shape:
                        cm5 = compare_maps(ig_maps_q[main_target], ig_maps_r[main_target], topk_frac=TOPK_FRACS[0])
                        cm20 = compare_maps(ig_maps_q[main_target], ig_maps_r[main_target], topk_frac=TOPK_FRACS[1])
                        cross.update({
                            "ig_ig_pearson": float(cm5.get("pearson", 0.0)),
                            "ig_ig_spearman": float(cm5.get("spearman", 0.0)),
                            "ig_ig_iou_5": float(cm5.get("iou_top5pct", 0.0)),
                            "ig_ig_iou_20": float(cm20.get("iou_top20pct", 0.0)),
                        })

                # GradCAM vs GradCAM
                if gradcam_maps_q and gradcam_maps_r and main_target in gradcam_maps_q and main_target in gradcam_maps_r:
                    if gradcam_maps_q[main_target].shape == gradcam_maps_r[main_target].shape:
                        cm5 = compare_maps(gradcam_maps_q[main_target], gradcam_maps_r[main_target], topk_frac=TOPK_FRACS[0])
                        cm20 = compare_maps(gradcam_maps_q[main_target], gradcam_maps_r[main_target], topk_frac=TOPK_FRACS[1])
                        cross.update({
                            "gc_gc_pearson": float(cm5.get("pearson", 0.0)),
                            "gc_gc_spearman": float(cm5.get("spearman", 0.0)),
                            "gc_gc_iou_5": float(cm5.get("iou_top5pct", 0.0)),
                            "gc_gc_iou_20": float(cm20.get("iou_top20pct", 0.0)),
                        })

                rec.update({
                    "report": r_report,
                    "attn_txt_path": attn_txt_path,
                    "attn_img_path": attn_img_path,
                    "attn_comb_path": attn_comb_path,
                    "ig_path": ig_path,
                    "gradcam_path": gc_path,
                    "compare_metrics": cross,
                })

                # keep final_patch_map for retrieval to retrieval score
                if att_maps_r.get("final_patch_map") is not None:
                    retrieved_final_patch_maps.append(att_maps_r["final_patch_map"])

            except Exception as e:
                rec["error"] = str(e)

            per_retrieved.append(rec)

        # compute retrieval to retrieval diversity
        if len(retrieved_final_patch_maps) > 1:
            overlaps = []
            for i in range(len(retrieved_final_patch_maps)):
                for j in range(i + 1, len(retrieved_final_patch_maps)):
                    cm = compare_maps(
                        retrieved_final_patch_maps[i],
                        retrieved_final_patch_maps[j],
                        topk_frac=TOPK_FRACS[0],
                    )
                    overlaps.append(cm.get("iou_top5pct", 0.0))
            avg_overlap = float(np.mean(overlaps)) if overlaps else 0.0
            avg_diversity = 1.0 - avg_overlap
        else:
            avg_overlap, avg_diversity = None, None

        results.append({
            "qid": qid,
            "query_report": q_report,
            "retrieval": per_retrieved,
            "retrieval_overlap_iou5": avg_overlap,
            "retrieval_diversity_score": avg_diversity,
        })

    except Exception as e:
        print(f"[WARN] query {qid} failed: {e}")

# save the JSON
with open(OUT_DIR / "retrieval_report.json", "w") as f:
    json.dump(results, f, indent=2)

print("Saved report:", OUT_DIR / "retrieval_report.json")

end src\Evaluate\diversity_retrieval_report.py

src\Evaluate\EDA_after_split.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import numpy as np
import matplotlib.pyplot as plt
import json
from DataHandler import parse_openi_xml
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
import os
import pandas as pd

try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent
XML_DIR = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
SPLIT_DIR = BASE_DIR / 'splited_data'
LABEL_CSV = BASE_DIR / 'outputs' / 'openi_labels_final_cleaned.csv'

def avg_labels_per_image(records, field='labels'):
    mat = np.array([r[field] for r in records])
    return mat.sum(axis=1).mean()

def label_distribution(records, field='labels'):
    mat = np.array([r[field] for r in records])
    return mat.sum(axis=0)

def get_eda_after_split(xml_dir=XML_DIR, dicom_root=DICOM_ROOT, 
                        split_dir=SPLIT_DIR, label_csv=LABEL_CSV, 
                        combined_groups=None):
    # Label setup
    combined_group_temp = {
        **disease_groups, 
        **normal_groups,
        **finding_groups,
        **symptom_groups
        }
    combined_groups = combined_group_temp if combined_groups is None else combined_groups
    label_cols = sorted(combined_groups.keys())

    # Load final labels
    labels_df = pd.read_csv(label_csv).set_index("id")

    # Parse raw XML records
    raw_records = parse_openi_xml(xml_dir, dicom_root, combined_groups=combined_groups)

    # Merge labels into parsed records
    labeled_records = []
    for rec in raw_records:
        rec_id = rec["id"]
        if rec_id in labels_df.index:
            rec["labels"] = labels_df.loc[rec_id, label_cols].tolist()
            labeled_records.append(rec)

    # Load split IDs
    with open(split_dir / 'train_split_ids.json') as f: train_ids = set(json.load(f))
    with open(split_dir / 'val_split_ids.json')   as f: val_ids   = set(json.load(f))
    with open(split_dir / 'test_split_ids.json')  as f: test_ids  = set(json.load(f))

    # Filter records into splits
    train_records = [r for r in labeled_records if r['id'] in train_ids]
    val_records   = [r for r in labeled_records if r['id'] in val_ids]
    test_records  = [r for r in labeled_records if r['id'] in test_ids]

    print(f"Train size: {len(train_records)}, Val size: {len(val_records)}, Test size: {len(test_records)}")

    # Compute distributions
    train_counts = label_distribution(train_records)
    val_counts   = label_distribution(val_records)
    test_counts  = label_distribution(test_records)

    x = np.arange(len(label_cols))
    plt.figure(figsize=(14,6))
    plt.bar(x-0.3, train_counts, width=0.25, label='Train')
    plt.bar(x,      val_counts,   width=0.25, label='Val')
    plt.bar(x+0.3,  test_counts,  width=0.25, label='Test')
    plt.xticks(x, label_cols, rotation=45, ha="right")
    plt.ylabel("Number of Cases")
    plt.title("Final (Verified) Disease Label Distribution Across Splits")
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Print avg labels/image
    print(f"Average labels/image:")
    print(f"  Train: {avg_labels_per_image(train_records):.2f}")
    print(f"  Val:   {avg_labels_per_image(val_records):.2f}")
    print(f"  Test:  {avg_labels_per_image(test_records):.2f}")

    # Save to Markdown report
    with open(SPLIT_DIR / "split_stats_verified.md", "w") as f:
        f.write(f"# Final Labeled Data Split Summary (80/10/10)\n\n")
        f.write(f"- Train records: {len(train_records)}\n")
        f.write(f"- Val   records: {len(val_records)}\n")
        f.write(f"- Test  records: {len(test_records)}\n\n")
        for name, t, v, te in zip(label_cols, train_counts, val_counts, test_counts):
            f.write(f"- **{name}** -> train: {t}, val: {v}, test: {te}\n")
        f.write(f"\n- Avg labels/image -> train: {avg_labels_per_image(train_records):.2f}, "
                f"val: {avg_labels_per_image(val_records):.2f}, "
                f"test: {avg_labels_per_image(test_records):.2f}\n")
end src\Evaluate\EDA_after_split.py

src\Evaluate\EDA_before_split.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from collections import defaultdict
from DataHandler import DICOMImagePreprocessor, parse_openi_xml
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups

# Resolve paths...
try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

XML_DIR     = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT  = BASE_DIR / 'data' / 'openi' / 'dicom'
OUTPUT_FILE = BASE_DIR / "outputs" / "openi_labels_final.csv"
OUTPUT_DROP_ZERO = BASE_DIR / "outputs"
MODEL_PLACE = BASE_DIR / "models"
EDA_DIR     = BASE_DIR / "eda_data"
os.environ['TRANSFORMERS_CACHE'] = str(MODEL_PLACE)


def get_eda_before_split(xml_dir=XML_DIR, dicom_root=DICOM_ROOT, eda_dir=EDA_DIR,
                            output_file=OUTPUT_FILE, combine_groups=None,
                            drop_zero=True, save_cleaned=True,
                            max_show=10, output_drop_zero=OUTPUT_DROP_ZERO     
                            ):
    """
    Compute various statistics and plots for the OpenI dataset.

    Parameters
    ----------
    xml_dir : str, optional
        Path to the OpenI XML directory.
    dicom_root : str, optional
        Path to the OpenI DICOM directory.
    eda_dir : str, optional
        Path to the directory where the EDA plots will be saved.
    output_file : str, optional
        Path to the file which has the corrected labels generated and mapped correctly after llm verification.
    combine_groups : dict, optional
        Dictionary mapping label group names to their corresponding sub-labels.
    drop_zero : bool, optional
        If True, drop the zero-label reports and save the cleaned dataset.
    save_cleaned : bool, optional
        If True, save the cleaned dataset (with zero-label reports dropped).
    max_show : int, optional
        Maximum number of reports to show for each statistic.
    output_drop_zero : str, optional
        Path to the directory where the cleaned dataset backup will be saved.

    Returns
    -------
    None

    Notes
    -----
    This function assumes that the OpenI dataset has been preprocessed and that the corrected labels have been saved to a CSV file.

    The function first loads the corrected labels and computes the label prevalence, then plots a bar chart of the label prevalence. It then computes the label co-occurrence matrix and plots a heatmap of the matrix. Finally, it computes the report length distribution and plots a histogram of the distribution.

    If drop_zero is True, the function drops the zero-label reports and saves the cleaned dataset to a CSV file. If save_cleaned is True, the function also saves a backup of the dataset with zero-label reports to a CSV file.

    The function returns None.
    """

    eda_dir.mkdir(parents=True, exist_ok=True)
    log_file = eda_dir / "eda_before_split_summary.txt"

    # utility to log and print
    def log_and_print(msg=""):
        print(msg)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(str(msg) + "\n")

    # clear old log file
    open(log_file, "w").close()

    combined_groups_temp = {
        **disease_groups,
        **finding_groups,
        **symptom_groups,
        **normal_groups
    }
    combined_groups = combined_groups_temp if combine_groups is None else combine_groups
    label_names = sorted(combined_groups.keys())

    # --------------------------
    # Load data
    # --------------------------
    if output_file.exists():
        log_and_print(f"[INFO] Using corrected labels from {output_file}")
        df = pd.read_csv(output_file)
        label_matrix = df[label_names].values
        use_records = False
    else:
        log_and_print("[INFO] Corrected CSV not found, falling back to parsing XML")
        records = parse_openi_xml(xml_dir, dicom_root, combined_groups=combined_groups)
        label_matrix = np.array([rec['labels'] for rec in records])
        df = pd.DataFrame({
            'id':          [rec['id'] for rec in records],
            'report_text': [rec['report_text'] for rec in records],
            **{name: [vec[i] for vec in (rec['labels'] for rec in records)]
               for i, name in enumerate(label_names)}
        })
        use_records = True
    
    if "report_text" in df.columns:
        df.rename(columns={"report_text": "text"}, inplace=True)

    # --------------------------
    # Normal vs abnormal counts
    # --------------------------
    normal_idx = label_names.index("Normal")
    n_strict_normal = sum(
        vec[normal_idx] == 1 and sum(vec) == 1
        for vec in label_matrix
    )
    n_abnormal = sum(
        any(vec[i] for i in range(len(vec)) if i != normal_idx)
        for vec in label_matrix
    )
    log_and_print(f"Strict Normal samples (only 'Normal' = 1): {n_strict_normal}")
    log_and_print(f"Abnormal samples (any disease group = 1): {n_abnormal}")

    plt.figure()
    plt.pie([n_strict_normal, n_abnormal],
            labels=["Normal", "Abnormal"],
            autopct="%1.1f%%", startangle=90)
    plt.title("Normal vs Abnormal Cases")
    plt.axis("equal")
    plt.tight_layout()
    plt.savefig(eda_dir / "normal_vs_abnormal.png")
    plt.close()

    if use_records:
        # Check for shared reports
        report_map = defaultdict(list)
        for rec in records:
            report_map[rec['report_text']].append(rec['id'])

        shared = [ids for ids in report_map.values() if len(ids) > 1]
        log_and_print(f"Unique reports: {len(report_map)}")
        log_and_print(f"Reports shared by multiple images: {len(shared)}")
        log_and_print(f"Avg images per reused report: {np.mean([len(ids) for ids in shared]):.2f}")

        # Debug a sample DICOM
        dp = DICOMImagePreprocessor()
        sample = records[0]
        arr = dp.load_raw_array(sample['dicom_path'])

        plt.figure()
        plt.imshow(arr, cmap="gray")
        plt.title("DICOM Image")
        plt.axis("off")
        plt.show()

        # Print sample report + labels
        log_and_print("--- Report ---")
        log_and_print(sample["report_text"])
        log_and_print("\n--- Labels ---")
        log_and_print({name: val for name, val in zip(label_names, sample["labels"]) if val})

    # Label prevalence bar chart
    label_cols = label_names
    label_counts = df[label_cols].sum().sort_values(ascending=False)

    plt.figure(figsize=(10,6))
    sns.barplot(x=label_counts.values, y=label_counts.index)
    plt.title("Label Prevalence (N reports)")
    plt.xlabel("Count")
    plt.tight_layout()
    plt.savefig(eda_dir / "label_prevalence.png")
    plt.close()

    # Histogram of labels per report
    df['num_labels'] = df[label_cols].sum(axis=1)
    plt.figure(figsize=(6,4))
    sns.histplot(df['num_labels'],
                 bins=range(0, int(df['num_labels'].max())+2), discrete=True)
    plt.title("Labels per Report")
    plt.xlabel("Number of Labels")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.savefig(eda_dir / "labels_per_report.png")
    plt.close()

    # Label co-occurrence heatmap
    co_mat = df[label_cols].T.dot(df[label_cols])
    plt.figure(figsize=(12,10))
    sns.heatmap(co_mat, annot=False, cmap="Blues", fmt="d")
    plt.title("Label Co-occurrence Matrix")
    plt.tight_layout()
    plt.savefig(eda_dir / "label_cooccurrence.png")
    plt.close()

    # Report length analysis
    df['word_count'] = df['text'].str.split().map(len)
    plt.figure(figsize=(6,4))
    sns.histplot(df['word_count'], bins=30)
    plt.title("Report Word Count Distribution")
    plt.xlabel("Words per Report")
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.savefig(eda_dir / "report_word_count.png")
    plt.close()

    # Boxplot for top-5 frequent labels
    top5 = label_counts.index[:5].tolist()
    melted = pd.melt(df, id_vars=['word_count'], value_vars=top5,
                     var_name='label', value_name='has_label')
    melted_pos = melted[melted['has_label'] == 1]

    plt.figure(figsize=(10,6))
    sns.boxplot(data=melted_pos, x='label', y='word_count')
    plt.title("Report Length by Top-5 Labels (only positive cases)")
    plt.xlabel("Label")
    plt.ylabel("Words per Report")
    plt.tight_layout()
    plt.savefig(eda_dir / "report_length_by_label.png")
    plt.close()

    # Positive:Negative ratio
    N = len(df)
    pos_neg = pd.DataFrame({
        'label': label_counts.index,
        'pos':   label_counts.values,
        'neg':   N - label_counts.values
    })
    pos_neg['pos_neg_ratio'] = pos_neg['pos'] / pos_neg['neg']

    plt.figure(figsize=(10,6))
    sns.barplot(x='pos_neg_ratio', y='label',
                data=pos_neg.sort_values('pos_neg_ratio'))
    plt.title("Positive:Negative Ratio by Label")
    plt.xlabel("Positive / Negative")
    plt.tight_layout()
    plt.savefig(eda_dir / "pos_neg_ratio.png")
    plt.close()

    log_and_print("\nClass imbalance summary:")
    log_and_print(pos_neg.sort_values('pos_neg_ratio').to_string(index=False, float_format="%.3f"))

    # Print out the zero‐label reports 
    zero_idxs = df.index[df['num_labels'] == 0].tolist()
    log_and_print(f"\nFound {len(zero_idxs)} reports with 0 labels. Showing up to 10 of them:\n")

    max_show = 10
    for idx in zero_idxs[:max_show]:
        row = df.loc[idx]
        log_and_print(f"--- Report ID: {row['id']} (index={idx}) ---")
        log_and_print(row['text'])
        log_and_print("-" * 80)

    # Optional: drop and save
    if drop_zero and len(zero_idxs) > 0:
        log_and_print(f"\n[INFO] Dropping {len(zero_idxs)} zero-label reports.")

        # Keep a backup before dropping
        if save_cleaned:
            backup_path = output_drop_zero / "openi_labels_final_with_zero.csv"
            df.to_csv(backup_path, index=False)
            log_and_print(f"[INFO] Backup with zero-labels saved to {backup_path}")

        # Drop and reset index
        df = df[df['num_labels'] > 0].reset_index(drop=True)

        if save_cleaned:
            cleaned_path = output_drop_zero / "openi_labels_final_cleaned.csv"
            df.to_csv(cleaned_path, index=False)
            log_and_print(f"[INFO] Cleaned dataset saved to {cleaned_path}")

end src\Evaluate\EDA_before_split.py

src\Evaluate\eval_on_test.py
import torch
import numpy as np
import pandas as pd
import json
from pathlib import Path
from sklearn.metrics import f1_score, roc_auc_score, precision_recall_fscore_support

from Helpers import Config
from Model import MultiModalRetrievalModel
from DataHandler import build_dataloader, parse_openi_xml
from LabelData import disease_groups, normal_groups

BASE_DIR = Path(__file__).resolve().parent.parent.parent
CONFIG_PATH = BASE_DIR / "configs" / "config.yaml"
CKPT_PATH   = BASE_DIR / "checkpoints" / "model_best.pt"
SPLIT_DIR  = BASE_DIR / "splited_data"
XML_DIR    = BASE_DIR / "data" / "openi" / "xml" / "NLMCXR_reports" / "ecgen-radiology"
DICOM_ROOT = BASE_DIR / "data" / "openi" / "dicom"
LABELS_CSV = BASE_DIR / "outputs" / "openi_labels_final.csv"

def eval_on_test():
    # Load config and device
    cfg = Config.load(CONFIG_PATH)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load label columns
    label_cols = list(disease_groups.keys()) + list(normal_groups.keys())

    # Load test IDs and filter records
    with open(SPLIT_DIR / "test_split_ids.json") as f:
        test_ids = set(json.load(f))

    df_labels = pd.read_csv(LABELS_CSV).set_index("id")
    parsed_records = parse_openi_xml(XML_DIR, DICOM_ROOT)

    # Align records
    records = []
    for rec in parsed_records:
        rec_id = rec["id"]
        if rec_id in test_ids and rec_id in df_labels.index:
            try:
                label_vec = df_labels.loc[rec_id, label_cols].astype(int).tolist()
                if len(label_vec) != len(label_cols):
                    print(f"[WARN] Skipping {rec_id}: label vector length mismatch")
                    continue
                rec["labels"] = label_vec
                records.append(rec)
            except Exception as e:
                print(f"[WARN] Skipping {rec_id}: {e}")


    print(f"[INFO] Loaded {len(records)} test records")

    # Build test dataloader
    test_loader = build_dataloader(
        records=records,
        batch_size=cfg.batch_size,
        mean=0.5,
        std=0.25,
        shuffle=False
    )

    # Load model
    model = MultiModalRetrievalModel(
        joint_dim=cfg.joint_dim,
        num_heads=cfg.num_heads,
        num_fusion_layers=cfg.num_fusion_layers,
        num_classes=len(label_cols),
        fusion_type=cfg.fusion_type,
        swin_ckpt_path=BASE_DIR / "models" / "swin_checkpoint.safetensors",
        bert_local_dir=BASE_DIR / "models" / "clinicalbert_local",
        checkpoint_path=str(CKPT_PATH),
        device=device,
        use_shared_ffn=cfg.use_shared_ffn,
        use_cls_only=cfg.use_cls_only,
        training=False
    ).to(device)

    model.eval()

    # Run on test set
    y_true, y_prob = [], []

    with torch.no_grad():
        for batch in test_loader:
            img  = batch["image"].to(device)
            ids  = batch["input_ids"].to(device)
            mask = batch["attn_mask"].to(device)
            lbls = batch["labels"].cpu().numpy()

            output = model.predict(img, ids, mask, threshold=0.5, explain=False)
            probs = output["probs"]

            y_true.extend(lbls)
            y_prob.extend(probs)

    y_true = np.array(y_true)
    y_prob = np.array(y_prob)
    y_pred = (y_prob > 0.5).astype(int)

    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)
    auroc = [
        roc_auc_score(y_true[:, i], y_prob[:, i])
        if np.any(y_true[:, i]) else 0.0
        for i in range(y_true.shape[1])
    ]

    # Print per-label results
    print(f"{'Label':40s}  Prec   Rec    F1     AUROC")
    for i, label in enumerate(label_cols):
        print(f"{label:40s}  {prec[i]:.3f}  {rec[i]:.3f}  {f1[i]:.3f}  {auroc[i]:.3f}")

    # Macro average
    macro_f1 = np.mean(f1)
    macro_auc = np.mean(auroc)
    print(f"\n[MACRO] F1: {macro_f1:.4f}  AUROC: {macro_auc:.4f}")

if __name__ == "__main__":
    eval_on_test()
end src\Evaluate\eval_on_test.py

src\Evaluate\finalOutputDataEDA.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import pandas as pd
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
import matplotlib.pyplot as plt
import seaborn as sns

try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

ORIGINAL_FILE = BASE_DIR / "outputs" / "openi_labels.csv"
FINAL_FILE    = BASE_DIR / "outputs" / "openi_labels_final_cleaned.csv"

def compare_final_to_original(orig_path=ORIGINAL_FILE, final_path=FINAL_FILE, eda_dir=BASE_DIR / "eda_data", combined_groups=None):
    eda_dir.mkdir(parents=True, exist_ok=True)
    log_file = eda_dir / "compare_original_to_final_summary.txt"
    # utility to log and print
    def log_and_print(msg=""):
        print(msg)
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(str(msg) + "\n")

    # clear old log file
    open(log_file, "w").close()

    # Load original & final
    orig  = pd.read_csv(orig_path)
    final = pd.read_csv(final_path)

    # Define one‑hot columns
    combined_groups = combined_groups or {**disease_groups, **normal_groups, **finding_groups, **symptom_groups}
    label_cols = list({**combined_groups}.keys())

    final["llm_status"] = final["llm_status"].astype(str).str.strip().str.title()
    status_counts = final["llm_status"].value_counts()
    delta = final[label_cols] - orig[label_cols] # Compute delta = final - orig

    # Summarize absolute counts
    summary = pd.DataFrame({
        "added":     (delta == 1).sum(),
        "removed":   (delta == -1).sum(),
        "unchanged": (delta == 0).sum(),
    })

    # Compute percents relative to total reports
    total = len(delta)
    summary["% added"]   = summary["added"]   / total * 100
    summary["% removed"] = summary["removed"] / total * 100

    # Net-change just for reference
    summary["net_change"] = summary["added"] - summary["removed"]
    summary = summary.sort_values("% added", ascending=False)

    # count how many 1’s per row
    final["n_labels"] = final[label_cols].sum(axis=1)

    # summary stats
    log_and_print(final["n_labels"].describe())
    log_and_print(summary[["added","% added","removed","% removed","net_change"]])
    log_and_print(status_counts)

    # --- Plotting ---

    # Percentage added vs removed (horizontal bar chart)
    fig, ax = plt.subplots(figsize=(8, max(4, len(label_cols)*0.2)))
    summary[["% added", "% removed"]].plot.barh(ax=ax)
    ax.set_xlabel("% of reports")
    ax.set_title("Percentage of Reports Where Each Label Was Added or Removed")
    plt.tight_layout()
    plt.savefig(eda_dir / "percentage_added_removed.png")
    plt.close()

    # Net count change (horizontal bar chart)
    fig, ax2 = plt.subplots(figsize=(8, max(4, len(label_cols)*0.2)))
    summary["net_change"].sort_values().plot.barh(ax=ax2)
    ax2.set_xlabel("Net change (added – removed)")
    ax2.set_title("Net Count Change by Label")
    plt.tight_layout()
    plt.savefig(eda_dir / "net_count_change.png")
    plt.close()

    # LLM Status Distribution
    plt.figure(figsize=(6,5))
    status_counts.plot.bar()
    plt.title("LLM Status Distribution")
    plt.ylabel("Count")
    plt.xlabel("llm_status")
    plt.tight_layout()
    plt.savefig(eda_dir / "llm_status_distribution.png")
    plt.close()

    # histogram
    cooc = final[label_cols].T.dot(final[label_cols])
    plt.figure(figsize=(8, 5))
    final["n_labels"].hist(bins=range(final["n_labels"].max() + 2))
    plt.title("Distribution of Number of Labels per Report")
    plt.xlabel("Number of Labels")
    plt.ylabel("Number of Reports")
    plt.tight_layout()
    plt.savefig(eda_dir / "n_labels_distribution.png")
    plt.close()

    # percentage of reports with label i that also have j
    cooc_norm = cooc.div(cooc.values.diagonal(), axis=0)
    plt.figure(figsize=(12,10))
    sns.heatmap(cooc_norm, cmap="viridis", xticklabels=False, yticklabels=False)
    plt.title("Label Co-occurrence (normalized)")
    plt.tight_layout()
    plt.savefig(eda_dir / "cooccurrence_normalized.png")
    plt.close()

end src\Evaluate\finalOutputDataEDA.py

src\Evaluate\retrieval_diversity_compute.py
import json
from pathlib import Path
from typing import List, Optional, Dict, Any
import pandas as pd
import numpy as np

def load_json(path: str) -> List[Dict[str,Any]]:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"JSON file not found: {path}")
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError("Expected JSON root to be a list of query objects.")
    return data

# ------------------------
# Query → Retrieval parsing
# ------------------------
def flatten_retrievals(data: List[Dict[str,Any]]) -> pd.DataFrame:
    """Flatten the JSON into a long DataFrame: one row per retrieval entry (Q to R)."""
    rows = []
    for q in data:
        qid = q.get("qid")
        qreport = q.get("query_report")
        retrievals = q.get("retrieval", []) or []
        for rank, r in enumerate(retrievals, start=1):
            base = {
                "qid": qid,
                "query_report": qreport,
                "rid": r.get("rid"),
                "dist": r.get("dist"),
                "rank": rank,
                "report": r.get("report"),
                "attn_txt_path": r.get("attn_txt_path"),
                "attn_img_path": r.get("attn_img_path"),
                "attn_comb_path": r.get("attn_comb_path"),
                "ig_path": r.get("ig_path"),
                "gradcam_path": r.get("gradcam_path"),
                "error": r.get("error"),
            }
            cm = r.get("compare_metrics") or {}
            for k, v in cm.items():
                try:
                    base[k] = float(v) if v is not None else np.nan
                except Exception:
                    base[k] = np.nan
            rows.append(base)
    df = pd.DataFrame(rows)
    id_cols = ["qid", "rid", "rank", "dist", "report",
               "query_report", "error",
               "attn_txt_path", "attn_img_path", "attn_comb_path",
               "ig_path", "gradcam_path"]
    metric_cols = [c for c in df.columns if c not in id_cols]
    ordered = id_cols + sorted(metric_cols)
    ordered = [c for c in ordered if c in df.columns]
    return df[ordered]

# ------------------------
# Retrieval → Retrieval parsing
# ------------------------
def flatten_rr(data: List[Dict[str,Any]]) -> pd.DataFrame:
    """
    Flatten retrieval to retrieval metrics at query-level (one row per qid).
    Works with keys like retrieval_overlap_iou5 / retrieval_diversity_score.
    """
    rows = []
    for q in data:
        qid = q.get("qid")
        row = {"qid": qid}
        found = False
        for key in ["retrieval_overlap_iou5", "retrieval_diversity_score"]:
            if key in q:
                try:
                    row[key] = float(q[key]) if q[key] is not None else np.nan
                except Exception:
                    row[key] = np.nan
                found = True
        if found:
            rows.append(row)
    return pd.DataFrame(rows)

# ------------------------
# Utility functions
# ------------------------
def discover_metrics(df: pd.DataFrame) -> List[str]:
    non_metric = {"qid","rid","rank","dist","report","query_report","error",
                  "attn_txt_path","attn_img_path","attn_comb_path","ig_path","gradcam_path"}
    return [c for c in df.columns if c not in non_metric]

def get_metric_scores(df: pd.DataFrame, metric: str, dropna: bool = True) -> pd.DataFrame:
    if metric not in df.columns:
        raise KeyError(f"Metric '{metric}' not found. Available: {discover_metrics(df)}")
    subset = df[["qid","rid","rank","dist","report", metric]].copy()
    subset = subset.rename(columns={metric: "score"})
    if dropna:
        subset = subset[subset["score"].notna()]
    return subset.sort_values(["qid","rank"])

def summary_all_metrics(df: pd.DataFrame) -> pd.DataFrame:
    metrics = discover_metrics(df)
    rows = []
    total = len(df)
    for m in metrics:
        s = pd.to_numeric(df[m], errors="coerce")
        cnt = int(s.count())
        rows.append({
            "metric": m,
            "mean": float(s.mean()) if cnt>0 else np.nan,
            "std": float(s.std()) if cnt>1 else np.nan,
            "median": float(s.median()) if cnt>0 else np.nan,
            "min": float(s.min()) if cnt>0 else np.nan,
            "max": float(s.max()) if cnt>0 else np.nan,
            "count_nonnull": cnt,
            "total_rows": total,
            "pct_missing": 100.0 * (1.0 - cnt/total) if total>0 else np.nan
        })
    return pd.DataFrame(rows).set_index("metric").sort_index()

def aggregate_per_qid(df: pd.DataFrame, metric: str, agg_funcs: Optional[List[str]] = None) -> pd.DataFrame:
    if metric not in df.columns:
        raise KeyError(f"Metric '{metric}' not found.")
    if agg_funcs is None:
        agg_funcs = ['mean','median','std','min','max','count']
    s = df[["qid", metric]].copy()
    s[metric] = pd.to_numeric(s[metric], errors="coerce")
    group = s.groupby("qid").agg({metric: agg_funcs})
    group.columns = ["_".join(col) if isinstance(col, tuple) else col for col in group.columns.values]
    group = group.reset_index()
    counts = df.groupby("qid").size().rename("total_retrieved").reset_index()
    nonnulls = df.groupby("qid")[metric].apply(lambda x: x.notna().sum()).rename("nonnull_count").reset_index()
    group = group.merge(counts, on="qid", how="left").merge(nonnulls, on="qid", how="left")
    group["pct_missing"] = 100.0 * (1.0 - group["nonnull_count"] / group["total_retrieved"])
    return group

def top_k_by_metric(df: pd.DataFrame, metric: str, k: int = 1, higher_is_better: bool = True) -> pd.DataFrame:
    if metric not in df.columns:
        raise KeyError(f"Metric '{metric}' not found.")
    s = df[["qid","rid","rank","dist","report",metric]].copy()
    s[metric] = pd.to_numeric(s[metric], errors="coerce")
    s = s[s[metric].notna()].copy()
    s = s.sort_values(["qid", metric], ascending=[True, not higher_is_better])
    topk = s.groupby("qid").head(k).reset_index(drop=True)
    topk["k_rank"] = topk.groupby("qid").cumcount()+1
    return topk

def save_df(df: pd.DataFrame, outpath: str) -> None:
    Path(outpath).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(outpath, index=False)
    print(f"Saved: {outpath}")

# -----------------------
# CLI
# -----------------------
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Parse retrieval JSON and extract metric results (Q to R and R and R).")
    parser.add_argument("--json", type=str, default="retrieval_diversity_score/retrieval_reports/retrieval_report.json",
                        help="Path to retrieval JSON (list of queries).")
    parser.add_argument("--out-dir", type=str, default="retrieval_diversity_score/retrieval_reports/",
                        help="Directory to save flattened CSV and summaries.")
    args = parser.parse_args()

    data = load_json(args.json)

    # ---- Q→R ----
    df = flatten_retrievals(data)
    print(f"[Q to R] Flattened rows: {len(df)}  unique qids: {df['qid'].nunique() if 'qid' in df else 0}")
    outdir = Path(args.out_dir); outdir.mkdir(parents=True, exist_ok=True)
    save_df(df, str(outdir / "retrieval_metrics_flat.csv"))
    summary = summary_all_metrics(df)
    summary.to_csv(outdir / "retrieval_metrics_summary.csv")
    print(summary.head(20))

    # ---- R↔R ----
    df_rr = flatten_rr(data)
    if not df_rr.empty:
        print(f"[R and R] Rows: {len(df_rr)}  unique qids: {df_rr['qid'].nunique()}")
        save_df(df_rr, str(outdir / "retrieval_retrieval_metrics.csv"))
        summary_rr = summary_all_metrics(df_rr)
        summary_rr.to_csv(outdir / "retrieval_retrieval_summary.csv")
        print(summary_rr.head(20))
    else:
        print("[R and R] No retrieval to retrieval metrics found in JSON.")

    # Example: per-qid aggregation on Q→R metric
    example_metrics = [m for m in ["txt2img_pearson","img2txt_pearson",
                                   "final_patch_map_iou_5","final_patch_map_iou_20"]
                       if m in discover_metrics(df)]
    for m in example_metrics:
        agg = aggregate_per_qid(df, m)
        save_df(agg, str(outdir / f"per_qid_agg__{m}.csv"))

    # Example: top-1 by pearson if exists
    metric_for_topk = "final_patch_map_pearson" if "final_patch_map_pearson" in df.columns else None
    if metric_for_topk:
        top1 = top_k_by_metric(df, metric_for_topk, k=1, higher_is_better=True)
        save_df(top1, str(outdir / f"top1_by__{metric_for_topk}.csv"))
        print("Top-1 sample:")
        print(top1.head(10))

end src\Evaluate\retrieval_diversity_compute.py

src\Evaluate\retrieval_eval.py
import json
import numpy as np
import torch
from pathlib import Path
import pandas as pd
import time
from typing import List
from Helpers import Config
from Model import MultiModalRetrievalModel
from Retrieval import make_retrieval_engine
from DataHandler import parse_openi_xml, build_dataloader
from retrieval_metrics import precision_at_k, mean_average_precision, mean_reciprocal_rank
from LabelData import disease_groups, normal_groups

BASE_DIR    = Path(__file__).resolve().parent.parent.parent
CONFIG_PATH = BASE_DIR / "configs" / "config.yaml"
EMBED_DIR   = BASE_DIR / "embeddings"
GT_DIR      = BASE_DIR / "ground_truths"
SPLIT_DIR   = BASE_DIR / "splited_data"
CKPT_PATH   = BASE_DIR.parent / "checkpoints" / "model_best.pt"
MODEL_DIR   = BASE_DIR / "models"
XML_DIR     = BASE_DIR / "data" / "openi" / "xml" / "NLMCXR_reports" / "ecgen-radiology"
DICOM_ROOT  = BASE_DIR / "data" / "openi" / "dicom"


def main(k=10):
    cfg = Config.load(CONFIG_PATH)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    label_cols = list(disease_groups.keys()) + list(normal_groups.keys())

    with open(GT_DIR / "test_relevance.json") as f:
        gt_general = json.load(f)

    with open(GT_DIR / "test_to_train_relevance.json") as f:
        gt_historical = json.load(f)

    engine_testdb  = make_retrieval_engine(
        str(EMBED_DIR / "test_joint_embeddings.npy"),
        str(EMBED_DIR / "test_ids.json"),
        method="dls",
        link_threshold=0.5,
        max_links=10
    )

    engine_traindb = make_retrieval_engine(
        str(EMBED_DIR / "train_joint_embeddings.npy"),
        str(EMBED_DIR / "train_ids.json"),
        method="dls",
        link_threshold=cfg.focal_ratio,
        max_links=int(cfg.num_heads)
    )

    model = MultiModalRetrievalModel(
        joint_dim=cfg.joint_dim,
        num_heads=cfg.num_heads,
        num_fusion_layers=cfg.num_fusion_layers,
        num_classes=len(label_cols),
        fusion_type=cfg.fusion_type,
        swin_ckpt_path=MODEL_DIR / "swin_checkpoint.safetensors",
        bert_local_dir= MODEL_DIR / "clinicalbert_local",
        checkpoint_path=str(CKPT_PATH),
        use_shared_ffn=cfg.use_shared_ffn,
        use_cls_only=cfg.use_cls_only,
        device=device,
        training=True
    ).to(device)
    model.eval()

    label_cols = list(disease_groups.keys()) + list(normal_groups.keys())

    df_test = pd.read_csv(SPLIT_DIR / "openi_test_labeled.csv")
    with open(SPLIT_DIR / "test_split_ids.json") as f:
        test_ids = json.load(f)
    df_test = df_test[df_test["id"].isin(test_ids)].reset_index(drop=True)

    parsed_records = parse_openi_xml(XML_DIR, DICOM_ROOT)
    labels_df = pd.read_csv(BASE_DIR / "outputs" / "openi_labels_final.csv").set_index("id")

    records = []
    for rec in parsed_records:
        rec_id = rec["id"]
        if rec_id in labels_df.index:
            label_vec = labels_df.loc[rec_id, label_cols].tolist()
            records.append({
                "id": rec["id"],
                "report_text": rec["report_text"],
                "dicom_path": rec["dicom_path"],
                "labels": label_vec
        })

    test_records = [r for r in records if r["id"] in test_ids]

    test_loader = build_dataloader(
        records=test_records,
        batch_size=cfg.batch_size,
        mean=0.5,
        std=0.25,
        shuffle=False
    )

    all_ret_gen,  all_rel_gen  = [], []
    all_ret_hist, all_rel_hist = [], []
    gen_times: List[float]  = []
    hist_times: List[float] = []

    for batch in test_loader:
        qid = batch["id"][0]

        img   = batch["image"].to(device)
        ids   = batch["input_ids"].to(device)
        mask  = batch["attn_mask"].to(device)

        with torch.no_grad():
            outputs = model(img, ids, mask, return_attention=False)
            if isinstance(outputs, tuple) or isinstance(outputs, list):
                joint_emb = outputs[0]
            else:
                joint_emb = outputs

        q_emb = joint_emb.cpu().numpy()

        # Generalization: test to test
        t0 = time.perf_counter()
        ret_ids, _ = engine_testdb.retrieve(q_emb, K=5)
        gen_times.append(time.perf_counter() - t0)
        all_ret_gen.append(ret_ids)
        all_rel_gen.append(gt_general[qid])

        # Historical: test to train
        t0 = time.perf_counter()
        ret_ids, _ = engine_traindb.retrieve(q_emb, K=5)
        hist_times.append(time.perf_counter() - t0)
        all_ret_hist.append(ret_ids)
        all_rel_hist.append(gt_historical[qid])

    # Evaluate precision at k
    p_gen   = np.mean([precision_at_k(r, rel, k=k) for r, rel in zip(all_ret_gen, all_rel_gen)])
    p_hist  = np.mean([precision_at_k(r, rel, k=k) for r, rel in zip(all_ret_hist, all_rel_hist)])

    map_gen  = mean_average_precision(all_ret_gen, all_rel_gen, k=k)
    map_hist = mean_average_precision(all_ret_hist, all_rel_hist, k=k)

    mrr_gen  = mean_reciprocal_rank(all_ret_gen, all_rel_gen)
    mrr_hist = mean_reciprocal_rank(all_ret_hist, all_rel_hist)

    # average query times (ms)
    avg_gen_ms  = 1000 * np.mean(gen_times)
    avg_hist_ms = 1000 * np.mean(hist_times)

    print(f"Generalization (test to test)   P@{k}: {p_gen:.4f}   AvgTime: {avg_gen_ms:.2f} ms")
    print(f"Historical    (test to train)  P@{k}: {p_hist:.4f}   AvgTime: {avg_hist_ms:.2f} ms")
    print(f"Generalization (test to test)   mAP: {map_gen:.4f}   MRR: {mrr_gen:.4f}")
    print(f"Historical    (test to train)  mAP: {map_hist:.4f}   MRR: {mrr_hist:.4f}")

    result_dir = BASE_DIR / "retrieval_eval_result"
    result_dir.mkdir(exist_ok=True)

    result_path = result_dir / f"eval_results_k{k}.txt"
    with open(result_path, "w") as f:
        f.write(f"Generalization (test to test)   P@{k}: {p_gen:.4f}   AvgTime: {avg_gen_ms:.2f} ms\n")
        f.write(f"Historical    (test to train)  P@{k}: {p_hist:.4f}   AvgTime: {avg_hist_ms:.2f} ms\n")
        f.write(f"Generalization (test to test)   mAP: {map_gen:.4f}   MRR: {mrr_gen:.4f}\n")
        f.write(f"Historical    (test to train)  mAP: {map_hist:.4f}   MRR: {mrr_hist:.4f}\n")
    
    print(f"[INFO] Results saved to: {result_path}")

if __name__ == "__main__":
    main(k=5)
end src\Evaluate\retrieval_eval.py

src\Evaluate\retrieval_metrics.py
import numpy as np
from typing import List, Set, Tuple

def precision_at_k(retrieved_ids, relevant_ids, k=5):
    """
    Precision@k = (# relevant in top-k) / k
    """
    retrieved_topk = retrieved_ids[:k]
    rel_set = set(relevant_ids)
    num_relevant = sum([1 for r in retrieved_topk if r in rel_set])
    return num_relevant / k

def recall_at_k(retrieved_ids, relevant_ids, k=5):
    """
    Recall@k = (# relevant in top-k) / total relevant
    """
    rel_set = set(relevant_ids)
    if len(rel_set) == 0:
        return 0.0
    retrieved_topk = retrieved_ids[:k]
    num_relevant = sum([1 for r in retrieved_topk if r in rel_set])
    return num_relevant / len(rel_set)

def average_precision(retrieved: List[str], relevant: Set[str], k: int = None) -> float:
    """
    AP = sum_{i=1..K} [Precision@i * rel(i)] / (# relevant)
    where rel(i)=1 if retrieved[i] is relevant else 0.
    If k is None, we use all retrieved results.
    """
    if k is None:
        k = len(retrieved)
    hits = 0
    score = 0.0
    for i, r in enumerate(retrieved[:k], start=1):
        if r in relevant:
            hits += 1
            score += hits / i
    return score / len(relevant) if relevant else 0.0

def mean_average_precision(
    all_retrieved: List[List[str]],
    all_relevant:  List[Set[str]],
    k: int = None
) -> float:
    """
    mAP over a set of queries.
    all_retrieved[i] is the retrieved list for query i
    all_relevant[i]  is the set of relevant IDs for query i
    """
    APs = [
        average_precision(ret, rel, k)
        for ret, rel in zip(all_retrieved, all_relevant)
    ]
    return float(np.mean(APs))

def mean_reciprocal_rank(
    all_retrieved: List[List[str]],
    all_relevant:  List[Set[str]]
) -> float:
    """
    MRR = mean( 1 / rank_i ), where rank_i is the first position of a relevant item.
    If none relevant found, reciprocal rank is 0 for that query.
    """
    rr_list = []
    for retrieved, relevant in zip(all_retrieved, all_relevant):
        rr = 0.0
        for i, r in enumerate(retrieved, start=1):
            if r in relevant:
                rr = 1.0 / i
                break
        rr_list.append(rr)
    return float(np.mean(rr_list))
end src\Evaluate\retrieval_metrics.py

src\Evaluate\__init__.py
from .data_phrase_check import dataPhraseCheck
from .dataEDAnLabeledCheck import edaLabeledCheck
from .EDA_before_split import get_eda_before_split
from .EDA_after_split import get_eda_after_split


__all__ = [
    'dataPhraseCheck',
    'edaLabeledCheck',
    'get_eda_before_split',
    'get_eda_after_split'
]
end src\Evaluate\__init__.py

src\Helpers\config.py
from dataclasses import dataclass, field, fields, asdict
import yaml
from typing import Any, Dict

@dataclass
class Config:
    # Training parameters
    epochs: int = 50
    patience: int = 10
    batch_size: int = 1
    lr: float = 2e-5

    # Model parameters
    num_fusion_layers: int = 3
    use_focal: bool = False
    use_hybrid: bool = True
    image_backbone: str = "swin"
    fusion_type: str = "cross"
    joint_dim: int = 1024
    num_heads: int = 32
    text_dim: int = 512
    use_shared_ffn: bool = True
    use_cls_only: bool = False

    # Knowledge graph parameters
    kg_method: str = "cosine"
    kg_emb_dim: int = 200
    kg_epochs: int = 5
    kg_weight: float = 0.1
    kg_mode: str = "dataset"

    # Loss parameters
    cls_weight: float = 1.5
    cont_weight: float = 0.3
    weight_img_joint: float = 0.1
    weight_text_joint: float = 0.1
    gamma_focal: float = 1.0
    focal_ratio: float = 0.3
    temperature: float = 0.125

    # Wandb parameters
    project_name: str = "multi-modal-retrieval-predict"

    # Auto-generated
    run_name: str = field(init=False, default="")

    def __post_init__(self):
        if self.use_hybrid:
            name_method = "hybrid(bce_focal)"
        elif self.use_focal:
            name_method = "focal"
        else:
            name_method = "bce"
        self.set_run_name(name_method)
        # run basic validation so bad values are caught early
        self.validate()

    def set_run_name(self, name_method: str):
        # build dict skipping run_name, only include init fields
        cfg_dict = {
            fld.name: getattr(self, fld.name)
            for fld in fields(self)
            if fld.init and fld.name != "run_name"
        }

        parts = [f"method={name_method}"]
        for k, v in cfg_dict.items():
            if isinstance(v, float):
                if "lr" in k:
                    parts.append(f"{k}={v:.0e}")
                else:
                    parts.append(f"{k}={v:.4f}")
            else:
                parts.append(f"{k}={v}")
        self.run_name = "_".join(parts)

    def validate(self) -> None:
        """Basic sanity checks raise ValueError if something invalid."""
        if not (isinstance(self.lr, (float, int)) and self.lr >= 0.0):
            raise ValueError(f"lr must be a non-negative number. Got {self.lr!r}")
        if not (isinstance(self.batch_size, int) and self.batch_size > 0):
            raise ValueError(f"batch_size must be a positive integer. Got {self.batch_size!r}")
        if not (isinstance(self.epochs, int) and self.epochs > 0):
            raise ValueError(f"epochs must be a positive integer. Got {self.epochs!r}")
        if not (isinstance(self.temperature, (float, int)) and self.temperature > 0.0):
            raise ValueError(f"temperature must be > 0. Got {self.temperature!r}")

    @staticmethod
    def _coerce_value(raw: Any, target_type: Any) -> Any:
        """Try to coerce common scalar types. If coercion fails, return raw."""
        if raw is None:
            return None
        try:
            if target_type is float:
                return float(raw)
            if target_type is int:
                return int(raw)
            if target_type is bool:
                if isinstance(raw, bool):
                    return raw
                if isinstance(raw, str):
                    return raw.strip().lower() in ("true", "1", "yes", "y")
                return bool(raw)
            if target_type is str:
                return str(raw)
        except Exception:
            # fallback to raw value if conversion fails
            return raw
        return raw

    @staticmethod
    def load(path: str) -> "Config":
        """Load config from YAML, coerce types for dataclass fields and validate."""
        with open(path, "r", encoding="utf-8") as fh:
            yaml_data = yaml.safe_load(fh) or {}

        if not isinstance(yaml_data, dict):
            raise ValueError("Config file must contain a YAML mapping (keys/values).")

        # map of init-able dataclass fields
        field_map: Dict[str, Any] = {fld.name: fld for fld in fields(Config) if fld.init}

        # warn about unknown keys in YAML
        extra_keys = set(yaml_data.keys()) - set(field_map.keys())
        if extra_keys:
            print(f"[WARN] Unknown keys in config file (will be ignored): {sorted(extra_keys)}")

        used_defaults = []
        data: Dict[str, Any] = {}
        for name, fld in field_map.items():
            if name in yaml_data:
                raw = yaml_data[name]
                coerced = Config._coerce_value(raw, fld.type)
                data[name] = coerced
            else:
                data[name] = fld.default
                used_defaults.append(name)

        cfg = Config(**data)  # __post_init__ will run and call validate()
        if used_defaults:
            print("Using default values for:", ", ".join(used_defaults))
        return cfg

    def to_dict(self) -> Dict[str, Any]:
        """Return plain dict representation (includes run_name)."""
        return asdict(self)

if __name__ == "__main__":
    import os
    example_path = os.path.join("D:\Github\multi-modal-retrieval-predict-project\configs\config.yaml")
    if os.path.exists(example_path):
        cfg = Config.load(example_path)
        print("Loaded config:", cfg)
        print("run_name:", cfg.run_name)
        print("LR type:", type(cfg.lr), "value:", cfg.lr)
    else:
        # fallback: show default config
        cfg = Config()
        print("No config.yaml found — using defaults:")
        print(cfg)
        print("run_name:", cfg.run_name)

end src\Helpers\config.py

src\Helpers\contructGT.py
import json
import pandas as pd
from pathlib import Path

BASE_DIR     = Path(__file__).resolve().parent.parent.parent
SPLIT_DIR    = BASE_DIR / "splited_data"
GT_DIR       = BASE_DIR / "ground_truths"
GT_DIR.mkdir(exist_ok=True)

def create_gt(split_dir=None, gt_save_dir=None, combined_groups=True):
    """
    Creates a ground truth file for generalization and historical relevance.
    Ground truth files are saved in the GT_DIR directory by default.

    Parameters
    ----------
    split_dir : str
        Path to the directory containing the split IDs JSON files
    gt_save_dir : str
        Path to the directory where the ground truth files will be saved
    combined_groups : bool
        If True, ensure the label columns are consistent with disease and normal groups

    Returns
    -------
    None

    Saves two JSON files: test_relevance.json and test_to_train_relevance.json
    """
    if split_dir is None:
        split_dir = SPLIT_DIR
    
    with open(split_dir / "test_split_ids.json") as f:
        test_ids = json.load(f)
    with open(split_dir / "train_split_ids.json") as f:
        train_ids = json.load(f)

    df_test  = pd.read_csv(split_dir / "openi_test_labeled.csv")
    df_test  = df_test[df_test["id"].isin(test_ids)].reset_index(drop=True)

    df_train = pd.read_csv(split_dir / "openi_train_labeled.csv")
    df_train = df_train[df_train["id"].isin(train_ids)].reset_index(drop=True)

    # Ensure the label columns are consistent with disease and normal groups
    label_cols = list(combined_groups.keys())
    test_vals  = df_test[label_cols].values.astype(int)   # shape (N_test, L)
    test_ids    = df_test["id"].tolist()

    train_vals = df_train[label_cols].values.astype(int)  # shape (N_train, L)
    train_ids   = df_train["id"].tolist()

    test_relevance = {}
    for i, qid in enumerate(test_ids):
        # boolean mask of same-label examples
        shared = (test_vals & test_vals[i]).sum(axis=1) > 0
        # exclude the query itself
        rel_ids = [test_ids[j] for j, keep in enumerate(shared) if keep and j != i]
        test_relevance[qid] = rel_ids

    # Build test→train relevance (historical)
    test_to_train = {}
    for i, qid in enumerate(test_ids):
        shared = (train_vals & test_vals[i]).sum(axis=1) > 0
        rel_ids = [train_ids[j] for j, keep in enumerate(shared) if keep]
        test_to_train[qid] = rel_ids

    rel_counts = [len(v) for v in test_relevance.values()]
    print("Min relevant items per query:", min(rel_counts))
    print("Max relevant items per query:", max(rel_counts))
    print("Avg relevant items per query:", sum(rel_counts)/len(rel_counts))

    if gt_save_dir is None:
        gt_save_dir = GT_DIR

    with open(gt_save_dir / "test_relevance.json", "w") as f:
        json.dump(test_relevance, f, indent=2)

    with open(gt_save_dir / "test_to_train_relevance.json", "w") as f:
        json.dump(test_to_train, f, indent=2)

    print(f"Saved generalization GT -> {gt_save_dir/'test_relevance.json'}")
    print(f"Saved historical  GT -> {gt_save_dir/'test_to_train_relevance.json'}")

end src\Helpers\contructGT.py

src\Helpers\contruct_test_db.py
import numpy as np, json, torch
import pandas as pd
from pathlib import Path
from .config import Config
from DataHandler import build_dataloader, parse_openi_xml

BASE_DIR    = Path(__file__).resolve().parent.parent.parent
CONFIG_PATH = BASE_DIR / "configs" / "config.yaml"
CKPT_PATH   = BASE_DIR / "checkpoints" / "model_best.pt"
EMBED_DIR   = BASE_DIR / "embeddings"
MODEL_DIR   = BASE_DIR / "models"
SPLIT_DIR   = BASE_DIR / "splited_data"
XML_DIR     = BASE_DIR / "data" / "openi" / "xml" / "NLMCXR_reports" / "ecgen-radiology"
DICOM_ROOT  = BASE_DIR / "data" / "openi" / "dicom"
EMBED_DIR.mkdir(exist_ok=True)

def get_model(cfg, ckpt_path, model_dir, device, label_cols):
    from Model import MultiModalRetrievalModel
    model = MultiModalRetrievalModel(
        joint_dim=cfg.joint_dim,
        num_heads=cfg.num_heads,
        num_classes=len(label_cols),
        fusion_type=cfg.fusion_type,
        swin_ckpt_path=model_dir / "swin_checkpoint.safetensors",
        bert_local_dir= model_dir / "clinicalbert_local",
        checkpoint_path=str(ckpt_path),
        use_shared_ffn=cfg.use_shared_ffn,
        use_cls_only=cfg.use_cls_only,
        device=device,
        training=True
    ).to(device)
    model.eval()

    return model

def construct_db_test(config_path=None, ckpt_path=None, 
                        split_dir=None, model_dir=None, 
                        xml_dir=None, dicom_root=None, 
                        embed_dir=None,
                        combined_groups=None):
    """
    Constructs a test database for the web application by sampling N records from the test split IDs.

    Parameters
    ----------
    config_path : str
        Path to config file
    ckpt_path : str
        Path to model checkpoint
    split_dir : str
        Path to folder containing train, validation, and test split IDs
    model_dir : str
        Path to folder containing model weights
    xml_dir : str
        Path to folder containing individual .xml report files
    dicom_root : str
        Root folder where .dcm files live (possibly nested)
    embed_dir : str
        Path to save the test embeddings to
    combined_groups : dict of str to list of str
        Dictionary where keys are disease/normal group names and values are lists of labels

    Returns
    -------
    None

    Saves a JSON file with the following format: {<rid>: {<dicom_path>, <report_text>, <labels>}} to the output directory
    """
    # Resolve paths
    if config_path is None:
        config_path = CONFIG_PATH
    if ckpt_path is None:
        ckpt_path = CKPT_PATH
    if split_dir is None:
        split_dir = SPLIT_DIR
    if xml_dir is None:
        xml_dir = XML_DIR
    if dicom_root is None:
        dicom_root = DICOM_ROOT
    if model_dir is None:
        model_dir = MODEL_DIR
    if embed_dir is None:
        embed_dir = EMBED_DIR
    if combined_groups is not None:
        raise ValueError("Please provide a least a list of disease groups and normal groups to label the report with.")
    
    cfg    = Config.load(config_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    label_cols = list(combined_groups.keys())

    df_test = pd.read_csv(split_dir / "openi_test_labeled.csv")
    with open(split_dir / "test_split_ids.json") as f:
        test_ids = json.load(f)
    df_test = df_test[df_test["id"].isin(test_ids)].reset_index(drop=True)

    parsed_records = parse_openi_xml(xml_dir, dicom_root, combined_groups=combined_groups)
    labels_df = pd.read_csv(BASE_DIR / "outputs" / "openi_labels_final.csv").set_index("id")

    records = []
    for rec in parsed_records:
        rec_id = rec["id"]
        if rec_id in labels_df.index:
            label_vec = labels_df.loc[rec_id, label_cols].tolist()
            records.append({
                "id": rec["id"],
                "report_text": rec["report_text"],
                "dicom_path": rec["dicom_path"],
                "labels": label_vec
            })

    test_records = [r for r in records if r["id"] in test_ids]

    test_loader = build_dataloader(
        records=test_records,
        batch_size=cfg.batch_size,
        mean=0.5,
        std=0.25,
        shuffle=False
    )

    model = get_model(cfg, ckpt_path, model_dir, device, label_cols)

    all_embs = []
    all_ids  = []

    with torch.no_grad():
        for batch in test_loader:
            img  = batch["image"].to(device)
            ids  = batch["input_ids"].to(device)
            mask = batch["attn_mask"].to(device)
            meta_id = batch["id"][0]

            joint_emb, _, _ = model(img, ids, mask, return_attention=False)
            all_embs.append(joint_emb.cpu().numpy().squeeze(0))
            all_ids.append(meta_id)

    all_embs = np.vstack(all_embs)
    np.save(embed_dir / "test_joint_embeddings.npy", all_embs)

    with open(embed_dir / "test_ids.json", "w") as f:
        json.dump(all_ids, f)

    print(f"Saved test embeddings to {embed_dir/'test_embeddings.npy'}")
    print(f"Saved test IDs to {embed_dir/'test_ids.json'}")

if __name__ == "__main__":
    construct_db_test()

end src\Helpers\contruct_test_db.py

src\Helpers\dumpEmbedding.py
import numpy as np
import json
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent.parent
EMBEDDINGS_DIR = BASE_DIR / "embeddings"

def createDumpEmbedding(base_dir, embeddings_dir):
    """
    Creates a merged dump of train and validation embeddings and IDs.

    Parameters
    ----------
    base_dir : Path
        The base directory of the project.
    embeddings_dir : Path
        The directory where the embeddings are stored.

    Returns
    -------
    None
    """
    if not base_dir:
        base_dir = BASE_DIR
    if not embeddings_dir:
        embeddings_dir = EMBEDDINGS_DIR

    train_emb = np.load(embeddings_dir / "train_joint_embeddings.npy")
    val_emb = np.load(embeddings_dir / "val_joint_embeddings.npy")
    merged_emb = np.concatenate([train_emb, val_emb], axis=0)
    np.save(embeddings_dir / "trainval_joint_embeddings.npy", merged_emb)

    with open(embeddings_dir / "train_ids.json") as f1, open(embeddings_dir / "val_ids.json") as f2:
        train_ids = json.load(f1)
        val_ids = json.load(f2)
        merged_ids = train_ids + val_ids

    with open(EMBEDDINGS_DIR / "trainval_ids.json", "w") as fout:
        json.dump(merged_ids, fout)

    print(f"Saved merged embeddings to: {embeddings_dir / 'trainval_joint_embeddings.npy'}")
    print(f"Saved merged IDs to:        {embeddings_dir / 'trainval_ids.json'}")

end src\Helpers\dumpEmbedding.py

src\Helpers\helper.py
from scipy.stats import pearsonr, spearmanr
import numpy as np
import io
import re
import base64
import matplotlib.pyplot as plt
from PIL import Image
import torch
import os
from PIL import Image
import math
from typing import Optional, Dict, Any
from pathlib import Path
import pickle
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

from Helpers import Config
from DataHandler import  parse_openi_xml

BASE_DIR = Path(__file__).resolve().parent.parent.parent
CKPT_DIR = BASE_DIR / "checkpoints"
MODEL_DIR = BASE_DIR / "models"
EMBEDDINGS_DIR = BASE_DIR / "embeddings"
OUTPUT_DIR = BASE_DIR / "outputs"
XML_DIR = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
CONFIG_DIR = BASE_DIR / "configs"
CKPT_PATH = BASE_DIR / "checkpoints" / "model_best.pt"
REPORT_CACHE_PATH = OUTPUT_DIR / "openi_reports.pkl"

cfg = Config.load(CONFIG_DIR / "config.yaml")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[loader] device = {device}")

def load_report_lookup_via_parser(xml_dir=None, dicom_root=None,
                                    report_cache_path=None,
                                    combined_groups=None
                                ) -> dict:
    """Load a mapping of OpenI id -> report_text from a cache file (if present)
    or by parsing XML files in the given directory.

    Args:
        xml_dir: Directory containing the OpenI XML files.
        dicom_root: Directory containing the OpenI DICOM files.

    Returns:
        A dictionary mapping OpenI id to report_text.
    """
    if xml_dir is None:
        xml_dir = XML_DIR
    if dicom_root is None:
        dicom_root = DICOM_ROOT
    if report_cache_path is None:
        report_cache_path = REPORT_CACHE_PATH
    
    if report_cache_path.exists():
        print(f"[loader] Loading cached report lookup from {report_cache_path.name}")
        with open(report_cache_path, "rb") as f:
            return pickle.load(f)

    print("[loader] Parsing reports using parse_openi_xml().")
    parsed_records = parse_openi_xml(xml_dir=xml_dir, dicom_root=dicom_root, combined_groups=combined_groups)
    id_to_report = {
        rec["id"]: rec["report_text"]
        for rec in parsed_records
        if "id" in rec and "report_text" in rec
    }

    report_cache_path.parent.mkdir(parents=True, exist_ok=True)
    with open(report_cache_path, "wb") as f:
        pickle.dump(id_to_report, f)
    print(f"[loader] Cached {len(id_to_report)} reports to {report_cache_path.name}")
    return id_to_report

def find_dicom_file(rid: str, dicom_root=None) -> Path:
    """Find the DICOM file path for the given id.

    Args:
        rid: The id of the DICOM file to find.
        dicom_root: The root directory of the DICOM files.

    Returns:
        The path to the DICOM file.
    """
    if dicom_root is None:
        dicom_root = DICOM_ROOT
    
    primary = list(dicom_root.rglob(f"{rid}.dcm"))
    if primary:
        return primary[0]

    # Try fallback without leading patient ID
    fallback_id = "_".join(rid.split("_")[1:])
    fallback = list(dicom_root.rglob(f"{fallback_id}.dcm"))
    if fallback:
        print(f"[loader] Using fallback DICOM path: {fallback[0].name}")
        return fallback[0]

    raise FileNotFoundError(f"DICOM not found for either {rid}.dcm or {fallback_id}.dcm")

def safe_unpack_topk(topk_any):
    """Normalize topk returned shapes to a flat python list."""
    if topk_any is None:
        return []
    # if nested lists (B x K) and user passed topk for batch, take first row
    if isinstance(topk_any, list) and len(topk_any) > 0 and isinstance(topk_any[0], (list, tuple)):
        return list(topk_any[0])
    # if numpy array
    try:
        return list(np.array(topk_any).tolist())
    except Exception:
        return list(topk_any)

def resize_to_match(src: np.ndarray, ref: np.ndarray) -> np.ndarray:
    """
    Resize src (H_src x W_src) to match shape of ref (H_ref x W_ref).
    Returns resized array (float32). Tries cv2, then skimage, then scipy.ndimage.zoom,
    then a crude numpy repeat fallback.
    """
    import numpy as np
    Ht, Wt = ref.shape
    Hs, Ws = src.shape
    if (Hs, Ws) == (Ht, Wt):
        return src.astype(np.float32)

    src_f = src.astype(np.float32)

    # try cv2
    try:
        import cv2
        # cv2.resize takes (width, height)
        resized = cv2.resize(src_f, (Wt, Ht), interpolation=cv2.INTER_LINEAR)
        return resized.astype(np.float32)
    except Exception:
        pass

    # try skimage
    try:
        from skimage.transform import resize
        resized = resize(src_f, (Ht, Wt), order=1, preserve_range=True, anti_aliasing=True)
        return resized.astype(np.float32)
    except Exception:
        pass

    # try scipy.ndimage.zoom
    try:
        from scipy.ndimage import zoom
        zh = Ht / float(Hs)
        zw = Wt / float(Ws)
        resized = zoom(src_f, (zh, zw), order=1)  # order=1 -> bilinear-like
        return resized.astype(np.float32)
    except Exception:
        pass

    # fallback: crude nearest-repeat
    try:
        # compute integer repetition factors (ceil), then crop
        rh = int(np.ceil(Ht / Hs))
        rw = int(np.ceil(Wt / Ws))
        rep = np.repeat(np.repeat(src_f, rh, axis=0), rw, axis=1)
        resized = rep[:Ht, :Wt].astype(np.float32)
        return resized
    except Exception:
        # last resort: pad or crop
        out = np.zeros((Ht, Wt), dtype=np.float32)
        h = min(Hs, Ht)
        w = min(Ws, Wt)
        out[:h, :w] = src_f[:h, :w]
        return out

def compare_maps(map_a: np.ndarray, map_b: np.ndarray, topk_frac: float = 0.05):
    """
    Compute metrics comparing two attention maps.
    """
    a = map_a.flatten()
    b = map_b.flatten()

    # Defaults
    pearson_val = 0.0
    spearman_val = 0.0

    try:
        a_const = np.all(a == a[0])
        b_const = np.all(b == b[0])

        if a_const or b_const:
            const_source = "a" if a_const else "b"
            print(f"[WARN] compare_maps: input {const_source} is constant, correlation undefined")
        else:
            pearson_val = float(pearsonr(a, b)[0])
            spearman_val = float(spearmanr(a, b).correlation)
    except Exception as e:
        print(f"[WARN] compare_maps correlation failed: {e}")

    # IoU on top-k fraction
    k = max(1, int(len(a) * topk_frac))
    a_top = (a >= np.partition(a, -k)[-k])
    b_top = (b >= np.partition(b, -k)[-k])
    inter = np.logical_and(a_top, b_top).sum()
    union = np.logical_or(a_top, b_top).sum()
    iou = float(inter) / (float(union) + 1e-8)

    return {
        'pearson': pearson_val,
        'spearman': spearman_val,
        f'iou_top{int(topk_frac*100)}pct': iou
    }

def to_numpy(x):
    """Return a numpy array whether x is a torch tensor or numpy already."""
    if x is None:
        return None
    if torch.is_tensor(x):
        return x.detach().cpu().numpy()
    return np.asarray(x)

def heatmap_to_base64_overlay(orig_img: np.ndarray,
                            heatmap: np.ndarray,
                            cmap: str = 'jet',
                            alpha: float = 0.5) -> str:
    # convert orig to 2D grayscale if needed
    """
    Returns a base64-encoded PNG of the original image with the heatmap overlayed.

    Parameters
    ----------
    orig_img : np.ndarray
        The original image. May be 2D (grayscale) or 3D (RGB).
    heatmap : np.ndarray
        The heatmap. Will be resized if shape is different from `orig_img`.
    cmap : str, optional
        The matplotlib colormap to use. Defaults to 'jet'.
    alpha : float, optional
        The alpha channel value of the heatmap. Defaults to 0.5.

    Returns
    -------
    str
        The base64-encoded PNG.
    """
    img = orig_img.squeeze()
    if img.ndim == 3 and img.shape[2] == 3:
        base = img
    else:
        # grayscale -> RGB
        if img.max() <= 1.0:
            img_u8 = (img * 255).astype('uint8')
        else:
            img_u8 = img.astype('uint8')
        base = np.stack([img_u8, img_u8, img_u8], axis=-1)
        base = base.astype('uint8')

    # normalize heatmap to 0..1
    h = heatmap.astype(np.float32)
    h = (h - h.min()) / (h.max() - h.min() + 1e-8)

    # resize heatmap if different shape
    H, W = base.shape[:2]
    if h.shape != (H, W):
        h_img = Image.fromarray((h * 255).astype('uint8')).resize((W, H), resample=Image.BILINEAR)
        h = np.asarray(h_img).astype(np.float32) / 255.0

    # colorize via matplotlib cmap
    cmap_fn = plt.get_cmap(cmap)
    colored = cmap_fn(h)[:, :, :3]  # HxWx3 float 0..1
    colored_u8 = (colored * 255).astype('uint8')

    # ensure base is uint8
    if base.dtype != np.uint8:
        if base.max() <= 1.0:
            base_u8 = (base * 255).astype('uint8')
        else:
            base_u8 = base.astype('uint8')
    else:
        base_u8 = base

    # blend
    blended = (base_u8 * (1 - alpha) + colored_u8 * alpha).astype('uint8')

    pil = Image.fromarray(blended)
    buf = io.BytesIO()
    pil.save(buf, format='PNG', optimize=True)
    buf.seek(0)
    return base64.b64encode(buf.read()).decode('ascii')

def save_b64_map(
    b64_str: str,
    qid: str,
    rid: str = None,
    map_type: str = "attn_txt",
    base_dir: str = "outputs/reports"
) -> str:
    """
    Save a base64-encoded map into the folder of the query (qid).

    Parameters
    ----------
    b64_str : str
        Base64 string (from heatmap_to_base64_overlay).
    qid : str
        Query ID.
    rid : str, optional
        Retrieval ID. If None, saves under 'query' folder.
    map_type : str
        Type of map: 'attn_txt', 'attn_img', 'attn_comb', 'ig', 'gradcam'.
    base_dir : str
        Base output directory. Defaults to 'outputs/reports'.

    Returns
    -------
    str
        Path to saved PNG.
    """
    if b64_str is None:
        return None

    img_data = base64.b64decode(b64_str)
    img = Image.open(io.BytesIO(img_data))

    if rid is None:
        out_dir = Path(base_dir) / str(qid) / "query"
        fname = f"{map_type}.png"
    else:
        out_dir = Path(base_dir) / str(qid) / str(rid)
        fname = f"{map_type}.png"

    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / fname
    img.save(out_path, format="PNG", optimize=True)
    return str(out_path)

def attention_to_html(tokens, scores):
    """
    Convert attention scores to HTML visualization.

    Parameters
    ----------
    tokens : list[str]
        Tokens to visualize.
    scores : np.ndarray
        Attention scores for each token.

    Returns
    -------
    str
        HTML visualization of attention scores.
    """
    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)
    spans = []
    for tok, s in zip(tokens, scores):
        color = f"rgba(255,0,0,{s:.2f})"  # red with alpha = score
        spans.append(f"<span style='background-color:{color}'>{tok}</span>")
    return " ".join(spans)

def make_attention_maps(
    model,
    fusion_layer,                 # the fusion module (callable) that supports return_attention=True
    img_global,                   # tensor (B,...) single-batch or (1,...)
    img_patches,                  # tensor (B,...) single-batch or (1,...)
    txt_feats,                    # tensor (B,...) single-batch or (1,...)
    device,
    rid: Optional[str] = None,
    combine_method: str = "avg",  # "avg" | "product" | "max"
    pad_to_square: bool = True,   # pad attention vector to next perfect square if needed
    save: bool = False,
    save_dir: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Compute attention maps for a single input using the given fusion layer and explainer.

    Parameters
    ----------
    model : Module
        The model containing the explainer and fusion layer.
    fusion_layer : callable
        The fusion module that supports `return_attention=True`.
    img_global : Tensor
        The global image features. Expected shape: (B, ...) where B is the batch size.
    img_patches : Tensor
        The image patches. Expected shape: (B, ...) where B is the batch size.
    txt_feats : Tensor
        The text features. Expected shape: (B, ...) where B is the batch size.
    device : torch.device
        The device to run the computation on.
    rid : Optional[str]
        The request ID (if any) for logging or saving.
    combine_method : str
        The method to combine the text->image and image->text attention maps. One of "avg", "product", or "max".
        Defaults to "avg".
    pad_to_square : bool
        Whether to pad the attention vector to the next perfect square if needed. Defaults to True.
    save : bool
        Whether to save the attention maps to disk. Defaults to False.
    save_dir : Optional[str]
        The directory to save the attention maps to. Defaults to the `BASE_DIR/attention_maps/eval` directory.

    Returns
    -------
    out : Dict[str, Any]
        A dictionary containing the attention maps, tensors, and other information.
        The keys are:
            - "att_map_txt": The text->image attention map.
            - "att_map_img": The image->text attention map.
            - "att_map_comb": The combined attention map.
            - "att_txt_tensor": The text->image attention tensor.
            - "att_img_tensor": The image->text attention tensor.
            - "att_comb_tensor": The combined attention tensor.
    """
    out = {
        "att_map_txt": None,
        "att_map_img": None,
        "att_map_comb": None,
        "att_txt_tensor": None,
        "att_img_tensor": None,
        "att_comb_tensor": None,
    }

    try:
        if fusion_layer is None:
            raise RuntimeError("fusion_layer is None")

        # Make sure inputs are 1-batch and on device
        ig = img_global[0:1].to(device)
        ip = img_patches[0:1].to(device)
        tf = txt_feats[0:1].to(device)

        # request attention dict
        _, att = fusion_layer(ig, ip, tf, return_attention=True)
        if att is None:
            raise RuntimeError("fusion returned no attention")

        # helper: coerce common shapes to (B,1,N)
        def _to_patch_attention(a):
            if a is None:
                return None
            if not torch.is_tensor(a):
                try:
                    a = torch.as_tensor(a, device=device)
                except Exception:
                    a = torch.tensor(a, device=device)
            # common cases:
            # (B, heads, N) -> mean over heads -> (B,1,N)
            # (B,1,N) -> keep
            # (B,N,1) -> transpose -> (B,1,N)
            if a.dim() == 3:
                B, A, C = a.shape
                # already (B,1,N)
                if A == 1:
                    return a
                # (B,heads,N) -> average heads
                if C > 1:
                    return a.mean(dim=1, keepdim=True)
                # (B,N,1) -> transpose to (B,1,N)
                if C == 1:
                    return a.transpose(1, 2)
            if a.dim() == 2:
                # (B,N) -> (B,1,N)
                return a.unsqueeze(1)
            # otherwise return as-is
            return a

        att_txt = _to_patch_attention(att.get("txt2img", None))
        att_img = _to_patch_attention(att.get("img2txt", None))

        if att_txt is None and att_img is None:
            raise RuntimeError("no usable txt2img or img2txt attention found")

        # align patch counts if both exist
        if att_txt is not None and att_img is not None and att_txt.shape[-1] != att_img.shape[-1]:
            # try simple heuristics; otherwise prefer txt2img
            n_txt = att_txt.shape[-1]
            n_img = att_img.shape[-1]
            if n_img == 1 and n_txt > 1:
                att_img = att_img.expand(-1, -1, n_txt)
            elif n_txt == 1 and n_img > 1:
                att_txt = att_txt.expand(-1, -1, n_img)
            else:
                # fallback: drop img->txt (prefer text->image mapping)
                att_img = None

        # choose combination
        if att_txt is None:
            comb = att_img
        elif att_img is None:
            comb = att_txt
        else:
            if combine_method == "product":
                comb = att_txt * att_img
            elif combine_method == "max":
                comb = torch.max(att_txt, att_img)
            else:  # avg
                comb = (att_txt + att_img) / 2.0

        # normalize comb along patches
        comb = comb.float()
        denom = comb.sum(dim=-1, keepdim=True)
        comb = comb / (denom + 1e-8)

        # store raw tensors
        out["att_txt_tensor"] = att_txt
        out["att_img_tensor"] = att_img
        out["att_comb_tensor"] = comb

        # helper to prepare grid size, pad if required, and call explainer
        def _prepare_and_map(att_tensor):
            if att_tensor is None:
                return None
            B = att_tensor.shape[0]
            N = int(att_tensor.shape[-1])
            G = int(math.isqrt(N))
            if G * G != N:
                # try ceil to next square
                G = int(math.ceil(math.sqrt(N)))
                target_N = G * G
                if pad_to_square and target_N >= N:
                    # pad zeros on last dim to match target_N
                    pad_len = target_N - N
                    pad_tensor = torch.zeros((B, 1, pad_len), device=att_tensor.device, dtype=att_tensor.dtype)
                    att_tensor = torch.cat([att_tensor, pad_tensor], dim=-1)
                else:
                    # unable to reshape reasonably
                    print(f"[WARN] attention length N={N} is not a perfect square and pad_to_square is False")
                    return None
            # now call explainer
            try:
                return model.explainer.compute_attention_map(att_tensor, grid_size=G)
            except Exception as e:
                print(f"[WARN] compute_attention_map failed for rid={rid}: {e}")
                return None

        out["att_map_txt"] = _prepare_and_map(att_txt)
        out["att_map_img"] = _prepare_and_map(att_img)
        out["att_map_comb"] = _prepare_and_map(comb)

        # optional save
        if save:
            if save_dir is None:
                save_dir = Path(BASE_DIR) / "attention_maps" / "eval"

            os.makedirs(save_dir, exist_ok=True)
            def _save_map(m, fname):
                if m is None:
                    return
                arr = np.array(m).astype(np.float32)
                # normalize to 0..1
                if (arr.max() - arr.min()) > 1e-8:
                    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)
                else:
                    arr = np.clip(arr, 0.0, 1.0)
                img = Image.fromarray((arr * 255).astype(np.uint8))
                img.save(os.path.join(save_dir, fname))

            _save_map(out["att_map_txt"], f"{rid}_txt2img.png")
            _save_map(out["att_map_img"], f"{rid}_img2txt.png")
            _save_map(out["att_map_comb"], f"{rid}_combined_{combine_method}.png")

        return out

    except Exception as e:
        print(f"[WARN] make_attention_maps failed for {rid}: {e}")
        # ensure returned structure exists even on error
        return out

def kg_alignment_loss(joint_emb, batch_ids, kg_embs, node2id, trainer, labels=None, label_cols=None, loss_type="cosine"):
    """
    Align joint embeddings with KG embeddings.
    - First tries explicit 'report:{id}' node
    - If missing, falls back to average of its label-based KG nodes
    - If no mapping exists, falls back to zero vector
    """
    if joint_emb.device != kg_embs.device:
        kg_embs = kg_embs.to(joint_emb.device)

    kg_vecs = []
    for i, id_ in enumerate(batch_ids):
        node_key = f"report:{id_}"
        if node_key in node2id:
            # direct mapping
            kg_vecs.append(kg_embs[node2id[node_key]])
        else:
            # try label-based fallback
            if labels is not None and label_cols is not None:
                if i < len(labels):  # safety check
                    label_vec = labels[i].cpu().numpy()
                    pos_labels = [label_cols[j] for j, v in enumerate(label_vec) if v > 0.5]

                    label_embs = []
                    for lab in pos_labels:
                        lab_key = f"label:{lab}"
                        if lab_key in node2id:
                            label_embs.append(kg_embs[node2id[lab_key]])

                    if len(label_embs) > 0:
                        kg_vecs.append(torch.stack(label_embs).mean(dim=0))
                        continue  # done with fallback

            # if no report node and no labels mapped
            kg_vecs.append(torch.zeros_like(kg_embs[0]))

    kg_vecs = torch.stack(kg_vecs).to(joint_emb.device)

    # project joint_emb into KG space
    joint_proj = trainer.proj_to_kg(joint_emb)

    if loss_type == "mse":
        return torch.nn.functional.mse_loss(joint_proj, kg_vecs)
    elif loss_type == "cosine":
        return 1 - torch.nn.functional.cosine_similarity(joint_proj, kg_vecs).mean()
    else:
        raise ValueError(f"Unknown loss_type: {loss_type}")

def log_and_print(*msgs, log_file=None):
    text = " ".join(str(m) for m in msgs)
    print(text)
    
    if log_file is None:
        raise ValueError("log_file must be provided")
    
    log_file = Path(log_file)
    if not log_file.parent.exists():
        log_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(log_file, "a", encoding="utf-8") as f:
        f.write(text + "\n")

def _sanitize_node(s: str) -> str:
    s2 = s.strip()
    s2 = re.sub(r'\s+', '_', s2)
    s2 = re.sub(r'[^A-Za-z0-9_:.-]', '', s2)
    return s2

def safe_roc_auc(y_true, y_pred, label_names=None):
    results = []
    skipped = []
    for i in range(y_true.shape[1]):
        if len(np.unique(y_true[:, i])) < 2:
            results.append(float("nan"))
            if label_names is not None:
                skipped.append(label_names[i])
        else:
            results.append(roc_auc_score(y_true[:, i], y_pred[:, i]))
    if skipped:
        print(f"Skipped AUROC for classes with no positives/negatives: {skipped}")
    return np.array(results)

def safe_avg_precision(y_true, y_pred, label_names=None):
    """
    Compute per-class average precision (PR-AUC). If a class has only one label
    present in y_true (all 0s or all 1s), return NaN for that class and optionally
    collect its name in skipped.
    Returns: np.array(per_class_ap), skipped_list
    """
    results = []
    skipped = []
    for i in range(y_true.shape[1]):
        true_col = y_true[:, i]
        # if only one class present -> skip
        if len(np.unique(true_col)) < 2:
            results.append(float("nan"))
            if label_names is not None:
                skipped.append(label_names[i])
            continue
        try:
            ap = average_precision_score(true_col, y_pred[:, i])
            results.append(float(ap))
        except Exception:
            # fallback to nan if something goes wrong
            results.append(float("nan"))
            if label_names is not None:
                skipped.append(label_names[i])
    if skipped:
        print(f"Skipped AP for classes with no positives/negatives: {skipped}")
    return np.array(results)

def contrastive_loss(x, y, temperature=0.1):
    x = torch.nn.functional.normalize(x, dim=1)
    y = torch.nn.functional.normalize(y, dim=1)
    logits = torch.matmul(x, y.T) / temperature  # (B, B)
    labels = torch.arange(logits.size(0), device=logits.device)
    return torch.nn.functional.cross_entropy(logits, labels)

__all__ = ("find_dicom_file", "load_report_lookup_via_parser",
            "report_lookup", "make_attention_maps", "attention_to_html",
            "kg_alignment_loss", "log_and_print", "_sanitize_node",
            "safe_roc_auc", "safe_avg_precision", "contrastive_loss")

end src\Helpers\helper.py

src\Helpers\model_utils.py
from transformers import AutoModel, AutoTokenizer
from pathlib import Path

try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

MODEL_PLACE = BASE_DIR / "models"

def load_hf_model_or_local(
    model_name: str,
    local_dir: (str | Path) = None,
    is_tokenizer: bool = False,
    **kwargs
):
    """
    Load a Hugging Face model or tokenizer from a local directory if valid, 
    otherwise download from HF hub, save locally, and return it.

    Args:
        model_name (str): HF model/tokenizer identifier.
        local_dir (str | Path, optional): Directory to load from or save to.
                                            Defaults to MODEL_PLACE.
        is_tokenizer (bool): If True, loads a tokenizer; else loads a model.
        **kwargs: Passed through to `from_pretrained`.

    Returns:
        Pretrained model or tokenizer.
    """
    local_dir = Path(local_dir) if local_dir else MODEL_PLACE
    loader = AutoTokenizer.from_pretrained if is_tokenizer else AutoModel.from_pretrained

    # Try local load
    try:
        print(f"[Local] Trying to load {'tokenizer' if is_tokenizer else 'model'} from {local_dir}")
        return loader(str(local_dir), **kwargs)
    except (OSError, ValueError):
        print(f"[Download] {model_name} → will save to {local_dir}")

    # Download from HF hub
    local_dir.mkdir(parents=True, exist_ok=True)
    instance = loader(model_name, cache_dir=str(local_dir), **kwargs)

    if not is_tokenizer:
        # Fix non-contiguous tensor issue before saving
        state_dict = instance.state_dict()
        for k, v in state_dict.items():
            if not v.is_contiguous():
                state_dict[k] = v.contiguous()
        instance.save_pretrained(str(local_dir), safe_serialization=True, state_dict=state_dict)
    else:
        instance.save_pretrained(str(local_dir))

    return instance


end src\Helpers\model_utils.py

src\Helpers\swinDownload.py
import timm
from pathlib import Path
import shutil

BASE_DIR = Path(__file__).resolve().parent.parent.parent
MODEL_PLACE = BASE_DIR / 'models'
MODEL_PLACE.mkdir(exist_ok=True)

def download_swin(swin_name=None, swin_ckpt_path=None, method=None):
    """
    Download a Swin Transformer checkpoint and save to a target location.

    Parameters
    ----------
    swin_name : str, optional
        Timm model name (default = 'swin_base_patch4_window7_224')
    swin_ckpt_path : str or Path, optional
        Where to save the .safetensors checkpoint.
        If None, defaults to BASE_DIR/models/swin_checkpoint.safetensors
    method : {'copy','move'}, optional
        Whether to copy or move the file (default = move)
    """
    # Pick model name
    model_name = 'swin_base_patch4_window7_224' if swin_name is None else swin_name
    print(f"Triggering download for {model_name}...")
    _ = timm.create_model(model_name, pretrained=True, in_chans=1)

    # Find cached safetensors
    hf_cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
    model_cache_root = next(hf_cache_dir.glob(f"models--timm--{model_name.replace('/', '--')}*"))
    model_file = next(model_cache_root.rglob("model.safetensors"))

    # Decide target
    if swin_ckpt_path is None:
        swin_ckpt_path = MODEL_PLACE / "swin_checkpoint.safetensors"
    else:
        swin_ckpt_path = Path(swin_ckpt_path)
        swin_ckpt_path.parent.mkdir(parents=True, exist_ok=True)

    # Copy or move
    if method == "copy":
        print(f"Copying {model_file} to {swin_ckpt_path}")
        shutil.copy2(model_file, swin_ckpt_path)
    else:
        print(f"Moving {model_file} to {swin_ckpt_path}")
        shutil.move(model_file, swin_ckpt_path)

    print(f"Swin checkpoint saved at: {swin_ckpt_path.resolve()}")
    return swin_ckpt_path

if __name__ == "__main__":
    download_swin()
end src\Helpers\swinDownload.py

src\Helpers\uploadHF.py
from dotenv import load_dotenv
import os
import json
import requests
from pathlib import Path
from huggingface_hub import HfApi, HfFolder, Repository, create_repo, upload_folder

BASE_DIR = Path(__file__).resolve().parent.parent

load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN", "")
repo_id = "ppddddpp/multi-modal-retrieval-predict"

local_dir = BASE_DIR / "upload"

upload_folder(
    repo_id=repo_id,
    folder_path=local_dir,
    repo_type="model",
    path_in_repo="",  # root of the repo
    commit_message="Update model and embedding 24082025" 
)
end src\Helpers\uploadHF.py

src\Helpers\webTestSetContruct.py
import json
import shutil
import random
from pathlib import Path
from DataHandler import parse_openi_xml

BASE_DIR = Path(__file__).resolve().parent.parent.parent
XML_DIR = BASE_DIR / "data/openi/xml/NLMCXR_reports/ecgen-radiology"
DICOM_DIR = BASE_DIR / "data/openi/dicom"
SPLIT_DIR = BASE_DIR / "splited_data"
OUTPUT_DIR = BASE_DIR / "web_test_set"

def create_test_set_for_web(xml_dir=XML_DIR, dicom_dir=DICOM_DIR, split_dir=SPLIT_DIR, output_dir=OUTPUT_DIR, combined_groups=None, num_samples=10):
    """
    Creates a test set for the web application by sampling N records from the test split IDs.

    Parameters
    ----------
    xml_dir : str
        Path to folder containing individual .xml report files
    dicom_dir : str
        Root folder where .dcm files live (possibly nested)
    split_dir : str
        Path to folder containing train, validation, and test split IDs
    output_dir : str
        Path to save the test set to
    combined_groups : dict of str to list of str
        Dictionary where keys are disease/normal group names and values are lists of labels
    num_samples : int
        Number of test samples to generate

    Returns
    -------
    None

    Saves a JSON file with the following format: {<rid>: {<dicom_path>, <report_text>}} to the output directory
    """
    # Load test IDs
    with open(split_dir / "test_split_ids.json") as f:
        test_ids = json.load(f)

    if combined_groups is None:
        raise ValueError("Please provide a least a list of disease groups and normal groups to label the report with.")

    # Parse all records
    records = parse_openi_xml(xml_dir, dicom_dir, combined_groups=combined_groups)
    record_map = {r["id"]: r for r in records}

    # Choose N samples
    sampled_ids = random.sample(test_ids, num_samples)
    meta_info = {}
    output_dir = OUTPUT_DIR if output_dir is None else Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    output_dcm = output_dir / "dicom"
    output_dcm.mkdir(parents=True, exist_ok=True)
    output_reports = output_dir / "reports"
    output_reports.mkdir(parents=True, exist_ok=True)

    for rid in sampled_ids:
        if rid not in record_map:
            continue
        rec = record_map[rid]
        dicom_src = rec["dicom_path"]
        report_text = rec["report_text"]

        # Copy DICOM file
        dicom_dst = output_dcm / f"{rid}.dcm"
        shutil.copy2(dicom_src, dicom_dst)

        # Save report text
        with open(output_reports / f"{rid}.txt", "w", encoding="utf-8") as f:
            f.write(report_text.strip())

        meta_info[rid] = {
            "dicom": str(dicom_dst.name),
            "report": str(f"{rid}.txt")
        }

    # Save metadata
    with open(output_dir / "meta.json", "w") as f:
        json.dump(meta_info, f, indent=2)

    print(f"Saved {len(meta_info)} test samples to: {output_dir}")

if __name__ == "__main__":
    create_test_set_for_web()
end src\Helpers\webTestSetContruct.py

src\Helpers\__init__.py
from .config import Config
from .contruct_test_db import construct_db_test
from .contructGT import create_gt
from .dumpEmbedding import createDumpEmbedding
from .helper import load_report_lookup_via_parser, find_dicom_file, safe_unpack_topk, resize_to_match, compare_maps, to_numpy, heatmap_to_base64_overlay
from .helper import save_b64_map, attention_to_html, make_attention_maps, kg_alignment_loss, log_and_print, _sanitize_node, safe_roc_auc, safe_avg_precision
from .helper import contrastive_loss
from .model_utils import load_hf_model_or_local
from .swinDownload import download_swin
from .webTestSetContruct import create_test_set_for_web

__all__ = [
    "Config",
    "construct_db_test",
    "create_gt",
    "createDumpEmbedding",
    "load_report_lookup_via_parser",
    "find_dicom_file",
    "safe_unpack_topk",
    "resize_to_match",
    "compare_maps",
    "to_numpy",
    "heatmap_to_base64_overlay",
    "save_b64_map",
    "attention_to_html",
    "make_attention_maps",
    "kg_alignment_loss",
    "load_hf_model_or_local",
    "download_swin",
    "create_test_set_for_web",
    "log_and_print",
    "_sanitize_node",
    "safe_roc_auc",
    "safe_avg_precision",
    "contrastive_loss"
]
end src\Helpers\__init__.py

src\KnowledgeGraph\KG_Builder.py
from pathlib import Path
import json, csv, re
from tqdm import tqdm
from typing import List, Optional, Tuple, Dict
from DataHandler import parse_openi_xml
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
from Helpers import _sanitize_node

BASE_DIR = Path(__file__).resolve().parent.parent.parent
KG_DIR = BASE_DIR / "knowledge_graph"
ONTO_DIR = BASE_DIR / "data" / "ontologies"
KG_DIR.mkdir(parents=True, exist_ok=True)


class KGBuilder:
    """
    Build node/relation vocabularies and triples CSV for training KG embeddings.
    Supports:
      - Dataset-only (OpenI)
      - Ontology-only (RadLex / DOID)
      - Hybrid (dataset + mappings + ontology)
    """

    def __init__(self, out_dir: str = "knowledge_graph", combined_groups: Optional[Dict[str, str]] = None):
        """
        Initialize KGBuilder with output directory and optional combined group labels.

        Args:
            out_dir (str): Output directory for triples.csv and node/relation vocabularies (default: "knowledge_graph")
            combined_groups (Optional[Dict[str, str]]): Optional dictionary of combined group labels (default: all groups combined)
        """
        self.out_dir = Path(out_dir) if Path(out_dir).is_absolute() else (BASE_DIR / out_dir)
        self.out_dir.mkdir(parents=True, exist_ok=True)

        self.triples: List[Tuple[str, str, str, float, str]] = []
        self.node2id: Dict[str, int] = {}
        self.relation2id: Dict[str, int] = {}
        self.entity_meta: Dict[int, str] = {}

        self.combined_groups = {**disease_groups, **normal_groups, **finding_groups, **symptom_groups} \
                                if combined_groups is None else combined_groups
        self.label_names = sorted(self.combined_groups.keys())

    # ---------- dataset-driven triples ----------
    def build_from_parsed(self, xml_dir: str, dicom_root: str, min_label_conf: float = 0.0):
        print("[KGBuilder] Building triples from dataset...")
        records = parse_openi_xml(xml_dir, dicom_root, combined_groups=self.combined_groups)

        for r in tqdm(records, desc="Dataset records"):
            rid = f"report:{r['id']}"
            iid = f"image:{r['id']}"
            self.triples.append((rid, "REPORT_OF", iid, 1.0, "extracted"))

            vec = r.get("labels", [])
            for idx, v in enumerate(vec):
                if v:
                    label = self.label_names[idx]
                    lbl_node = f"label:{_sanitize_node(label)}"
                    self.triples.append((rid, "HAS_LABEL", lbl_node, 1.0, "extracted"))

        print(f"[KGBuilder] Dataset triples added: {len(self.triples)}")

    # ---------- ontology mapping triples ----------
    def add_ontology_mapping(self, mapping_path: Path):
        if not mapping_path.exists():
            print(f"[KGBuilder] Mapping file not found: {mapping_path}")
            return
        mapping = json.loads(mapping_path.read_text(encoding="utf8"))

        for group, labels in tqdm(mapping.items(), desc=f"Ontology mapping {mapping_path.name}"):
            for label, ont in labels.items():
                lbl_node = f"label:{_sanitize_node(label)}"
                if ont and not str(ont).startswith("LOCAL:"):
                    ont_node = f"onto:{_sanitize_node(ont)}"
                    self.triples.append((lbl_node, "MAPPED_TO", ont_node, 1.0, "mapping"))

        print(f"[KGBuilder] Mapping triples added from {mapping_path.name}: {len(self.triples)}")

    # ---------- ontology-native triples ----------
    def add_doid(self, doid_path: Path):
        if not doid_path.exists():
            print(f"[KGBuilder] DOID not found: {doid_path}")
            return

        print(f"[KGBuilder] Parsing DOID from {doid_path.name}...")
        with doid_path.open(encoding="utf8") as f:
            lines = f.readlines()

        cur_id = None
        for line in tqdm(lines, desc="DOID terms"):
            line = line.strip()
            if line == "[Term]":
                cur_id = None
                continue
            if line.startswith("id: DOID:"):
                cur_id = line.split("id: ")[1]
            elif line.startswith("is_a:"):
                parent = line.split("is_a: ")[1].split()[0]
                if cur_id and parent:
                    self.triples.append((f"doid:{cur_id}", "is_a", f"doid:{parent}", 1.0, "doid"))

        print(f"[KGBuilder] DOID triples added: {len(self.triples)}")

    def add_radlex(self, radlex_path: Path):
        if not radlex_path.exists():
            print(f"[KGBuilder] RadLex not found: {radlex_path}")
            return

        print(f"[KGBuilder] Parsing RadLex from {radlex_path.name}...")
        triples_before = len(self.triples)

        with radlex_path.open(encoding="utf8") as f:
            buf = []
            for line in tqdm(f, desc="RadLex scan"):
                buf.append(line.strip())
                if "</owl:Class>" in line:  # end of a class block
                    block = " ".join(buf)
                    buf = []
                    sub_match = re.search(r'rdf:about=".*?(RID\d+)"', block)
                    parent_match = re.search(r'rdfs:subClassOf rdf:resource=".*?(RID\d+)"', block)
                    if sub_match and parent_match:
                        sub, parent = sub_match.group(1), parent_match.group(1)
                        self.triples.append((f"radlex:{sub}", "is_a", f"radlex:{parent}", 1.0, "radlex"))

        print(f"[KGBuilder] RadLex triples added: {len(self.triples) - triples_before}")

    # ---------- curated CSV ----------
    def add_curated_csv(self, csv_path: str):
        p = Path(csv_path)
        if not p.exists():
            raise FileNotFoundError(csv_path)

        print(f"[KGBuilder] Adding curated triples from {p.name}...")
        with p.open(newline='', encoding='utf8') as f:
            reader = list(csv.DictReader(f))

        for row in tqdm(reader, desc="Curated CSV"):
            s = row['s']; r = row['r']; o = row['o']
            conf = float(row.get('confidence', 1.0))
            self.triples.append((s, r, o, conf, "curated"))

        print(f"[KGBuilder] Curated triples added: {len(self.triples)}")

    # ---------- vocab building ----------
    def _ensure_vocab(self):
        next_n, next_r = 0, 0
        for s, r, o, conf, src in self.triples:
            for ent in (s, o):
                if ent not in self.node2id:
                    self.node2id[ent] = next_n
                    self.entity_meta[next_n] = ent
                    next_n += 1
            if r not in self.relation2id:
                self.relation2id[r] = next_r
                next_r += 1

    # ---------- save ----------
    def save(self):
        self._ensure_vocab()
        tpath = self.out_dir / "triples.csv"
        with tpath.open('w', newline='', encoding='utf8') as f:
            writer = csv.writer(f)
            writer.writerow(['s_id', 'r_id', 'o_id', 'confidence', 'source'])
            for s, r, o, conf, src in self.triples:
                writer.writerow([self.node2id[s], self.relation2id[r], self.node2id[o], conf, src])

        (self.out_dir / "node2id.json").write_text(json.dumps(self.node2id, indent=2, ensure_ascii=False))
        (self.out_dir / "relation2id.json").write_text(json.dumps(self.relation2id, indent=2, ensure_ascii=False))
        (self.out_dir / "entity_meta.json").write_text(json.dumps(self.entity_meta, indent=2, ensure_ascii=False))
        print(f"[KGBuilder] wrote: {tpath} and maps to {self.out_dir}")

    # ---------- build entrypoint ----------
    def build(self, xml_dir: Optional[str] = None, dicom_root: Optional[str] = None,
              curated_csv: Optional[str] = None, mode: str = "hybrid"):
        """
        mode: "dataset", "ontology", "hybrid"
        """
        if mode == "dataset":
            self.build_from_parsed(xml_dir, dicom_root)

        elif mode == "ontology":
            self.add_doid(ONTO_DIR / "doid.obo")
            self.add_radlex(ONTO_DIR / "RadLex.owl")

        elif mode == "hybrid":
            self.build_from_parsed(xml_dir, dicom_root)
            for mp in ["disease_label2ontology.json", "finding_label2ontology.json",
                       "normal_label2ontology.json", "symptom_label2ontology.json"]:
                self.add_ontology_mapping(ONTO_DIR / mp)
            self.add_doid(ONTO_DIR / "doid.obo")
            self.add_radlex(ONTO_DIR / "RadLex.owl")
        else:
            raise ValueError(f"Unknown KG mode: {mode}")

        if curated_csv:
            self.add_curated_csv(curated_csv)

        self.save()

    # ---------- ensure triples exist ----------
    @classmethod
    def ensure_exists(cls, xml_dir: Optional[str] = None, dicom_root: Optional[str] = None,
                      curated_csv: Optional[str] = None, mode: str = "dataset"):
        tpath = KG_DIR / "triples.csv"
        if not tpath.exists():
            print(f"[KGBuilder] triples.csv not found, building KG in mode={mode}...")
            builder = cls()
            builder.build(xml_dir=xml_dir, dicom_root=dicom_root,
                          curated_csv=curated_csv, mode=mode)
        else:
            print(f"[KGBuilder] triples.csv already exists -> using cached KG (mode={mode})")

end src\KnowledgeGraph\KG_Builder.py

src\KnowledgeGraph\KG_Trainer.py
from pathlib import Path
import json
import csv
import os
import numpy as np
import random
from typing import List, Optional, Tuple, Dict
import datetime
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import wandb

BASE_DIR = Path(__file__).resolve().parent.parent.parent
KG_DIR = BASE_DIR / "knowledge_graph"
KG_DIR.mkdir(parents=True, exist_ok=True)

class TransEModel(nn.Module):
    def __init__(self, n_nodes: int, n_rels: int, emb_dim: int = 200, p_norm: int = 1):
        super().__init__()
        self.ent = nn.Embedding(n_nodes, emb_dim)
        self.rel = nn.Embedding(n_rels, emb_dim)
        nn.init.xavier_uniform_(self.ent.weight.data)
        nn.init.xavier_uniform_(self.rel.weight.data)
        self.p = p_norm

    def score(self, s_idx, r_idx, o_idx):
        e_s = self.ent(s_idx)
        e_o = self.ent(o_idx)
        r = self.rel(r_idx)
        diff = e_s + r - e_o
        if self.p == 1:
            return torch.norm(diff, p=1, dim=1)
        else:
            return torch.norm(diff, p=2, dim=1)

class KGTransETrainer:
    """
    Class wrapper for training TransE on the triples.csv created by KGBuilder.

    Example:
        tr = KGTransETrainer(kg_dir="kg", emb_dim=128)
        tr.load_triples()
        tr.train(epochs=10)
        tr.save_embeddings("kg/node_emb.npy", "kg/rel_emb.npy")
    """

    def __init__(self, kg_dir: str = "kg", emb_dim: int = 200, joint_dim: Optional[int] = None, margin: float = 1.0, 
                    lr: float = 1e-3, curated_factor: float = 3.0, device: Optional[str] = None):
        if kg_dir is None:
            self.kg_dir = KG_DIR
        else:
            self.kg_dir = Path(kg_dir) if Path(kg_dir).is_absolute() else (BASE_DIR / kg_dir)
        self.emb_dim = emb_dim
        self.joint_dim = joint_dim or emb_dim  # default to emb_dim
        self.margin = margin
        self.lr = lr
        self.device = torch.device(device) if device else (torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu"))
        self.curated_factor = curated_factor
        # these will be filled by load_triples()
        self.node2id = {}
        self.rel2id = {}
        self.triples = []  # tuples (s,r,o,conf,src)
        self.train_triples = []
        self.val_triples = []
        self.model = None
        self.optimizer = None

        if self.joint_dim != self.emb_dim:
            self.proj_to_kg = nn.Linear(self.joint_dim, self.emb_dim, bias=False).to(self.device)
        else:
            self.proj_to_kg = nn.Identity()

    def load_maps(self):
        with (self.kg_dir / "node2id.json").open(encoding='utf8') as f:
            self.node2id = json.load(f)
        with (self.kg_dir / "relation2id.json").open(encoding='utf8') as f:
            self.rel2id = json.load(f)

    def load_triples(self, triples_csv: str = None):
        # default to kg/triples.csv
        tpath = Path(triples_csv) if triples_csv else self.kg_dir / "triples.csv"
        if not tpath.exists():
            raise FileNotFoundError(tpath)
        # ensure maps loaded
        self.load_maps()
        self.triples = []
        with tpath.open(newline='', encoding='utf8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                s = int(row['s_id']); r = int(row['r_id']); o = int(row['o_id'])
                conf = float(row.get('confidence', 1.0))
                src = row.get('source', 'extracted')

                # boost curated edges
                if src == "curated":
                    conf *= getattr(self, "curated_factor", 3.0)
                
                self.triples.append((s, r, o, conf, src))
        print(f"[KGTransETrainer] loaded {len(self.triples)} triples")

        # train/val split
        random.shuffle(self.triples)
        split = int(0.9 * len(self.triples))
        self.train_triples = self.triples[:split]
        self.val_triples = self.triples[split:]

        # build train-only arrays
        self.pos_s = np.array([t[0] for t in self.train_triples], dtype=np.int64)
        self.pos_r = np.array([t[1] for t in self.train_triples], dtype=np.int64)
        self.pos_o = np.array([t[2] for t in self.train_triples], dtype=np.int64)
        self.pos_conf = np.array([t[3] for t in self.train_triples], dtype=np.float32)

        # init model
        n_nodes = len(self.node2id)
        n_rels = len(self.rel2id)
        self.model = TransEModel(n_nodes, n_rels, self.emb_dim).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)

    def train(self,
            epochs: int = 5,
            batch_size: int = 1024,
            normalize: bool = True,
            save_every: int = 1,
            wandb_config: Optional[dict] = None,
            log_to_wandb: bool = True,
            patience: Optional[int] = None,    # NEW: early stopping patience
            metric: str = "mrr"                # NEW: which metric to monitor
            ):
        """
        Train KG TransE embeddings.

        - Supports optional early stopping with patience.
        - Saves best checkpoint (by chosen metric).
        - Robust wandb handling.
        """
        if self.model is None:
            raise RuntimeError("Call load_triples() first.")

        n = len(self.pos_s)
        steps = max(1, (n + batch_size - 1) // batch_size)

        # wandb init bookkeeping
        started_wandb = False
        if log_to_wandb:
            try:
                if getattr(wandb, "run", None) is None:
                    run_name = wandb_config.get("name") if wandb_config and "name" in wandb_config \
                            else f"kg_train_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    init_kwargs = {
                        "project": wandb_config.get("project", "multi-modal-kg") if wandb_config else "multi-modal-kg",
                        "name": run_name,
                        "config": wandb_config if wandb_config else None,
                        "reinit": True,
                    }
                    if os.environ.get("WANDB_MODE") == "offline" or (wandb_config and wandb_config.get("mode") == "offline"):
                        init_kwargs["mode"] = "offline"
                    wandb.init(**init_kwargs)
                    started_wandb = True
            except Exception as e:
                print(f"[WARN] wandb.init() failed — disabling wandb logging: {e}")
                log_to_wandb = False

        # early stopping bookkeeping
        best_val = -float("inf")
        bad_epochs = 0

        for epoch in tqdm(range(1, epochs + 1), desc="KG Train", unit="epoch"):
            idxs = np.arange(n)
            np.random.shuffle(idxs)
            epoch_loss = 0.0

            for step in tqdm(range(steps), desc=f"Epoch {epoch}/{epochs}", unit="batch", leave=False):
                start = step * batch_size
                end = min((step + 1) * batch_size, n)
                if start >= end:
                    continue
                batch_idx = idxs[start:end]

                s_batch = torch.LongTensor(self.pos_s[batch_idx]).to(self.device)
                r_batch = torch.LongTensor(self.pos_r[batch_idx]).to(self.device)
                o_batch = torch.LongTensor(self.pos_o[batch_idx]).to(self.device)
                conf_batch = torch.FloatTensor(self.pos_conf[batch_idx]).to(self.device)

                # positive scores
                pos_scores = self.model.score(s_batch, r_batch, o_batch)

                # negative sampling
                corrupt_head = np.random.rand(len(batch_idx)) < 0.5
                neg_s = self.pos_s[batch_idx].copy()
                neg_o = self.pos_o[batch_idx].copy()
                rand_nodes = np.random.randint(0, self.model.ent.num_embeddings, size=len(batch_idx))
                neg_s[corrupt_head] = rand_nodes[corrupt_head]
                neg_o[~corrupt_head] = rand_nodes[~corrupt_head]
                neg_s_t = torch.LongTensor(neg_s).to(self.device)
                neg_o_t = torch.LongTensor(neg_o).to(self.device)

                neg_scores = self.model.score(neg_s_t, r_batch, neg_o_t)

                # margin ranking loss
                loss_sample = torch.relu(pos_scores + self.margin - neg_scores)
                loss = (loss_sample * conf_batch).mean()

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                epoch_loss += float(loss.item())

            # normalize embeddings
            if normalize:
                with torch.no_grad():
                    w = self.model.ent.weight.data
                    denom = w.norm(p=2, dim=1, keepdim=True).clamp(min=1e-6)
                    self.model.ent.weight.data = w / denom

            avg_loss = epoch_loss / max(1, steps)
            print(f"[KGTransETrainer] Epoch {epoch}/{epochs} avg_loss={avg_loss:.6f}")

            # prepare logging payload
            log_data = {"kg/epoch": epoch, "kg/loss": avg_loss}

            # evaluate
            if hasattr(self, "val_triples") and len(self.val_triples) > 0:
                try:
                    mrr, hits1, hits10 = self.evaluate(self.val_triples, k=10)
                    print(f"[Eval] MRR={mrr:.4f}, Hits@1={hits1:.4f}, Hits@10={hits10:.4f}")
                    log_data.update({
                        "kg/val_mrr": mrr,
                        "kg/val_hits1": hits1,
                        "kg/val_hits10": hits10,
                    })

                    # early stopping
                    val_score = {"mrr": mrr, "hits1": hits1, "hits10": hits10}[metric]
                    if val_score > best_val:
                        best_val = val_score
                        bad_epochs = 0
                        self.save_embeddings(suffix="best")
                    else:
                        bad_epochs += 1
                        if patience and bad_epochs >= patience:
                            print(f"[EarlyStop] No improvement in {patience} epochs. Stopping at epoch {epoch}.")
                            break

                except Exception as e:
                    print(f"[WARN] evaluation failed: {e}")

            if log_to_wandb:
                try:
                    wandb.log(log_data)
                except Exception as e:
                    print(f"[WARN] wandb.log failed: {e}")

            if epoch % save_every == 0:
                try:
                    self.save_embeddings(suffix=f"epoch{epoch}")
                except Exception as e:
                    print(f"[WARN] save_embeddings failed: {e}")

        if started_wandb and getattr(wandb, "run", None) is not None:
            try:
                wandb.finish()
            except Exception as e:
                print(f"[WARN] wandb.finish failed: {e}")

    def evaluate(self, triples: list, k: int = 10):
        self.model.eval()
        with torch.no_grad():
            ranks = []
            for s, r, o, conf, src in triples:
                s_t = torch.tensor([s], device=self.device)
                r_t = torch.tensor([r], device=self.device)

                # compute scores for all possible objects
                all_objs = torch.arange(self.model.ent.num_embeddings, device=self.device)
                scores = self.model.score(s_t.repeat(len(all_objs)), r_t.repeat(len(all_objs)), all_objs)
                # lower = better (since it's distance)
                rank = torch.argsort(scores).tolist().index(o) + 1
                ranks.append(rank)

            mrr = np.mean([1.0/r for r in ranks])
            hits1 = np.mean([1 if r <= 1 else 0 for r in ranks])
            hits10 = np.mean([1 if r <= 10 else 0 for r in ranks])
        self.model.train()
        return mrr, hits1, hits10

    def save_embeddings(self, node_out: str = None, rel_out: str = None, suffix: str = ""):
        node_out = Path(node_out) if node_out else (self.kg_dir / f"node_embeddings{('_'+suffix) if suffix else ''}.npy")
        rel_out = Path(rel_out) if rel_out else (self.kg_dir / f"rel_embeddings{('_'+suffix) if suffix else ''}.npy")
        np.save(node_out, self.model.ent.weight.detach().cpu().numpy())
        np.save(rel_out, self.model.rel.weight.detach().cpu().numpy())
        print(f"[KGTransETrainer] saved node embeddings -> {node_out}, rel embeddings -> {rel_out}")

end src\KnowledgeGraph\KG_Trainer.py

src\KnowledgeGraph\mapper_runner.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
from ontology_mapper import OntologyMapper
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
from Helpers import log_and_print

BASE_DIR = Path(__file__).resolve().parent.parent.parent
ONTO_DIR = BASE_DIR / 'data' / "ontologies"
LOG_DIR = BASE_DIR / 'mapped' 

mapper = OntologyMapper(use_doid=True, use_radlex=True, use_bioportal=True)
log_file = LOG_DIR / 'mapped_log.txt'

def print_unmapped(nested_mapping, title="", log_file=None):
    """Print unmapped labels per group, one per line."""
    log_and_print(f"\n=== Unmapped Labels Report ({title}) ===", log_file=log_file)
    for group, mapping in nested_mapping.items():
        unmapped = [lbl for lbl, oid in mapping.items()
                    if oid is None or str(oid).startswith("LOCAL:")]
        if unmapped:
            log_and_print(f"\n{group} ({len(unmapped)} unmapped):", log_file=log_file)
            for lbl in unmapped:
                log_and_print(f"  - {lbl}", log_file=log_file)

# ------------------ Diseases ------------------
nested_disease = mapper.map_grouped_labels(disease_groups)
mapper.report_group_coverage(nested_disease)
mapper.save_mapping(nested_disease, ONTO_DIR / "disease_label2ontology.json")
print_unmapped(nested_disease, "Disease Groups", log_file=log_file)

nested_finding = mapper.map_grouped_labels(finding_groups)
mapper.report_group_coverage(nested_finding)
mapper.save_mapping(nested_finding, ONTO_DIR / "finding_label2ontology.json")
print_unmapped(nested_finding, "Finding Groups", log_file=log_file)

nested_normal = mapper.map_grouped_labels(normal_groups)
mapper.report_group_coverage(nested_normal)
mapper.save_mapping(nested_normal, ONTO_DIR / "normal_label2ontology.json")
print_unmapped(nested_normal, "Normal Groups", log_file=log_file)

nested_symptom = mapper.map_grouped_labels(symptom_groups)
mapper.report_group_coverage(nested_symptom)
mapper.save_mapping(nested_symptom, ONTO_DIR / "symptom_label2ontology.json")
print_unmapped(nested_symptom, "Symptom Groups", log_file=log_file)
end src\KnowledgeGraph\mapper_runner.py

src\KnowledgeGraph\ontology_mapper.py
import difflib
import re
import time
import random
import os
from pathlib import Path
import json
import requests
from google import genai
from typing import Dict, List, Optional
from dotenv import load_dotenv

BASE_DIR = Path(__file__).resolve().parent.parent.parent
ONTO_DIR = BASE_DIR / "data" / "ontologies"
ONTO_DIR.mkdir(parents=True, exist_ok=True)

DOID_URL = "http://purl.obolibrary.org/obo/doid.obo"
DOID_PATH = ONTO_DIR / "doid.obo"

RADLEX_URL = "https://bioportal.bioontology.org/ontologies/RADLEX"  # metadata page
RADLEX_PATH = ONTO_DIR / "RadLex.owl"
load_dotenv()
API_KEY = os.getenv("BIOPORTAL_API_KEY")

class OntologyMapper:
    def __init__(self, use_doid: bool = True, use_radlex: bool = False,
                    use_bioportal: bool = True, bioportal_key: Optional[str] = None,
                    api_keys: Optional[List[str]] = None):
        self.name2id: Dict[str, str] = {}
        self.use_bioportal = use_bioportal
        self.bioportal_key = bioportal_key

        # Load Gemini keys
        raw = os.getenv("GEMINI_KEYS", "")
        env_keys = [k.strip() for k in raw.split(",") if k.strip()]
        self.gemini_keys = api_keys if api_keys else env_keys
        if not self.gemini_keys:
            print("[Gemini] Warning: no API keys provided")

        # Track current key index for round-robin usage
        self._key_index = 0

        # BioPortal
        self.bioportal_key = bioportal_key if bioportal_key is not None else os.getenv("BIOPORTAL_API_KEY")

        # Load ontologies
        if use_doid:
            if not DOID_PATH.exists():
                print(f"[OntologyMapper] Downloading Disease Ontology to {DOID_PATH}")
                self._download_file(DOID_URL, DOID_PATH)
            self._load_doid(DOID_PATH)

        if use_radlex:
            if not RADLEX_PATH.exists():
                print(f"[OntologyMapper] Please manually download RadLex OWL from {RADLEX_URL} into {RADLEX_PATH}")
            else:
                self._load_radlex(RADLEX_PATH)

    def _next_gemini_key(self) -> Optional[str]:
        """Rotate through Gemini API keys (round-robin)."""
        if not self.gemini_keys:
            return None
        key = self.gemini_keys[self._key_index]
        self._key_index = (self._key_index + 1) % len(self.gemini_keys)
        return key

    # ------------------------- Loaders -------------------------
    def _download_file(self, url: str, out_path: Path):
        r = requests.get(url)
        r.raise_for_status()
        out_path.write_text(r.text, encoding="utf8")

    def _load_doid(self, path: Path):
        cur_id, cur_syns = None, []
        with open(path, encoding="utf8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("id: DOID:"):
                    cur_id = line.split("id: ")[1]
                    cur_syns = []
                elif line.startswith("name:"):
                    name = line.split("name: ")[1].lower()
                    ont_id = cur_id if cur_id and cur_id.startswith("DOID:") else (f"DOID:{cur_id}" if cur_id else None)
                    if ont_id:
                        self.name2id[name] = ont_id
                elif line.startswith("synonym:"):
                    parts = line.split("\"")
                    if len(parts) > 1:
                        cur_syns.append(parts[1].lower())
                elif line == "" and cur_id:
                    ont_id = cur_id if cur_id.startswith("DOID:") else f"DOID:{cur_id}"
                    for syn in cur_syns:
                        self.name2id[syn] = ont_id
                    cur_id, cur_syns = None, []

    def _load_radlex(self, path: Path):
        # Minimal OWL parser: extract "RIDxxx" IDs and labels
        with open(path, encoding="utf8") as f:
            content = f.read()
            entries = re.findall(
                r'<Class rdf:about=".*?(RID\d+)".*?>.*?<rdfs:label>(.*?)</rdfs:label>',
                content,
                re.S
            )
            for rid, label in entries:
                self.name2id[label.lower()] = f"RADLEX:{rid}"

    def _search_bioportal(self, term: str, ontology: str = "SNOMEDCT") -> Optional[str]:
        """Query BioPortal for a term in SNOMED CT (or other ontologies), with local caching."""
        if not self.bioportal_key:
            return None

        cache_file = ONTO_DIR / "bioportal_cache.json"
        if cache_file.exists():
            cache = json.loads(cache_file.read_text())
        else:
            cache = {}

        # Return cached result if available (even None)
        if term in cache:
            return cache[term]

        url = "https://data.bioontology.org/search"
        params = {
            "q": term,
            "ontologies": ontology,
            "apikey": self.bioportal_key
        }
        try:
            r = requests.get(url, params=params, timeout=10)
            r.raise_for_status()
            data = r.json()
            bio_id = None
            if data.get("collection"):
                best = data["collection"][0]
                bio_id = best.get("@id")  # BioPortal URI (e.g. SNOMED concept)

            # Save result (even None to avoid re-querying)
            cache[term] = bio_id
            cache_file.write_text(json.dumps(cache, indent=2))

            return bio_id
        except Exception as e:
            print(f"[BioPortal] Failed for '{term}': {e}")
            cache[term] = None
            cache_file.write_text(json.dumps(cache, indent=2))
            return None
    
    def _sleep_backoff(self, base: float, attempt: int, cap: float = 60.0):
        """Exponential backoff with jitter, capped at 60s."""
        wait = min(cap, base * (2 ** attempt) + random.uniform(0, base))
        print(f"[Backoff] Sleeping {wait:.1f}s before retry…")
        time.sleep(wait)

    def normalize_with_gemini(self, term: str, max_retries: int = None, base_sleep: float = 2.0) -> Optional[str]:
        """
        Normalize a free-text clinical label using Gemini.
        - Caches results
        - Retries with exponential backoff
        - Disables invalid/expired keys automatically
        """
        if not self.gemini_keys:
            return None

        cache_file = ONTO_DIR / "gemini_cache.json"
        if cache_file.exists():
            cache = json.loads(cache_file.read_text())
        else:
            cache = {}

        if term in cache:
            return cache[term]

        if max_retries is None:
            max_retries = len(self.gemini_keys)  # try each key once

        tried = 0
        result = None
        while tried < max_retries and self.gemini_keys:
            api_key = self._next_gemini_key()
            try:
                client = genai.Client(api_key=api_key)
                prompt = (
                    "You are a clinical terminology assistant. "
                    "Given the following free-text label, return the closest "
                    "canonical disease/finding name from standard ontologies "
                    "(SNOMED CT, DOID, or RadLex). "
                    "Respond ONLY with the cleaned term, no explanation.\n\n"
                    "Return ONLY the exact SNOMED CT preferred term for the following clinical finding.\n\n"
                    f"Label: {term}"
                )
                resp = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=prompt
                )
                cleaned = resp.text.strip()
                if cleaned:
                    result = cleaned
                    break
            except Exception as e:
                err = str(e)
                print(f"[Gemini] Key {api_key[:4]}… failed for '{term}': {err}")

                # Handle specific error types
                if "API_KEY_INVALID" in err or "expired" in err:
                    print(f"[Gemini] Removing invalid key {api_key[:4]}…")
                    self.gemini_keys.remove(api_key)
                    if not self.gemini_keys:
                        break
                elif "RESOURCE_EXHAUSTED" in err or "RATE_LIMIT_EXCEEDED" in err:
                    self._sleep_backoff(base=base_sleep, attempt=tried)
                else:
                    time.sleep(random.uniform(1, 3))

            tried += 1

        cache[term] = result
        cache_file.write_text(json.dumps(cache, indent=2, ensure_ascii=False))

        if result:
            return result

        print(f"[Gemini] All {len(self.gemini_keys)} keys failed for '{term}'")
        return None
    
    # ------------------------- Core mapping -------------------------
    def expand_label(self, lbl: str) -> List[str]:
        """Expand compound group labels into atomic pieces."""
        parts = re.split(r"[\/&\-\(\)]", lbl)
        return [p.strip().lower() for p in parts if p.strip()]

    def map_labels(self, labels: List[str]) -> Dict[str, Optional[str]]:
        mapping = {}
        for lbl in labels:
            candidates = self.expand_label(lbl)
            matched_ids = []

            for cand in candidates:
                # 1. Dictionary lookup
                if cand in self.name2id:
                    matched_ids.append(self.name2id[cand])
                    continue

                # 2. Fuzzy match
                norm2 = re.sub(r"[^a-z0-9 ]", "", cand)
                if norm2 in self.name2id:
                    matched_ids.append(self.name2id[norm2])
                    continue

                close = difflib.get_close_matches(cand, self.name2id.keys(), n=1, cutoff=0.8)
                if close:
                    matched_ids.append(self.name2id[close[0]])
                    continue

                # 3. BioPortal search
                bio_id = None
                if self.use_bioportal:
                    print(f"[BioPortal] Searching for '{cand}'")
                    bio_id = self._search_bioportal(cand, ontology="SNOMEDCT")
                    if bio_id:
                        matched_ids.append(bio_id)
                        continue

                # 4. Gemini normalization + retry BioPortal
                normalized = self.normalize_with_gemini(cand)
                if normalized and normalized.lower() != cand.lower():
                    print(f"[Gemini] Normalized '{cand}' -> '{normalized}'")
                    # try dictionary again
                    if normalized.lower() in self.name2id:
                        matched_ids.append(self.name2id[normalized.lower()])
                    else:
                        bio_id = self._search_bioportal(normalized, ontology="SNOMEDCT")
                        if bio_id:
                            matched_ids.append(bio_id)

            # assign
            if not matched_ids:
                mapping[lbl] = None
            elif len(matched_ids) == 1:
                mapping[lbl] = matched_ids[0]
            else:
                mapping[lbl] = matched_ids
        return mapping

    # ------------------------- Group-level mapping -------------------------
    def map_grouped_labels(self, groups: Dict[str, List[str]], 
                        auto_save: bool = True, 
                        out_path: Path = ONTO_DIR / "nested_label2ontology.json"
    ) -> Dict[str, Dict[str, str]]:
        """
        Map each element inside groups (like disease_groups).
        Returns nested dict: { group_name: { element: ontology_id or LOCAL:... } }
        """
        nested_mapping = {}
        for group_name, elements in groups.items():
            nested_mapping[group_name] = {}
            for el in elements:
                result = self.map_labels([el])
                ont_id = result[el]
                if ont_id is None:  # fallback to pseudo-ID
                    ont_id = f"LOCAL:{el.replace(' ', '_').upper()}"
                nested_mapping[group_name][el] = ont_id

        if auto_save:
            with open(out_path, "w", encoding="utf8") as f:
                json.dump(nested_mapping, f, indent=2, ensure_ascii=False)
            print(f"[OntologyMapper] Nested mapping saved to {out_path}")

        return nested_mapping
    
    # ------------------------- Reporting -------------------------
    def report_group_coverage(self, nested_mapping: Dict[str, Dict[str, str]]):
        """ Print coverage stats per group. """
        print("\n=== Group Coverage Report ===")
        for group_name, mapping in nested_mapping.items():
            total = len(mapping)
            matched = sum(1 for v in mapping.values() if v and not str(v).startswith("LOCAL:"))
            coverage = matched / total if total > 0 else 0.0
            print(f"{group_name}: {matched}/{total} mapped ({coverage:.1%})")

    # ------------------------- Save -------------------------
    def save_mapping(self, nested_mapping: Dict[str, Dict[str, str]], out_path: Path = ONTO_DIR / "nested_label2ontology.json"):
        with open(out_path, "w", encoding="utf8") as f:
            json.dump(nested_mapping, f, indent=2, ensure_ascii=False)
        print(f"[OntologyMapper] Nested mapping saved to {out_path}")
end src\KnowledgeGraph\ontology_mapper.py

src\KnowledgeGraph\__init__.py
from .KG_Builder import KGBuilder
from .KG_Trainer import KGTransETrainer
from .ontology_mapper import OntologyMapper

__all__ = ["KGTransETrainer", "KGBuilder", "OntologyMapper"]
end src\KnowledgeGraph\__init__.py

src\LabelData\labeledData.py
disease_groups = {
    "general": [
        "cardiovascular diseases", "chronic disease", "chronic lung disease",
        "lung disease", "lung diseases", "mediastinal diseases",
        "pulmonary disease", "respiratory tract diseases", "infection"
    ],
    "atelectasis": [
        "atelectasis", "pulmonary atelectasis", "atelectases",
        "discoid atelectasis", "focal atelectasis", "patchy atelectasis",
        "chronic atelectasis", "middle lobe syndrome",
        "obstructive atelectasis", "plate-like atelectasis"
    ],
    "infection_pneumonia": [
        "pneumonia", "right lower lobe pneumonia", "left lower lobe pneumonia",
        "multilobar pneumonia", "postobstructive pneumonia", "viral pneumonias",
        "atypical pneumonias", "pneumonia right lower lobe", "left upper lobe pneumonia",
        "right upper lobe pneumonia", "lower lobe pneumonia", "acute pneumonia",
        "recurrent pneumonia", "chest infection", "mycoplasma pneumoniae",
        "mycoplasma pneumonia", "pneumonia, viral", "pneumonia, mycoplasma",
        "bronchiolitides", "bronchiolitis, viral", "viral bronchiolitis",
        "superimposed infection", "opportunistic infection",
        "respiratory tract infections", "peribronchial pneumonia",
        "aspiration", "bronchiolitis", "bronchitis", "pneumonitis",
        "small airway disease"
    ],
    "fibrosis_ild": [
        "pulmonary fibrosis", "lung diseases, interstitial", "interstitial lung disease",
        "chronic interstitial lung disease", "pneumonitis, interstitial",
        "interstitial pneumonitides", "interstitial disease", "asbestosis",
        "pulmonary fibroses", "chronic fibrosis", "diffuse fibrosis",
        "asbestos exposure"
    ],
    "bronchiectasis": [
        "bronchiectasis", "bronchiectases"
    ],
    "copd_emphysema": [
        "emphysema", "pulmonary emphysema", "pulmonary disease, chronic obstructive",
        "copd", "chronic obstructive pulmonary disease",
        "pulmonary disease, chronic obstructive, severe early-onset",
        "centrilobular emphysema", "chronic obstructive lung disease",
        "emphysemas", "hyperinflation lungs", "hyperexpansion of lung",
        "obstructive pulmonary diseases", "pulmonary disease, obstructive",
        "obstructive lung disease", "lung diseases, obstructive"
    ],
    "cardiac": [
        "cardiomegaly", "heart failure", "congestive heart failure", "cardiac failure",
        "congestive cardiac failure", "cardiomyopathy, dilated", "dilated cardiomyopathies",
        "left ventricular enlargement", "left atrial enlargement", "right atrial enlargement",
        "ventricular hypertrophy", "hypertrophy, left ventricular",
        "dextrocardia"
    ],
    "aortic_vascular": [
        "atherosclerosis", "aortic ectasia", "aortic aneurysm",
        "aortic disease", "aortic dissection", "aneurysm",
        "aneurysm, dissecting", "aortic aneurysm, thoracic",
        "ascending aortic aneurysm", "peripheral vascular disease",
        "dissection", "aortic atherosclerosis"
    ],
    "pulmonary_vascular": [
        "pulmonary embolism", "chronic pulmonary embolism",
        "pulmonary hypertension", "pulmonary artery hypertension",
        "pulmonary arterial hypertension", "absence of right pulmonary artery",
        "hypertension, pulmonary"
    ],
    "malignancy": [
        "lung cancer", "metastatic disease", "metastases", "lymphoma", "lymphomas",
        "breast carcinoma", "thyroid cancer", "prostate cancer", "multiple myeloma",
        "carcinoma", "neoplasm", "neoplastic processes"
    ],
    "granulomatous_infection": [
        "tuberculosis", "tuberculosis, pulmonary", "sarcoidosis", "histoplasmosis",
        "pulmonary aspergillosis", "aspergilloma", "sarcoidoses", "sarcoid",
        "pulmonary sarcoidosis", "sarcoidosis, pulmonary"
    ],
    "others": [
        "asthma", "cystic fibrosis", "reactive airway disease"
    ],
    "msk_degenerative": [
        "degenerative change", "degeneration", "spondylosis", "spinal osteophytosis",
        "degenerative disc diseases", "degenerative disease", "degenerative joint disease",
        "degenerative arthritis", "arthritic changes", "arthritis", "osteophyte", "osteophytes",
        "diffuse idiopathic skeletal hyperostosis", "hyperostosis, diffuse idiopathic skeletal",
        "spondylarthritis", "thoracic spondylosis", "osteoarthritis",
        "osteoarthritis, knee", "cervical arthritis", "calcific tendinitis",
        "tendinopathy", "dish"
    ],
    "bone_metabolic": [
        "osteopenia", "generalized osteopenia", "osteoporoses", "osteoporosis",
        "bone diseases, metabolic", "bone density", "renal osteodystrophies"
    ],
    "granulomatous_chronic": [
        "granulomatous disease", "granulomatous disease, chronic", "granulomatous infection",
        "tuberculoses", "histoplasmoses", "histoplasmoma", "chronic granulomatous disease"
    ],
    "neoplasm_misc": [
        "lung neoplasms", "lung carcinoma", "brain tumor", "malignancy", "tumor",
        "tumor progression", "paratracheal mass", "anterior mediastinal masses",
        "pulmonary metastasis", "thyroid mass", "goiter"
    ],
    "vascular_general": [
        "aneurysms", "arterial abnormality", "atherosclerotic vascular disease",
        "atheroscleroses", "aortic diseases", "aorta tortuous", "venous engorgement",
        "venous hypertension", "ectasia", "vascular aneurysm",
        "vascular diseases"
    ],
    "abdominal_gi": [
        "colonic perforation", "intestinal perforation", "stomach volvulus",
        "organoaxial gastric volvulus", "small bowel obstruction",
        "intestinal obstruction", "cholelithiasis", "cholelithiases", "gallstones",
        "gall stone", "cholecystectomy", "cholecystectomies", "pancreatitis",
        "chronic pancreatitis", "gallbladder", "stomach", "chronic liver disease",
        "liver diseases", "pancreatitis, chronic", "distention", "gastric dilatation"
    ],
    "postop_procedures": [
        "postoperative period", "postoperative"
    ]
}

finding_groups = {
    "general_radiographic": [
        "density", "hypertrophy", "lucency", "markings", "opacity",
        "pulmonary process", "shift", "thickening", "widened mediastinum"
    ],
    "atelectasis_related": [
        "volume loss", "lung volume reduction", "retraction", "obliteration", "hypoventilation", "collapse"
    ],
    "infection_related": [
        "infiltrates", "infiltrate", "consolidation", "lung consolidation", "airspace disease",
        "inflammation", "mucus", "obstruction"
    ],
    "fibrosis_ild_related": [
        "scarring", "cicatrix", "fibrosis", "scar", "interstitial fibrosis",
        "traction bronchiectasis", "bronchial wall thickening", "scars", "fibroses"
    ],
    "copd_emphysema_related": [
        "hyperinflation", "hyperexpansion", "bullous emphysema", "bulla", "air trapping",
        "lung, hyperlucent", "lung hyperinflation", "bullae", "hyperlucent lung",
        "lungs, hyperlucent"
    ],
    "cardiac_central": [
        "enlarged heart", "cardiac shadow", "pulmonary edema", "pulmonary congestion",
        "edema", "congestion", "venous congestion", "interstitial pulmonary edema",
        "alveolar edema", "pericardial effusion", "pericardial fluid", "cardiothoracic ratio",
        "edemas", "chronic edema", "volume overload"
    ],
    "aortic_vascular_findings": [
        "aortic calcifications", "vascular calcification", "tortuous aorta",
        "calcifications of the aorta", "vascular calcifications",
        "dilatation, pathologic", "dilation"
    ],
    "suspicious_findings": [
        "mass", "mass lesion", "nodule", "solitary pulmonary nodule", "multiple pulmonary nodules",
        "cysts", "cyst", "pleural thickening", "mediastinal mass", "benign cysts", "lipoma",
        "hematoma", "hematomas", "mediastinal haematoma", "hemorrhages"
    ],
    "granulomatous_findings": [
        "calcified granuloma", "granuloma", "granulomas", "pulmonary granuloma",
        "cavitation", "hilar calcification", "lung calcification", "bronchial calcification",
        "apical granuloma", "lung granuloma"
    ],
    "lymph_node_findings": [
        "adenopathy", "lymphadenopathy", "hilar adenopathy", "mediastinal lymphadenopathy",
        "calcified lymph nodes", "lymphatic diseases", "lymph nodes", "lymph node",
        "hilar lymphadenopathy", "bilateral hilar adenopathy"
    ],
    "pleural_findings": [
        "pleural effusion", "effusion", "pleural fluid", "loculated effusion", "hemothorax",
        "right-sided pleural effusion", "left-sided pleural effusion", "pleural diseases",
        "empyema", "pleuritis", "bronchopleural fistula", "pulmonary effusion",
        "bilateral pleural effusion", "pleural plaque", "pleural calcification",
        "pleural effusions", "pleural disease", "bronchial fistula", "exudates and transudates"
    ],
    "air_leak": [
        "pneumothorax", "hemopneumothorax", "hydropneumothorax",
        "pneumatocele", "pneumoperitoneum", "pneumomediastinum",
        "mediastinal emphysema", "subcutaneous emphysema", "subcutaneous  emphysema"
    ],
    "msk_findings": [
        "fracture", "rib fracture", "spinal fractures", "humeral fracture", "clavicular fracture",
        "vertebral fracture", "hip fractures", "comminuted fracture", "dislocation",
        "fractures, bone", "rib fractures", "multiple fractures", "non-displaced fracture",
        "displaced fractures", "stable fracture", "fracture healing", "rotated fracture",
        "femoral neck fractures", "fourth rib fracture", "pathologic fractures",
        "fractures, pathologic", "fracture, pathologic", "fracture-dislocation",
        "shoulder dislocation", "fracture, comminuted", "callus", "contusion",
        "displacement", "acromioclavicular separation", "gunshot wounds",
        "wounds, gunshot", "dislocations", "humeral fractures"
    ],
    "hernia_diaphragm": [
        "hiatal hernia", "hernia, hiatal", "hernia, diaphragmatic", "diaphragmatic eventration",
        "elevated diaphragm", "elevated hemidiaphragm", "hiatus hernia", "large hiatal hernia",
        "eventration", "diaphragm eventration", "morgagni hernia", "bochdalek hernia",
        "hernia sac", "hernias", "hernia, hiatus"
    ],
    "spine_related": [
        "deformity", "spine curvature", "scoliosis", "scolioses", "spine injury",
        "spinal injuries", "thoracolumbar scoliosis", "kyphosis", "kyphoses",
        "anterolisthesis", "post traumatic deformity",
        "intervertebral disc degeneration", "intervertebral disc displacement"
    ],
    "bone_findings": [
        "bone island", "subchondral cyst", "subchondral cysts", "exostoses",
        "enchondroma", "calcinosis", "sclerosis", "scleroses", "sclerotic",
        "demineralization", "expansile bone lesions", "bone demineralization"
    ],
    "technical_quality": [
        "artifact", "artifacts", "artefact", "overlapping structures",
        "technical quality of image unsatisfactory", "nipple shadow",
        "air", "gases", "skin fold", "blister"
    ],
    "other_findings_misc": [
        "old injury", "retropulsion", "scaphoid abdomen",
        "funnel chest", "pectus excavatum", "pectus carinatum"
    ],
    "abdominal_misc": [
        "splenic calcification", "hepatic cyst"
    ]
}

symptom_groups = {
    "general_symptoms": [
        "shortness of breath", "sob", "dyspnea", "cp", "chest pain"
    ],
    "vital_signs_physiologic": [
        "hypoxia", "tachycardia", "hypovolemia", "elevated blood pressure", "hypertension"
    ],
    "constitutional": [
        "cachexia", "obese", "obesity, morbid"
    ],
    "symptom_misc": [
        "pain", "paralysis", "muscle spasm"
    ]
}

device_groups = {
    'Surgical & Medical Devices': [
        'mastectomy', 'surgery', 'catheters, indwelling', 'catheters',
        'picc line', 'sternotomy', 'surgical instruments',
        'implanted medical device', 'stent', 'sutures', 'clip',
        'pacemaker, artificial', 'defibrillators', 'cabg',
        'coronary artery bypass', 'spinal fusion', 'vertebroplasty',
        'thoracotomy', 'lobectomy', 'pneumonectomy', 'amputation', 'bypass',
        'drainage', 'arthroplasty, replacement', 'mastectomies',
        'central venous catheters', 'central venous catheter', 'surgical clip',
        'catheter', 'picc', 'defibrillator', 'cardiac monitor',
        'foreign bodies', 'piercing', 'foreign body', 'chest tubes',
        'tracheostomy', 'nephrostomy', 'catheterization', 'cardiac pacing, artificial',
        'catheterization, central venous', 'catheterization, peripheral',
        'bypass grafting', 'bypass grafts', 'coronary artery bypass graft',
        'coronary bypass', 'surgical resection', 'thoracotomies', 'cholecystectomies',
        'closure', 'resections', 'gastric banding', 'breast implants',
        'mammaplasty', 'prostheses and implants', 'joint prosthesis',
        'knee prosthesis', 'electronic cardiac device', 'cervical fusion', 'cervical spinal fusion',
        'fusion', 'internal fixation', 'stents', 'mid sternotomy',
        'cardiopulmonary bypass', 'thoracentesis', 'chest surgery', 'abdominal surgery',
        'esophagectomy', 'lumpectomy', 'colonic interposition',
        'myocardial revascularization', 'renal dialysis', 'dialysis',
        'cardiac pacing', 'nasogastric tube', 'intubation, intratracheal',
        'intubation, gastrointestinal', 'enteral nutrition',
        'in dwelling catheters', 'tube, inserted', 'medical device',
        'biliary stents', 'embolisation', 'tips', 'tipss',
        'arthroplasties', 'bilateral breast implants', 'lung surgeries',
        'prostheses', 'anchors', 'bone anchor', 'arthroplasty, replacement, knee',
        'prosthetic right shoulder', 'tracheostomies',
        'cervical spine surgeries', 'esophagectomies', 'fixation', 
        'fusion procedure', 'in-dwelling catheter', 'laparoscopic surgery', 
        'lumbar spine fusion', 'nephrostomy, percutaneous', 'stabilization',
        'cervical spine fusion', 'spinal rods', 'lines and tubes'
    ],
    'Valvular & Cardiac Structures': [
        'mitral annular calcification', 'aortic valve', 'mitral valve',
        'pulmonary valve', 'coronary vessels', 'heart valve prosthesis',
        'heart valve prosthesis implantation', 'aortic valve replacement',
        'mitral valve replacement'
    ]
}

technical_groups = {
    'Image Quality / Technical': [
        'technical quality of image unsatisfactory', 'rotation', 'artifacts',
        'artefact', 'overlapping structures', 'contrast media',
        'absorptiometry, photon', 'follow-up studies'
    ]
}

procedures_devices_groups = {
    "Procedures & Devices": [
        "prostheses", "prosthetic right shoulder", "breast implants", "bilateral breast implants",
        "prostheses and implants", "joint prosthesis", "arthroplasty, replacement",
        "arthroplasty, replacement, knee", "knee prosthesis", "anchors", "bone anchor",
        "stent", "stents", "cardiac pacing", "cardiac pacing, artificial", "pacemaker, artificial",
        "defibrillators", "defibrillator", "electronic cardiac device", "heart valve prosthesis",
        "heart valve prosthesis implantation", "mitral valve replacement", "aortic valve replacement",
        "clip", "sutures", "central venous catheters", "central venous catheter", "catheters",
        "picc", "picc line", "catheterization", "catheterization, central venous",
        "catheterization, peripheral", "tube, inserted", "tube inserted", "nasogastric tube",
        "chest tubes", "nephrostomy", "nephrostomy, percutaneous", "tipss", "biliary stents", "stent"
    ]
}

context_flag_groups = {
    "Context & Flags": [
        "follow-up studies", "postoperative period", "postoperative",
        "no active disease", "no active cardiopulmonary disease",
        "no acute disease", "no acute cardiopulmonary disease",
        "no acute cardiopulmonary abnormality", "no acute abnormality",
        "clear lungs", "no effusions", "no effusion", "stable chest",
        "no pneumothorax", "no destructive bony lesions"
    ]
}

normal_groups = {
    'Normal': ['normal', 'no indexing', 'azygos lobe',
               'no active disease', 'no acute disease', 'unremarkable',
                'no radiographic evidence of acute cardiopulmonary disease',
                'no acute cardiopulmonary disease', 'no active cardiopulmonary disease',
                'no acute cardiopulmonary abnormality', 'no acute abnormality', 
                'clear lungs', 'no effusions', 'no effusion', 'stable chest',
                'no pneumothorax', 'no destructive bony lesions'
    ]
}

anatomy_groups = {
    'Anatomy': [
        'abdomen', 'adipose tissue', 'aorta', 'aorta, abdominal', 
        'aorta, thoracic', 'ascending aorta', 'axilla', 'azygos vein', 
        'blood vessels', 'bone', 'bone and bones', 'bronchi', 
        'cervical vertebrae', 'chest wall', 'clavicle', 'costophrenic angle', 
        'descending aorta', 'diaphragm', 'diaphragms', 'epicardial fat', 
        'esophagogastric junction', 'esophagus', 'fat pads', 'femur', 
        'gallbladder', 'heart', 'heart atria', 'heart ventricles', 
        'humeral head', 'humerus', 'intervertebral disc', 'intestine, large', 
        'intestine, small', 'jugular veins', 'kidney', 'knee joint', 
        'left atrium', 'left ventricle', 'lumbosacral region', 
        'lumbar vertebrae', 'lung', 'lymph', 'mediastinum', 'neck', 
        'nipple', 'pleura', 'pulmonary arteries', 'pulmonary artery', 
        'pyriform sinus', 'rib', 'ribs', 'right atrium', 'right ventricle', 
        'sacrum', 'shoulder', 'shoulder joint', 'spine', 'sternum', 'stomach', 
        'subclavian vein', 'sulcus', 'thoracic aorta', 'thoracic vertebrae', 
        'thoracic wall', 'thorax', 'thyroid gland', 'torso', 'trachea', 
        'trachea, carina', 'vena cava, superior'
    ]
}

metadata_groups = {
    'Metadata': [r'^Indiana University Chest X-?ray Collection$',  'Indiana University Chest X-ray Collection']
}

end src\LabelData\labeledData.py

src\LabelData\__init__.py
from .labeledData import disease_groups, device_groups, finding_groups, symptom_groups, technical_groups, normal_groups, anatomy_groups


__all__ = [
    "disease_groups", 
    "device_groups", 
    "finding_groups", 
    "symptom_groups", 
    "technical_groups", 
    "normal_groups", 
    "anatomy_groups"
]
end src\LabelData\__init__.py

src\Model\explain.py
import torch
import torch.nn.functional as F
from captum.attr import IntegratedGradients
from typing import Union, List, Dict
import numpy as np
import math
import warnings

class ExplanationEngine:
    def __init__(
        self,
        fusion_model: torch.nn.Module,
        classifier_head: torch.nn.Module,
        image_size=(224,224),
        ig_steps: int = 50,
        device: torch.device = torch.device("cpu")
    ):
        """
        fusion_model    : image+text fusion backbone
        classifier_head : final head mapping fused features to logits
        image_size      : H x W of output heatmaps
        ig_steps        : n steps for Integrated Gradients
        device          : device to run the explanation on
        """
        self.fusion_model    = fusion_model
        self.classifier_head = classifier_head
        self.image_size      = image_size
        self.ig_steps        = ig_steps
        self.device = device
        if fusion_model is not None:
            self.image_proj = torch.nn.Linear(fusion_model.embed_dim, classifier_head[0].in_features).to(device)
            self.text_proj  = torch.nn.Linear(fusion_model.embed_dim, classifier_head[0].in_features).to(device)
        else:
            self.image_proj = None
            self.text_proj  = None

    def avg_heads(self, att):
        """
        Given a tensor of arbitrary shape, if it has a `heads` dimension (i.e., it has shape (B, H, Lq, Lk)),
        average over the `heads` dimension to return a tensor of shape (B, Lq, Lk). Otherwise, return the
        input tensor as-is.
        """
        
        if att is None:
            return None
        att = torch.as_tensor(att) if not torch.is_tensor(att) else att
        if att.dim() == 4:  # (B, H, Lq, Lk)
            return att.mean(dim=1)  # (B, Lq, Lk)
        return att

    def compute_attention_map(self, attn_tensor: torch.Tensor, grid_size: int) -> np.ndarray:
        # accept torch/numpy and various shapes, unify to (B,1,Np)
        """
        Compute attention map from attention tensor.

        Parameters
        ----------
        attn_tensor: torch.Tensor
            The attention tensor.
        grid_size: int
            The size of the output attention map.

        Returns
        -------
        np.ndarray
            The attention map as a numpy array.
        """
        if attn_tensor is None:
            return None
        if not torch.is_tensor(attn_tensor):
            attn_tensor = torch.as_tensor(attn_tensor)
        
        # unify shapes
        if attn_tensor.dim() == 1:
            attn_tensor = attn_tensor.unsqueeze(0).unsqueeze(0)
        elif attn_tensor.dim() == 2:
            # assume (B, Np) or (1, Np)
            attn_tensor = attn_tensor.unsqueeze(1)
        elif attn_tensor.dim() == 3:
            # assume (B,1,Np) or (B,Np,1) -> try to coerce
            if attn_tensor.shape[1] != 1 and attn_tensor.shape[2] == 1:
                attn_tensor = attn_tensor.transpose(1, 2)
        else:
            # unexpected dims -> try to reduce
            attn_tensor = attn_tensor.reshape(attn_tensor.shape[0], 1, -1)

        scores = attn_tensor[0].squeeze().cpu()
        # Normalize
        scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)

        # safety check: expected length == grid_size*grid_size
        expected = grid_size * grid_size
        if scores.numel() != expected:
            # If mismatch, try to resize via interpolation instead of reshaping (fallback)
            grid = scores.view(1, 1, 1, -1) # (1,1,1,N)
            up_t = F.interpolate(grid, size=self.image_size, mode='bilinear', align_corners=False)
            arr = up_t.squeeze().numpy()
            return self.upscale_heatmap(arr, target_size=self.image_size)

        grid = scores.view(1, 1, grid_size, grid_size)
        up = F.interpolate(grid, size=self.image_size, mode='bilinear', align_corners=False)
        return self.upscale_heatmap(up.squeeze().detach().numpy(), target_size=self.image_size)

    def upscale_heatmap(self, heatmap, target_size=None):
        """
        Upsample heatmap to target_size using bilinear interpolation.

        Args:
            heatmap: 2D numpy array of shape (H,W)
            target_size: tuple of ints (H2,W2) for desired output size

        Returns:
            2D numpy array of shape (H2,W2) upsampled from input heatmap
        """
        if target_size is None:
            target_size = self.image_size
        t = torch.tensor(heatmap, dtype=torch.float32).unsqueeze(0).unsqueeze(0)
        t_up = F.interpolate(t, size=target_size, mode='bilinear', align_corners=False)
        return t_up.squeeze().numpy()

    def _forward_patches_batchfirst(self, img_patches, txt_feats, target_idx=None):
        """
        Call the fusion model and classifier head on a batch of image patches and text features.

        Args:
            img_patches: 3D or 4D tensor of shape (B, Np, E) or (B, G1, G2, E) where B is the batch size,
                Np is the number of patches, G1 and G2 are the grid size, and E is the embedding size.
            txt_feats: 3D tensor of shape (B, T, E) where B is the batch size, T is the sequence length,
                and E is the embedding size.
            target_idx: int or None. If int, the index of the target class to compute the gradient with respect to.
                If None, return the logits for all classes.

        Returns:
            If target_idx is None, returns a 2D tensor of shape (B, num_classes) containing the logits for all classes.
            If target_idx is int, returns a 2D tensor of shape (B, 1) containing the logit for the specified class.
        """
        # Flatten if 4D -> (B, G1*G2, E)
        if img_patches.dim() == 4:
            B, G1, G2, E = img_patches.shape
            img_patches = img_patches.view(B, G1 * G2, E)
        elif img_patches.dim() != 3:
            raise ValueError(f"img_patches must be 3D or 4D, got {tuple(img_patches.shape)}")

        img_patches = img_patches.contiguous()
        txt_feats = txt_feats.contiguous()

        # create img_global as mean over patches (same as training)
        img_global = img_patches.mean(dim=1)   # (B, E)

        # call fusion model
        fusion_out = self.fusion_model(
            img_global = img_global,
            img_patch  = img_patches,
            txt_feats  = txt_feats,
            return_attention = False
        )

        if isinstance(fusion_out, (tuple, list)):
            fused = fusion_out[0]
        else:
            fused = fusion_out

        logits = self.classifier_head(fused)  # (B, num_classes)

        if target_idx is None:
            return logits
        else:
            return logits[:, int(target_idx)].unsqueeze(-1)

    def compute_gradcam_map_for_target(self,
                                    img_global: torch.Tensor,
                                    img_patches: torch.Tensor,
                                    txt_feats: torch.Tensor,
                                    target_idx: int,
                                    unimodal: str = None
                                    ) -> np.ndarray:
        """
        Compute Grad-CAM explanation map for a single input.

        Args:
            img_global: (B, E) global image features
            img_patches: (B, Np, E) or (B, G1, G2, E) image patches
            txt_feats:   (B, T, E) text features
            target_idx: int, the index of the target class to explain

        Returns:
            A 2D numpy array of shape (H, W) containing the explanation map.

        Notes:
            - Currently only supports batch size 1.
            - The returned map is normalized to [0,1] per sample.
        """
        device = self.device

        # -------- IMAGE ONLY --------
        if unimodal == "image":
            if img_patches is None:
                raise ValueError("Image-only mode requires img_patches")
            img_patches = img_patches.clone().detach().to(device).requires_grad_(True)
            img_global  = img_patches.mean(dim=1)

            # projection head for image-only
            feat = self.image_proj(img_global)
            logits = self.classifier_head(feat)

            target_score = logits[:, int(target_idx)].sum()
            grads = torch.autograd.grad(target_score, img_patches)[0]
            cam = (grads * img_patches).sum(dim=-1)
            cam = torch.relu(cam)

            # reshape + upscale
            B, Np = cam.shape
            G = int(np.sqrt(Np))
            cam_grid = cam.view(B, G, G).unsqueeze(1)  # (B,1,G,G)
            cam_up = F.interpolate(cam_grid, size=self.image_size, mode="bilinear", align_corners=False)
            out_map = cam_up.squeeze().detach().cpu().numpy()
            return (out_map - out_map.min()) / (out_map.max() - out_map.min() + 1e-8)

        # -------- TEXT ONLY --------
        if unimodal == "text":
            if txt_feats is None:
                raise ValueError("Text-only mode requires txt_feats")
            txt_feats = txt_feats.clone().detach().to(device).requires_grad_(True)

            # projection head for text-only (mean pooling or CLS)
            feat = self.text_proj(txt_feats.mean(dim=1))
            logits = self.classifier_head(feat)

            target_score = logits[:, int(target_idx)].sum()
            grads = torch.autograd.grad(target_score, txt_feats)[0]
            token_importance = (grads * txt_feats).sum(dim=-1).relu()  # (B, T)

            token_norm = (token_importance - token_importance.min(dim=1, keepdim=True)[0]) / \
                        (token_importance.max(dim=1, keepdim=True)[0] - token_importance.min(dim=1, keepdim=True)[0] + 1e-8)
            return token_norm[0].detach().cpu().numpy()  # (T,)

        # -------- MULTIMODAL (default) --------
        # Move to device and ensure patches require grad
        device = getattr(self, "device", img_patches.device)
        img_patches = img_patches.clone().detach().to(device).requires_grad_(True)  # (B,Np,E)
        txt_feats   = txt_feats.clone().detach().to(device)
        img_global  = img_global.clone().detach().to(device)

        fused_out = self.fusion_model(
            img_global = img_patches.mean(dim=1),
            img_patch  = img_patches,
            txt_feats  = txt_feats,
            return_attention = False
        )
        fused_out = fused_out[0] if isinstance(fused_out, (tuple, list)) else fused_out
        logits = self.classifier_head(fused_out)  # (B, num_classes)

        # Pick target score scalar: sum across batch to get a single scalar
        if logits.dim() == 2 and logits.size(1) == 1:
            target_score = logits.squeeze().sum()
        else:
            target_score = logits[:, int(target_idx)].sum()

        # compute gradients w.r.t. patches ONLY (no param grads)
        grads = torch.autograd.grad(
            outputs=target_score,
            inputs=img_patches,
            retain_graph=True,
            create_graph=False,
            allow_unused=True
        )[0]  # (B, Np, E) or None

        if grads is None:
            raise RuntimeError("Gradients wrt img_patches are None. Ensure forward path depends on img_patches.")

        # Channel-weighted sum analogous to Grad-CAM: elementwise product then sum over embedding dim
        cam = (grads * img_patches).sum(dim=-1)  # (B, Np)

        # Keep positive contributions only (ReLU)
        cam = torch.relu(cam)  # (B, Np)

        # Reshape into grid
        B, Np = cam.shape
        G = int(np.sqrt(Np))
        if G * G == Np:
            cam_grid = cam.view(B, G, G)  # (B, G, G)
        else:
            # fallback: treat as 1 x Np and later upsample (keeps spatial ordering if non-square)
            cam_grid = cam.view(B, 1, Np)

        # Upsample to image size (self.image_size should be (H,W) tuple)
        target_size = getattr(self, "image_size", (224, 224))
        cam_grid = cam_grid.unsqueeze(1)  # (B,1,G,G) or (B,1,1,Np)
        cam_up = F.interpolate(cam_grid, size=target_size, mode="bilinear", align_corners=False)  # (B,1,H,W)
        cam_up = cam_up.squeeze(1)  # (B,H,W)

        # Normalize per-sample to [0,1]
        cam_np = cam_up.detach().cpu().numpy()
        out_map = cam_np[0]  # take first sample
        eps = 1e-8
        out_map = (out_map - out_map.min()) / (out_map.max() - out_map.min() + eps)
        out_map = np.nan_to_num(out_map, nan=0.0, posinf=1.0, neginf=0.0)
        return out_map

    def compute_ig_map_for_target(
        self,
        img_global=None,
        img_patches=None,
        txt_feats=None,
        target_idx=0,
        steps=50,
        internal_batch_size=1,
        unimodal: str = None
    ):
        """
        Computes an Integrated Gradients heatmap over image patches for a single target class.

        Args:
            img_global: (B, E) global image features
            img_patches: (B, Np, E) or (B, G, G, E) image patches
            txt_feats:   (B, T, E) text features
            target_idx:  int, index of the target class to explain
            steps:       int, number of IG steps
            internal_batch_size: int, batch size to use for internal computation

        Returns:
            (H, W) numpy heatmap for the first item in batch normalized to [0,1]
        """
        device = self.device
        # -------- IMAGE ONLY --------
        if unimodal == "image":
            if img_patches is None:
                raise ValueError("Image-only IG requires img_patches")
            img_patches = img_patches.clone().detach().to(device).requires_grad_(True)

            ig = IntegratedGradients(lambda ip: self.classifier_head(self.image_proj(ip.mean(dim=1))))
            baselines = torch.zeros_like(img_patches, device=device)

            attributions = ig.attribute(
                inputs=img_patches,
                baselines=baselines,
                n_steps=steps,
                internal_batch_size=internal_batch_size,
                target=int(target_idx)
            )  # (B, Np, E)

            att = attributions.norm(p=1, dim=-1)  # (B, Np)
            G = int(np.sqrt(att.size(1)))
            att = att.view(-1, G, G)
            att = (att - att.min()) / (att.max() - att.min() + 1e-8)

            return self.upscale_heatmap(att[0].detach().cpu().numpy(), target_size=self.image_size)

        # -------- TEXT ONLY --------
        if unimodal == "text":
            if txt_feats is None:
                raise ValueError("Text-only IG requires txt_feats")
            txt_feats = txt_feats.clone().detach().to(device).requires_grad_(True)

            ig = IntegratedGradients(lambda t: self.classifier_head(self.text_proj(t.mean(dim=1))))
            baselines = torch.zeros_like(txt_feats, device=device)

            attributions = ig.attribute(
                inputs=txt_feats,
                baselines=baselines,
                n_steps=steps,
                internal_batch_size=internal_batch_size,
                target=int(target_idx)
            )  # (B, T, E)

            token_scores = attributions.norm(p=1, dim=-1)  # (B, T)
            token_scores = (token_scores - token_scores.min(dim=1, keepdim=True)[0]) / \
                        (token_scores.max(dim=1, keepdim=True)[0] - token_scores.min(dim=1, keepdim=True)[0] + 1e-8)
            return token_scores[0].detach().cpu().numpy()  # (T,)
        
        # -------- MULTIMODAL (default) --------
        img_patches = img_patches.clone().detach().to(self.device).requires_grad_(True)
        txt_feats   = txt_feats.clone().detach().to(self.device)

        # Use IntegratedGradients with a forward that accepts only the image patches
        ig = IntegratedGradients(lambda ip: self._forward_patches_batchfirst(ip, txt_feats, target_idx))

        # Baseline = zeros like image patches
        baselines = torch.zeros_like(img_patches, device=self.device)

        # Attribute (use small internal_batch_size to reduce memory / intermediate views)
        attributions = ig.attribute(
            inputs=img_patches,
            baselines=baselines,
            n_steps=steps,
            internal_batch_size=internal_batch_size,
            target=int(target_idx)
        )  # (B, Np, E) or (B, G, G, E)

        # Reduce embed dim to single importance per patch
        if attributions.dim() == 4:
            B, G1, G2, E = attributions.shape
            assert G1 == G2, f"Expected square grid for patches, got {G1}x{G2}"
            att = attributions.norm(p=1, dim=-1).view(B, G1 * G2)  # L1 over E
            G = G1
            att = att.view(B, G, G)
        else:
            B, Np, E = attributions.shape
            G = int(Np ** 0.5)
            assert G * G == Np, f"Number of patches {Np} is not a perfect square"
            att = attributions.norm(p=1, dim=-1)  # (B, Np)
            att = att.view(B, G, G)

        # ensure float
        att = att.to(dtype=torch.float32)

        # robust min/max over last two dims (works across PyTorch versions)
        try:
            min_vals = att.amin(dim=(-2, -1), keepdim=True)   # (B,1,1)
            max_vals = att.amax(dim=(-2, -1), keepdim=True)   # (B,1,1)
        except Exception:
            flat = att.view(B, -1)                             # (B, G*G)
            min_vals = flat.min(dim=1, keepdim=True)[0].view(B, 1, 1)
            max_vals = flat.max(dim=1, keepdim=True)[0].view(B, 1, 1)

        # Normalize to [0,1] safely
        range_vals = (max_vals - min_vals)
        att = (att - min_vals) / (range_vals + 1e-8)
        att = att.clamp(0.0, 1.0)

        # guard against any NaNs
        att = torch.nan_to_num(att, nan=0.0, posinf=1.0, neginf=0.0)

        ig_up = self.upscale_heatmap(att[0].detach().cpu().numpy(), target_size=self.image_size)
        return ig_up

    def _attn_to_patch_tensor(self, att, img_patches, method="mean"):
        """
        Convert attention weights of arbitrary (B, *, *) shape into a torch tensor shaped (B,1,Np)
        where Np = number of image patches (img_patches.shape[1]).
        Returns None if it cannot reliably infer a per-patch map.

        Parameters:
            att (torch tensor or None): attention weights
            img_patches (torch tensor): image patches (B, Np, E)
            method (str, optional): how to reduce attention weights when Lk > 1. Defaults to "mean"

        Returns:
            torch tensor (B,1,Np) or None
        """
        if att is None:
            return None

        # convert to torch tensor on same device as img_patches for shape checking
        if not torch.is_tensor(att):
            try:
                att = torch.as_tensor(att, device=img_patches.device)
            except Exception:
                att = torch.tensor(att, device=img_patches.device)

        # If heads dimension present (B, heads, Lq, Lk) -> average heads
        if att.dim() == 4:
            # assume shape (B, heads, Lq, Lk)
            att = att.mean(dim=1)  # -> (B, Lq, Lk)

        # now expect (B, Lq, Lk) or (Lq, Lk) or (B, L)
        if att.dim() == 3:
            B, Lq, Lk = att.shape
            Np = img_patches.shape[1]

            # Case: txt2img typical -> (B, 1, Np)
            if Lk == Np and Lq == 1:
                return att[:, 0:1, :].detach()

            # Case: img2txt when keys pooled -> (B, Np, 1)
            if Lq == Np and Lk == 1:
                return att[:, :, 0:1].transpose(1, 2).detach()  # (B,1,Np)

            # Case: img2txt when keys are tokens -> (B, Np, L_tokens)
            if Lq == Np and Lk > 1:
                if method == "mean":
                    per_patch = att.mean(dim=-1)  # (B, Np)
                elif method == "max":
                    per_patch = att.max(dim=-1)[0]
                else:
                    per_patch = att.mean(dim=-1)
                return per_patch.unsqueeze(1).detach()  # (B,1,Np)

            # Case: tokens->patches -> (B, L_tokens, Np)
            if Lk == Np and Lq > 1:
                per_patch = att.mean(dim=1)  # average queries -> (B, Np)
                return per_patch.unsqueeze(1).detach()

            # ambiguous: try aggregating over keys to yield (B, Lq) and see if Lq == Np
            agg_key = att.mean(dim=-1)  # (B, Lq)
            if agg_key.shape[1] == Np:
                return agg_key.unsqueeze(1).detach()

            # try averaging over queries -> (B, Lk)
            agg_q = att.mean(dim=1)
            if agg_q.shape[1] == Np:
                return agg_q.unsqueeze(1).detach()

            # give up -> return None
            return None

        # att.dim() == 2 -> maybe (B, Np) or (Np,) etc.
        if att.dim() == 2:
            if att.shape[1] == img_patches.shape[1]:
                return att.unsqueeze(1).detach()
            # maybe shape (Np, ) or (1, Np)
            if att.shape[0] == img_patches.shape[1]:
                return att.unsqueeze(0).unsqueeze(1).detach()
            return None

        # other dims -> cannot handle
        return None
    
    def compute_comb_token_vector(
        self,
        att_comb,
        txt_feats,
        attn_weights=None,
        *,
        eps=1e-8,
        atol_zero=1e-7,
        debug=False
    ):
        """
        Robust cascade to produce a token-level importance map [B, 1, T] from
        the combined attention `att_comb` and text features `txt_feats`.

        Order tried:
        1) self.comb_attention_to_token_vector(...) (existing heuristic)
        2) self._attn_to_token_tensor(att_comb, txt_feats, method="mean")
            (uses robust normalizer that returns uniform if constant)
        3) collapse heads/queries -> softmax over tokens -> attention-weighted tokens
        4) fallback to attn_weights['img2txt'] -> self.img2txt_to_token_vector(...)
        5) final fallback: mean token tensor via _attn_to_token_tensor (guaranteed non-empty)

        Returns:
        torch.Tensor of shape [B, 1, T] (float) always non-None.
        """
        device = txt_feats.device if txt_feats is not None else (att_comb.device if att_comb is not None else None)
        dtype = txt_feats.dtype if txt_feats is not None else torch.float32

        # 0: safety
        if txt_feats is None:
            # nothing to compute — return zeros shaped [B,1,1] to avoid None
            return None

        B, T, E = txt_feats.shape

        # 1) Try primary heuristic
        try:
            t_from_comb = self.comb_attention_to_token_vector(att_comb, txt_feats, min_mass_ratio=0.0)
            if debug:
                print("[DBG compute_comb_token_vector] primary comb_attention_to_token_vector ->",
                    "None" if t_from_comb is None else t_from_comb.shape)
        except Exception as e:
            if debug:
                print(f"[DBG compute_comb_token_vector] primary heuristic raised: {e}")
            t_from_comb = None

        # 2) Fallback: robust reduction to token tensor (this should produce uniform if constant)
        if t_from_comb is None:
            try:
                t_from_comb = self._attn_to_token_tensor(att_comb, txt_feats, method="mean")
                if debug:
                    print("[DBG compute_comb_token_vector] used _attn_to_token_tensor", t_from_comb.shape)
            except Exception as e:
                if debug:
                    print(f"[DBG compute_comb_token_vector] _attn_to_token_tensor failed: {e}")
                t_from_comb = None

        # Helper to check "all zeroish"
        def is_zeroish(t):
            return t is None or torch.allclose(t, torch.zeros_like(t), atol=atol_zero)

        # 3) If still degenerate, attempt collapsing heads/queries into token importance
        if not (t_from_comb is None) and is_zeroish(t_from_comb):
            # treat degenerate as not useful and try a recompute
            t_from_comb = None

        if t_from_comb is None:
            try:
                if att_comb is not None:
                    # collapse heads: want [B, Lq, Lk] or [B, Lk]
                    att_tmp = self.avg_heads(att_comb)
                    if debug:
                        print("[DBG compute_comb_token_vector] avg_heads ->", getattr(att_tmp, "shape", None))

                    # If att_tmp is [B, Lq, Lk], average queries (Lq) to get token weights
                    if att_tmp.dim() == 3:
                        att_tok = att_tmp.mean(dim=1)  # [B, Lk]
                    elif att_tmp.dim() == 2:
                        att_tok = att_tmp
                    else:
                        att_tok = att_tmp.reshape(att_tmp.shape[0], -1)

                    # Align length with txt_feats' T
                    B2, Lk = att_tok.shape
                    if Lk != T:
                        if Lk > T:
                            att_tok = att_tok[:, :T]
                        else:
                            pad = torch.full((B2, T - Lk), fill_value=1e-6, device=att_tok.device, dtype=att_tok.dtype)
                            att_tok = torch.cat([att_tok, pad], dim=1)

                    # stable softmax
                    att_tok = torch.nn.functional.softmax(att_tok, dim=-1)  # [B, T]
                    t_from_comb = att_tok.unsqueeze(1)
                    if debug:
                        print("[DBG compute_comb_token_vector] recomputed att_tok ->", t_from_comb.shape)
            except Exception as e:
                if debug:
                    print(f"[DBG compute_comb_token_vector] collapsed att recompute failed: {e}")
                t_from_comb = None

        # 4) Try img2txt alternative if still degenerate and attn_weights provided
        if is_zeroish(t_from_comb) and attn_weights is not None:
            try:
                att_img2txt = attn_weights.get("img2txt", None)
                if att_img2txt is not None:
                    t_alt = self.img2txt_to_token_vector(att_img2txt, txt_feats)
                    if not is_zeroish(t_alt):
                        t_from_comb = t_alt
                        if debug:
                            print("[DBG compute_comb_token_vector] used img2txt_to_token_vector")
            except Exception as e:
                if debug:
                    print(f"[DBG compute_comb_token_vector] img2txt fallback failed: {e}")

        # 5) Final fallback: guaranteed non-empty via robust normalizer
        if t_from_comb is None or is_zeroish(t_from_comb):
            try:
                t_from_comb = self._attn_to_token_tensor(att_comb, txt_feats, method="mean")
                if debug:
                    print("[DBG compute_comb_token_vector] final fallback to mean token tensor", t_from_comb.shape)
            except Exception as e:
                if debug:
                    print(f"[DBG compute_comb_token_vector] final fallback failed: {e}")
                # As absolute last resort, return uniform
                uniform = torch.ones((B, 1, T), device=device, dtype=dtype) / float(T)
                t_from_comb = uniform
                if debug:
                    print("[DBG compute_comb_token_vector] returned uniform last-resort", t_from_comb.shape)

        # ensure shape [B,1,T] and dtype/device
        t_from_comb = t_from_comb.to(device=device, dtype=dtype)
        return t_from_comb

    def _attn_to_token_tensor(self, att, txt_feats, method="mean"):
        """
        Fallback: reduce attention matrix to token-level importance scores.

        Args:
            att (torch.Tensor): raw attention [B, H, T, T] or [B, T, T].
            txt_feats (torch.Tensor): text features [B, T, D].
            method (str): "mean" (default) or "max".

        Returns:
            torch.Tensor: reduced token-level vector [B, 1, T], normalized to [0,1].
        """
        if att is None:
            return None

        # ensure [B, H, T, T]
        if att.dim() == 3:
            att = att.unsqueeze(1)

        B, H, T, _ = att.shape

        # reduce across heads
        if method == "mean":
            v = att.mean(dim=1)   # [B, T, T]
        elif method == "max":
            v = att.max(dim=1)[0] # [B, T, T]
        else:
            raise ValueError(f"Unknown reduction method: {method}")

        # collapse context dimension -> get a single score per token
        v = v.mean(dim=-1, keepdim=True).transpose(1, 2)  # [B, 1, T]

        # Robust normalization to [0,1]. If the map is constant (max==min),
        # return a uniform distribution instead of a zero vector.
        v_min = v.min(dim=-1, keepdim=True)[0]   # [B,1,1]
        v_max = v.max(dim=-1, keepdim=True)[0]   # [B,1,1]
        range_ = (v_max - v_min)

        eps = 1e-8
        is_constant = (range_.abs() < eps).squeeze(-1).squeeze(-1)  # [B] boolean per sample

        v_norm = (v - v_min) / (range_ + eps)

        if is_constant.any():
            # uniform over tokens for constant samples
            uniform = torch.ones_like(v_norm) / float(T)  # [B,1,T]
            mask = is_constant.view(B, 1, 1)
            v_norm = torch.where(mask, uniform, v_norm)

        v_norm = v_norm.clamp(0.0, 1.0)
        return v_norm.detach()

    def comb_attention_to_patch_vector(self, att, img_patches, min_mass_ratio=0.12):
        return self._comb_helper(att, img_patches, target_len=img_patches.shape[1], min_mass_ratio=min_mass_ratio, swap=False)

    def comb_attention_to_token_vector(self, att, txt_feats, min_mass_ratio=0.12):
        return self._comb_helper(att, txt_feats, target_len=txt_feats.shape[1], min_mass_ratio=min_mass_ratio, swap=True)

    def txt2img_to_patch_vector(self, att_txt2img, img_patches, reduction="mean"):
        """
        Extract a per-patch attention vector from the txt2img attention map.

        Args:
            att_txt2img (torch.Tensor): raw attention [B, H, T, Np] or [B, T, Np]
            img_patches (torch.Tensor): image patches [B, Np, E]
            reduction (str): reduction method, either "mean" (default) or "max"

        Returns:
            torch.Tensor: per-patch vector [B, 1, Np], normalized to [0,1]
        """
        att = self.avg_heads(att_txt2img)
        if att is None or att.dim() != 3:
            return None
        per_patch = att.mean(dim=1)
        return per_patch.unsqueeze(1).detach()

    def img2txt_to_token_vector(self, att_img2txt, txt_feats, reduction="mean"):
        """
        Extract a per-token attention vector from the img2txt attention map.

        Args:
            att_img2txt (torch.Tensor): raw attention [B, H, T, Nt] or [B, T, Nt]
            txt_feats (torch.Tensor): text features [B, Nt, D]
            reduction (str): reduction method, either "mean" (default) or "max"

        Returns:
            torch.Tensor: per-token vector [B, 1, Nt], normalized to [0,1]
        """
        att = self.avg_heads(att_img2txt)
        if att is None or att.dim() != 3:
            return None
        per_token = att.mean(dim=1)
        return per_token.unsqueeze(1).detach()
    
    def _comb_helper(self, att, other, target_len, min_mass_ratio=0.12, swap=False):
        """
        Generic sliding-window helper. If swap==False: slide over keys for patches.
        If swap==True: extract token block by swapping Lq/Lk roles.
        """
        if att is None:
            return None
        att = self.avg_heads(att)
        if att is None or att.dim() != 3:
            return None
        
        if other is not None and hasattr(other, "device"):
            try:
                att = att.to(other.device)
            except Exception:
                warnings.warn(f"Cannot transfer attention matrix to {other.device}; continuing on att's device")

        B, Lq, Lk = att.shape
        N = int(target_len)
        # quick exact matches
        if Lk == N:
            return att.mean(dim=1).unsqueeze(1).detach()   # (B,1,N)
        if Lq == N:
            return att.mean(dim=-1).unsqueeze(1).detach()
        # choose primary axis to slide
        if not swap:
            primary_len = Lk
            sums = att.sum(dim=1)  # (B, Lk)
        else:
            primary_len = Lq
            sums = att.sum(dim=-1)  # (B, Lq)

        if primary_len < N:
            return None

        cumsum = torch.cumsum(sums, dim=-1)
        per_sample = []
        for b in range(B):
            row = cumsum[b]
            if primary_len == N:
                wins = row[-1].unsqueeze(0)
            else:
                end = row[N-1:]
                start = torch.cat((torch.tensor([0.0], device=row.device), row[:-N]))
                wins = end - start
            max_val, max_idx = torch.max(wins, dim=0)
            total = (sums[b].sum() + 1e-12)
            if (max_val / total) < min_mass_ratio:
                per_sample.append(torch.zeros(N, device=att.device))
                continue
            off = int(max_idx.item())
            if not swap:
                slice_block = att[b:b+1, :, off:off+N]   # (1, Lq, N)
                per_vec = slice_block.mean(dim=1).squeeze(0)  # (N,)
            else:
                slice_block = att[b:b+1, off:off+N, :]   # (1, N, Lk)
                per_vec = slice_block.mean(dim=-1).squeeze(0)
            per_sample.append(per_vec)
        out = torch.stack(per_sample, dim=0)  # (B, N)
        return out.unsqueeze(1).detach()      # (B,1,N)

    def explain(
        self,
        img_global: torch.Tensor,
        img_patches: torch.Tensor,
        txt_feats: torch.Tensor,
        attn_weights: Dict[str,torch.Tensor],
        targets: Union[int, List[int]]
    ) -> Dict[str, Union[np.ndarray, Dict[int,np.ndarray]]]:
        # Attention
        """
        Computes explanation maps for a single input using three methods.

        Args:
            img_global: (B, E) global image features
            img_patches: (B, Np, E) or (B, G, G, E) image patches
            txt_feats:   (B, T, E) text features
            attn_weights: attention weights from fusion layer, expected to have key 'txt2img'
            targets: single int or list of ints for the class indices to explain

        Returns:
            dict with three keys:
                - 'attention_map': (H, W) attention map over image patches
                - 'ig_maps':       dict of target to (H, W) IG map over image patches
                - 'gradcam_maps':  dict of target to (H, W) Grad-CAM map over image patches
        """
        attention_maps = {}

        # txt2img -> per-patch
        att_txt2img = attn_weights.get("txt2img", None)
        p_from_txt = None
        if att_txt2img is not None:
            p_from_txt = self.txt2img_to_patch_vector(att_txt2img, img_patches)
            if p_from_txt is not None:
                N = p_from_txt.shape[-1]
                G = int(math.sqrt(N)) if int(math.sqrt(N)) ** 2 == N else None
                if G is None:
                    print(f"[WARN] txt2img skip: {N} not square")
                else:
                    attention_maps["txt2img"] = self.compute_attention_map(p_from_txt, grid_size=G)

        # img2txt -> per-token
        att_img2txt = attn_weights.get("img2txt", None)
        t_from_img = None
        if att_img2txt is not None:
            t_from_img = self.img2txt_to_token_vector(att_img2txt, txt_feats)
            if t_from_img is not None:
                attention_maps["img2txt"] = t_from_img[0, 0].cpu().numpy()

        # comb -> try both patch and token extraction (heuristic)
        att_comb = attn_weights.get("comb", None)
        p_from_comb = None
        t_from_comb = None

        if att_comb is not None:
            # patch-level comb
            p_from_comb = self.comb_attention_to_patch_vector(att_comb, img_patches, min_mass_ratio=0.06)
            if p_from_comb is None:
                p_from_comb = self._attn_to_patch_tensor(att_comb, img_patches, method="mean")
                print("[INFO] comb_img fallback: used mean patch tensor")

            if p_from_comb is not None:
                N = p_from_comb.shape[-1]
                G = int(math.sqrt(N)) if int(math.sqrt(N)) ** 2 == N else None
                if G is not None:
                    # if p_from_comb is constant zeros, consider fallback
                    if torch.allclose(p_from_comb, torch.zeros_like(p_from_comb)):
                        print("[INFO] comb_img was constant -> leaving out (fallback will be used)")
                    else:
                        attention_maps["comb_img"] = self.compute_attention_map(p_from_comb, grid_size=G)

            # token-level comb
            debug = False
            t_from_comb = self.compute_comb_token_vector(att_comb, txt_feats, attn_weights=attn_weights, debug=debug)

            if t_from_comb is not None:
                if debug:
                    print("[DBG] t_from_comb stats — min,max,mean,std:",
                        float(t_from_comb.min()), float(t_from_comb.max()),
                        float(t_from_comb.mean()), float(t_from_comb.std()))
                # if it's effectively all zeros skip, otherwise include it
                if torch.allclose(t_from_comb, torch.zeros_like(t_from_comb)):
                    print("[INFO] comb_txt was constant -> leaving out (fallback will be used)")
                else:
                    attention_maps["comb_txt"] = t_from_comb[0, 0].detach().cpu().numpy()

        # Combine to final maps (weighted)
        final_patch = None
        if p_from_txt is not None and p_from_comb is not None:
            device = p_from_txt.device
            p_from_comb = p_from_comb.to(device)

            # ensure same patch length before arithmetic (trim to shorter)
            Lp_txt = p_from_txt.size(-1)
            Lp_comb = p_from_comb.size(-1)
            if Lp_txt != Lp_comb:
                minL = min(Lp_txt, Lp_comb)
                p_from_txt = p_from_txt[..., :minL]
                p_from_comb = p_from_comb[..., :minL]
                print(f"[INFO] patch-length mismatch: trimmed to {minL} (was {Lp_txt},{Lp_comb})")
            final_patch = 0.6 * p_from_txt + 0.4 * p_from_comb
        elif p_from_txt is not None:
            final_patch = p_from_txt
        elif p_from_comb is not None:
            final_patch = p_from_comb

        # Combine token maps (already had code for tokens) — keep your trimming approach
        final_token = None
        if t_from_img is not None and t_from_comb is not None:
            device = t_from_img.device
            t_from_comb = t_from_comb.to(device)

            # ensure same token length before arithmetic (trim to shorter)
            L_img = t_from_img.size(-1)
            L_comb = t_from_comb.size(-1)
            if L_img != L_comb:
                minL = min(L_img, L_comb)
                t_from_img = t_from_img[..., :minL]
                t_from_comb = t_from_comb[..., :minL]
                print(f"[INFO] token-length mismatch: trimmed to {minL} (was {L_img},{L_comb})")
            final_token = 0.6 * t_from_img + 0.4 * t_from_comb
        elif t_from_img is not None:
            final_token = t_from_img
        elif t_from_comb is not None:
            final_token = t_from_comb

        def _norm_and_to_numpy(x):
            if x is None:
                return None
            y = x.clone().detach().cpu()
            mi = y.view(y.size(0), -1).min(dim=1)[0].view(-1, 1, 1)
            ma = y.view(y.size(0), -1).max(dim=1)[0].view(-1, 1, 1)
            y = (y - mi) / (ma - mi + 1e-8)
            return y

        final_patch = _norm_and_to_numpy(final_patch)
        final_token = _norm_and_to_numpy(final_token)

        if final_patch is not None:
            N = final_patch.shape[-1]
            G = int(math.sqrt(N)) if int(math.sqrt(N)) ** 2 == N else None
            if G is not None:
                attention_maps["final_patch_map"] = self.compute_attention_map(final_patch, grid_size=G)
            else:
                attention_maps["final_patch_map"] = None
        else:
            attention_maps["final_patch_map"] = None

        attention_maps["final_token_map"] = final_token[0, 0].cpu().numpy() if final_token is not None else None

        # IG maps (handle single or list of targets)
        if isinstance(targets, int):
            targets = [targets]

        ig_maps = {}
        for t in targets:
            ig_maps[t] = self.compute_ig_map_for_target(
                img_global[0:1].to(self.device),
                img_patches[0:1].to(self.device),
                txt_feats[0:1].to(self.device),
                t
            )

        # Grad-CAM maps
        gradcam_maps = {}
        for t in targets:
            gradcam_maps[t] = self.compute_gradcam_map_for_target(
                img_global[0:1],
                img_patches[0:1],
                txt_feats[0:1],
                t
            )

        return {
            "attention_map": attention_maps,
            "ig_maps": ig_maps,
            "gradcam_maps": gradcam_maps
        }
    
    def explain_image_only(self, img_global, img_patches, targets):
        if isinstance(targets, int):
            targets = [targets]

        gradcam_maps, ig_maps = {}, {}
        for t in targets:
            gradcam_maps[t] = self.compute_gradcam_map_for_target(
                img_global, img_patches, txt_feats=None, target_idx=t, unimodal="image"
            )
            ig_maps[t] = self.compute_ig_map_for_target(
                img_global, img_patches, txt_feats=None, target_idx=t, unimodal="image"
            )

        return {
            "attention_map": None,  # not relevant in unimodal
            "ig_maps": ig_maps,
            "gradcam_maps": gradcam_maps
        }

    def explain_text_only(self, txt_feats, targets):
        if isinstance(targets, int):
            targets = [targets]

        ig_maps, gradcam_maps = {}, {}
        for t in targets:
            ig_maps[t] = self.compute_ig_map_for_target(
                img_global=None, img_patches=None, txt_feats=txt_feats, 
                target_idx=t, unimodal="text"
            )
            gradcam_maps[t] = self.compute_gradcam_map_for_target(
                img_global=None, img_patches=None, txt_feats=txt_feats, 
                target_idx=t, unimodal="text"
            )

        return {
            "attention_map": None,   # not relevant in unimodal text
            "ig_maps": ig_maps,      # dict[target -> token IG scores]
            "gradcam_maps": gradcam_maps  # dict[target -> token Grad-CAM scores]
        }

end src\Model\explain.py

src\Model\fusion.py
import os
import torch
import torch.nn as nn
import timm
from Helpers import load_hf_model_or_local, download_swin
from safetensors.torch import load_file as load_safetensor
from pathlib import Path

try:
    BASE_DIR = Path(__file__).resolve().parent.parent.parent
except NameError:
    BASE_DIR = Path.cwd().parent.parent

MODEL_PLACE = BASE_DIR / 'models'
os.environ["TRANSFORMERS_CACHE"] = str(MODEL_PLACE)
os.environ["TORCH_HOME"] = str(MODEL_PLACE)

class PreFusionEnhancer(nn.Module):
    def __init__(self, dim, num_heads=4, dropout=0.1, max_len=512):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, batch_first=True, dropout=0.1)
        self.norm1 = nn.LayerNorm(dim)
        self.dropout = nn.Dropout(dropout)
        self.pos_embed = nn.Parameter(torch.zeros(1, max_len, dim))
        nn.init.trunc_normal_(self.pos_embed, std=0.02)
        self.alpha = nn.Parameter(torch.ones(1))

    def forward(self, x):  # x: (B, L, D)
        B, L, D = x.shape
        x = x + self.pos_embed[:, :L]  # Add trainable positional embedding
        x2, _ = self.self_attn(x, x, x) # Self attention for each token
        x = self.norm1(self.alpha * x + self.dropout(x2))
        return x

class Backbones(nn.Module):
    """
    Image and text backbones: Swin Transformer and ClinicalBERT.
    Outputs global image features and pooled CLS text embeddings.
    """
    def __init__(
            self, 
            img_backbone='swin',
            swin_model_name='swin_base_patch4_window7_224', 
            cnn_model_name='resnet50',
            bert_model_name='emilyalsentzer/Bio_ClinicalBERT',
            swin_checkpoint_path=None,
            bert_local_dir=None,
            pretrained=True,
            joint_dim=1024
            ):
        """
        Constructor for Backbones.

        Args:
            img_backbone (str): The image backbone to use. 
                One of 'swin' or 'cnn'. Defaults to 'swin'.
            swin_model_name (str): The Swin Transformer model name.
                Defaults to 'swin_base_patch4_window7_224'.
            cnn_model_name (str): The CNN model name.
                Defaults to 'resnet50'.
            bert_model_name (str): The ClinicalBERT model name.
                Defaults to 'emilyalsentzer/Bio_ClinicalBERT'.
            swin_checkpoint_path (str): The path to the Swin model safetensor checkpoint.
                If None, downloads from HuggingFace.
            bert_local_dir (str): The path to the local ClinicalBERT model directory.
                If None, downloads from HuggingFace.
            pretrained (bool): Whether to load the pre-trained weights.
                Defaults to True.
            joint_dim (int): The dimension of the joint embedding. 
            Defaults to 1024.
        """
        super().__init__()
        self.img_backbone = img_backbone

        if img_backbone == "swin":
            self.vision = timm.create_model(swin_model_name, pretrained=False, in_chans=1)
            if pretrained and swin_checkpoint_path:
                try:
                    state = load_safetensor(str(swin_checkpoint_path), device="cpu")
                    # collapse patch-embed weights
                    if "patch_embed.proj.weight" in state:
                        w3 = state["patch_embed.proj.weight"]
                        w1 = w3.mean(dim=1, keepdim=True)
                        state["patch_embed.proj.weight"] = w1
                    filtered = {k: v for k, v in state.items() if k in self.vision.state_dict()}
                    self.vision.load_state_dict(filtered, strict=False)
                except Exception as e:
                    print("[WARN] Failed to load Swin checkpoint:", e)
                    print("[INFO] Attempting to download pretrained Swin weights...")
                    download_swin(swin_name=swin_model_name, swin_ckpt_path=swin_checkpoint_path)
                    state = load_safetensor(str(swin_checkpoint_path), device="cpu")
                    filtered = {k: v for k, v in state.items() if k in self.vision.state_dict()}
                    self.vision.load_state_dict(filtered, strict=False)
            elif pretrained:
                self.vision = timm.create_model(swin_model_name, pretrained=True, in_chans=1)
            self.img_dim = self.vision.num_features

        elif img_backbone == "cnn":
            from torchvision import models
            if cnn_model_name == "resnet50":
                base = models.resnet50(pretrained=pretrained)
                self.vision = nn.Sequential(*list(base.children())[:-1])
                self.img_dim = base.fc.in_features
            elif cnn_model_name == "efficientnet_b0":
                base = models.efficientnet_b0(pretrained=pretrained)
                self.vision = nn.Sequential(*list(base.children())[:-1])
                self.img_dim = base.classifier[1].in_features
            else:
                raise ValueError(f"Unknown cnn_model_name {cnn_model_name}")

        else:
            raise ValueError(f"Unknown img_backbone {img_backbone}")
        self.swin = self.vision
        if hasattr(self.swin, "norm") and isinstance(getattr(self.swin, "norm"), nn.Module):
            self.swin_norm = self.swin.norm
        else:
            # self.img_dim should have been set already
            print("[Backbones] [WARN] Swin norm not found. Using LayerNorm.")
            self.swin_norm = nn.LayerNorm(self.img_dim)
        
        # ---- Text backbone ----
        self.bert = load_hf_model_or_local(bert_model_name, local_dir=bert_local_dir)
        self.txt_dim = self.bert.config.hidden_size

    def swin_features(self, x: torch.Tensor) -> torch.Tensor:
        """
        Extract patch-level Swin features and return (B, H, W, C).
        Robust to timm variants that may return different axis orders or a list of stage outputs.
        Prefers a stage whose channel dim matches self.vision.num_features when possible.
        """
        model = self.swin

        # get raw feats (may be tensor or list/tuple)
        if hasattr(model, "forward_features"):
            feats = model.forward_features(x)
        else:
            feats = model(x)

        # If list/tuple of stage outputs, try to pick stage with channel == vision.num_features
        if isinstance(feats, (list, tuple)):
            chosen = None
            for stage in reversed(feats):  # try deeper stages first
                if isinstance(stage, torch.Tensor) and stage.dim() == 4:
                    # try common layout (B, C, H, W) or other orders - check for a channel match
                    if stage.shape[1] == getattr(self.vision, "num_features", None):
                        chosen = stage
                        break
                    # also check other positions
                    if stage.shape[2] == getattr(self.vision, "num_features", None) or stage.shape[3] == getattr(self.vision, "num_features", None):
                        chosen = stage
                        break
            if chosen is None:
                chosen = feats[-1]
            feats = chosen

        # Now feats is a single tensor
        if not isinstance(feats, torch.Tensor):
            raise ValueError(f"Unexpected feats type: {type(feats)}")

        if feats.dim() != 4:
            # handle flattened (B, N, C)
            if feats.dim() == 3:
                B, N, C = feats.shape
                G = int(N ** 0.5)
                if G * G == N:
                    return feats.view(B, G, G, C).contiguous()
                pooled = feats.mean(dim=1).view(B, 1, 1, C)
                return pooled.contiguous()
            raise ValueError(f"Unexpected Swin feature shape: {tuple(feats.shape)}")

        # feats is 4D but channels might be on any axis: find the axis equal to expected channels
        B, d1, d2, d3 = feats.shape
        expected_C = getattr(self.vision, "num_features", None)

        # find the axis (1,2 or 3) that matches expected_C
        ch_axis = None
        for idx, s in enumerate((d1, d2, d3), start=1):
            if expected_C is not None and s == expected_C:
                ch_axis = idx
                break

        if ch_axis is None:
            # fallback: assume (B, C, H, W) as most timm models do and convert to (B, H, W, C)
            return feats.permute(0, 2, 3, 1).contiguous()

        # build permutation that moves batch first, then the two spatial dims (in original order), then the channel axis last.
        perm = [0] + [i for i in (1, 2, 3) if i != ch_axis] + [ch_axis]
        feats = feats.permute(*perm).contiguous()  # now (B, H, W, C)
        return feats

    def forward(self, image, input_ids=None, attention_mask=None):
        img_global, img_patches = None, None

        if image is not None:
            if self.img_backbone == "swin":
                # get patch-level features (B,H,W,C) without running the vision model twice
                patch_feats = self.swin_features(image)            # (B, H, W, C)
                B, H, W, C = patch_feats.shape
                patch_feats = patch_feats.view(B, H * W, C)       # (B, N_patches, C)
                img_patches = self.swin_norm(patch_feats)         # (B, N_patches, C)
                img_global = patch_feats.mean(dim=1)              # (B, C)

            elif self.img_backbone == "cnn":
                feats = self.vision(image)
                if isinstance(feats, (list, tuple)):
                    feats = feats[-1]

                # For torchvision CNNs, after chopping off fc, we usually get (B, C, H, W)
                if feats.dim() == 4:
                    B, C, H, W = feats.shape
                    img_global = feats.mean(dim=[2, 3])                   # (B, C)
                    img_patches = feats.flatten(2).transpose(1, 2)        # (B, H*W, C)
                elif feats.dim() == 2:
                    # Already pooled to (B, D)
                    img_global = feats
                    img_patches = feats.unsqueeze(1)                      # (B, 1, D)
                else:
                    raise ValueError(f"Unexpected CNN output shape: {feats.shape}")

        txt_feats = None
        if input_ids is not None:
            txt_feats = self.bert(input_ids=input_ids,
                                attention_mask=attention_mask).last_hidden_state

        return (img_global, img_patches), txt_feats

class CrossModalFusion(nn.Module):
    """
    Cross-modal fusion module.
    """
    def __init__(self, img_dim, txt_dim, joint_dim=256, num_heads=4, use_cls_only=False):
        """
        Constructor for CrossModalFusion.

        Args:
            img_dim (int): Dimensionality of the image features.
            txt_dim (int): Dimensionality of the text features.
            joint_dim (int, optional): Dimensionality of the joint embedding. Defaults to 256.
            num_heads (int, optional): Number of attention heads. Defaults to 4.
            use_cls_only (bool, optional): Whether to use only the CLS token. Defaults to False.
        """
        super().__init__()
        # —— Self-attention for text and image features ——
        self.txt_self_attn = PreFusionEnhancer(txt_dim, num_heads)
        self.img_patch_self_attn = PreFusionEnhancer(img_dim, num_heads)
        self.img_global_self_attn = PreFusionEnhancer(img_dim, num_heads)

        # —— Text features attend to image patches ——
        self.ln_img = nn.LayerNorm(joint_dim)
        self.ln_txt = nn.LayerNorm(joint_dim)

        # —— Text queries attend to image patches ——
        self.query_txt    = nn.Linear(txt_dim, joint_dim)
        self.key_img      = nn.Linear(img_dim, joint_dim)
        self.value_img    = nn.Linear(img_dim, joint_dim)
        self.attn_txt2img = nn.MultiheadAttention(joint_dim, num_heads, batch_first=True)

        # —— Image patches attend to text ——
        self.query_img     = nn.Linear(img_dim, joint_dim)
        self.key_txt       = nn.Linear(txt_dim, joint_dim)
        self.value_txt     = nn.Linear(txt_dim, joint_dim)
        self.attn_img2txt  = nn.MultiheadAttention(joint_dim, num_heads, batch_first=True)

        # Making sure text and image features have the same dimension
        self.txt_proj = nn.Linear(txt_dim, joint_dim)
        self.img_patch_proj = nn.Linear(img_dim, joint_dim)
        self.img_global_proj = nn.Linear(img_dim, joint_dim)

        # a learnable default pooled text token used when no text is provided
        self.default_txt_token = nn.Parameter(torch.zeros(1, 1, txt_dim))
        nn.init.trunc_normal_(self.default_txt_token, std=0.02)

        self.use_cls_only = use_cls_only
        self.comb_mlp = nn.Sequential(
            nn.Linear(joint_dim * 3, joint_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(joint_dim, joint_dim)
        )

    def forward(self, img_global, img_patch, txt_feats, return_attention=False):
        """
        Compute the cross-modal fusion of the input image and text features.

        Parameters:
            img_global (torch.Tensor): Global image features of shape (B, joint_dim).
            img_patch (torch.Tensor): Image patches of shape (B, N, img_dim).
            txt_feats (torch.Tensor): Text features of shape (B, txt_dim).
            return_attention (bool): Whether to return the attention weights.

        Returns:
            torch.Tensor: The fused features of shape (B, joint_dim).
            dict: A dictionary containing the attention weights if return_attention is True.
        """
        if txt_feats is None:
            # repeat the learnable default token for the batch
            B = img_global.shape[0] if img_global is not None else img_patch.shape[0]
            txt_feats = self.default_txt_token.expand(B, -1, -1).to(img_patch.device if img_patch is not None else (img_global.device if img_global is not None else 'cpu'))
        
        # img_global: (B, joint_dim)
        # img_patch:  (B, N_patches, img_dim)
        # txt_feats:  (B, txt_dim)
        txt_feats = self.txt_self_attn(txt_feats)
        img_global = self.img_global_self_attn(img_global.unsqueeze(1)).squeeze(1) # (B,D)
        img_patch = self.img_patch_self_attn(img_patch)           # (B, N, D)
        
        # Handle CLS token usage
        if self.use_cls_only:
            txt_feats_pooled = txt_feats[:, 0].unsqueeze(1)  # (B, 1, D)
        else:
            txt_feats_pooled = txt_feats                     # (B, L, D)
        B, Np, D = img_patch.shape

        # Text attends to image patches
        Q_txt = self.query_txt(txt_feats_pooled)                   # (B, L or 1, D)
        K_img = self.key_img(img_patch)                            # (B, N, D)
        V_img = self.value_img(img_patch)                          # (B, N, D)
        att_txt2img, attn_weights_txt2img = self.attn_txt2img(Q_txt, K_img, V_img)

        # Image patches attend to text
        Q_img = self.query_img(img_patch)                   # (B, N, D)
        K_txt = self.key_txt(txt_feats_pooled)              # (B, L or 1, D)
        V_txt = self.value_txt(txt_feats_pooled)            # (B, L or 1, D)
        att_img2txt, attn_weights_img2txt = self.attn_img2txt(Q_img, K_txt, V_txt)

        # Fuse image patches with text
        img_patch_proj = self.img_patch_proj(img_patch)     # (B, N, joint_dim)
        patches_fused = img_patch_proj + att_img2txt        # (B, N, joint_dim)

        # Fuse global image features with text
        img_global_proj = self.img_global_proj(img_global)                              # (B, joint_dim)
        att_txt2img_pooled = att_txt2img.mean(dim=1)
        img_global_updated = img_global_proj + att_txt2img_pooled
        x1 = self.ln_img(img_global_updated)                    
        
        # Pool text features
        txt_p = self.txt_proj(txt_feats)
        txt_cls = txt_p[:, 0]
        att_img2txt_pooled = att_img2txt.mean(dim=1)
        x2 = self.ln_txt(txt_cls + att_img2txt_pooled)

        patch_toks = patches_fused                                       # (B, N, joint_dim)
        cls_tok = x1.unsqueeze(1)                                        # (B, 1, joint_dim)
        txt_tok = x2.unsqueeze(1)                                        # (B, 1, joint_dim)

        attn_dict = {'txt2img': attn_weights_txt2img, 'img2txt': attn_weights_img2txt}
        
        if self.use_cls_only:
            # compute fused vector via MLP combiner
            patch_avg = patches_fused.mean(dim=1)  # (B, D)
            cat = torch.cat([x1, patch_avg, x2], dim=1)  # (B, 3*D)
            fused_vec = self.comb_mlp(cat)  # (B, D)

            attn_dict['patch_avg'] = patch_avg.detach() 
            if return_attention:
                return fused_vec, attn_dict
            return fused_vec, None

        seq = torch.cat([cls_tok, patch_toks, txt_tok], dim=1)  # (B, 1+Np+1, joint_dim)
        if return_attention:
            return seq, attn_dict
        return seq, None
end src\Model\fusion.py

src\Model\model.py
import torch
import torch.nn as nn
from .fusion import CrossModalFusion, Backbones
from Retrieval import  RetrievalEngine, make_retrieval_engine
from pathlib import Path
from .explain import ExplanationEngine
import numpy as np
import json

BASE_DIR = Path(__file__).resolve().parent.parent.parent
EMBEDDINGS_DIR = BASE_DIR / "embeddings"

class MultiHeadMLP(nn.Module):
    def __init__(self, dim, num_heads):
        super().__init__()
        assert dim % num_heads == 0
        self.head_dim = dim // num_heads
        self.num_heads = num_heads
        self.linear1 = nn.Linear(dim, dim * 2)
        self.act = nn.GELU()
        self.linear2 = nn.Linear(dim * 2, dim)

    def forward(self, x):
        x = self.linear1(x)
        x = self.act(x)
        x = self.linear2(x)
        return x

class StochasticDepth(nn.Module):
    def __init__(self, drop_prob):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x, residual):
        if not self.training or self.drop_prob == 0.:
            return x + residual
        keep_prob = 1 - self.drop_prob
        shape = [x.shape[0]] + [1] * (x.ndim - 1)  # (B, 1, 1...)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        binary_mask = torch.floor(random_tensor)
        return x + residual * binary_mask / keep_prob

class PositionalEncoding(nn.Module):
    def __init__(self, dim, max_len=512):
        super().__init__()
        self.dim = dim
        self.max_len = max_len
        self.pe = nn.Parameter(torch.zeros(1, max_len, dim))
        nn.init.normal_(self.pe, std=0.02)

    def forward(self, x):
        L = x.size(1)
        if L > self.max_len:
            # extend PE on the fly
            extra = torch.zeros(1, L - self.max_len, self.dim, device=x.device)
            nn.init.normal_(extra, std=0.02)
            pe = torch.cat([self.pe, extra], dim=1)
            return x + pe[:, :L]
        return x + self.pe[:, :L]

class MultiModalRetrievalModel(nn.Module):
    """
    Wraps Backbones + Fusion + Classification head
    into one model that produces:
      - joint_emb (B, joint_dim)
      - logits    (B, num_classes)
    """
    def __init__(
        self,
        joint_dim:    int = 256,
        num_heads:    int = 4,
        num_classes:  int = 22,
        num_fusion_layers: int = 3,
        fusion_type:  str = "cross",
        img_backbone: str = "swin",
        swin_name:    str = "swin_base_patch4_window7_224",
        cnn_name: str = "resnet50",
        bert_name:    str = "emilyalsentzer/Bio_ClinicalBERT",
        swin_ckpt_path:    str = None,
        bert_local_dir:   str = None,
        pretrained:   bool = True,
        checkpoint_path:str = None,
        device:         torch.device = torch.device("cpu"),
        training:      bool = False,
        use_shared_ffn: bool = True,
        use_cls_only: bool = False,
        model_type: str = "multimodal",
        retriever: RetrievalEngine = None
    ):
        """
        :param joint_dim: dimensionality of the joint embedding
        :param num_heads: number of attention heads for CrossModalFusion
        :param num_classes: number of output classes
        :param num_fusion_layers: number of fusion layers to use
        :param use_shared_ffn: whether to use a shared FFN across fusion layers
        :param fusion_type: type of fusion module to use; one of "cross", "simple", "gated"
        :param img_backbone: type of image backbone to use; one of "swin", "cnn"
        :param swin_name: name of the Swin transformer model to use
        :param cnn_name: name of the CNN model to use
        :param bert_name: name of the ClinicalBERT model to use
        :param swin_ckpt_path: path to a Swin transformer checkpoint to load
        :param bert_local_dir: directory containing a ClinicalBERT model to load
        :param pretrained: whether to load pre-trained weights for the Swin and ClinicalBERT models
        :param checkpoint_path: path to a model checkpoint to load
        :param device: device to run the model on
        :param training: whether the model is being trained or used for inference
        :param use_cls_only: whether to use only the CLS token for BERT
        :param model_type: type of model to use; one of "multimodal", "image", "text"
        :param retriever: optional retrieval engine for case-based retrieval

        Args:
            joint_dim (int, optional): dimensionality of the joint embedding. Defaults to 256.
            num_heads (int, optional): number of attention heads for CrossModalFusion. Defaults to 4.
            num_classes (int, optional): number of output classes. Defaults to 22.
            use_shared_ffn (bool, optional): whether to use a shared FFN across fusion layers. Defaults to True.
            num_fusion_layers (int, optional): number of fusion layers to use. Defaults to 3.
            fusion_type (str, optional): type of fusion module to use; one of "cross", "simple", "gated". Defaults to "cross".
            img_backbone (str, optional): type of image backbone to use; one of "swin", "cnn". Defaults to "swin".
            swin_name (str, optional): name of the Swin transformer model to use. Defaults to "swin_base_patch4_window7_224".
            cnn_name (str, optional): name of the CNN model to use. Defaults to "resnet50".
            bert_name (str, optional): name of the ClinicalBERT model to use. Defaults to "emilyalsentzer/Bio_ClinicalBERT".
            swin_ckpt_path (str, optional): path to a Swin transformer checkpoint to load. Defaults to None.
            bert_local_dir (str, optional): directory containing a ClinicalBERT model to load. Defaults to None.
            pretrained (bool, optional): whether to load pre-trained weights for the Swin and ClinicalBERT models. Defaults to True.
            checkpoint_path (str, optional): path to a model checkpoint to load. Defaults to None.
            device (torch.device, optional): device to run the model on. Defaults to torch.device("cpu").
            training (bool, optional): whether the model is being trained or used for inference. Defaults to False.
            use_cls_only (bool, optional): whether to use only the CLS token for BERT. Defaults to False.
            model_type (str, optional): type of model to use; one of "multimodal", "image", "text". Defaults to "multimodal".
            retriever (RetrievalEngine, optional): optional retrieval engine for case-based retrieval. Defaults to None.
        """
        super().__init__()
        self.device = device
        self.use_shared_ffn = use_shared_ffn
        # instantiate vision+text backbones
        self.backbones = Backbones(
            img_backbone       = img_backbone,
            swin_model_name    = swin_name,
            cnn_model_name     = cnn_name,
            bert_model_name    = bert_name,
            swin_checkpoint_path = swin_ckpt_path,
            bert_local_dir       = bert_local_dir,
            pretrained           = pretrained
        ).to(device)
        img_dim = self.backbones.img_dim
        txt_dim = self.backbones.txt_dim
        
        # set up model type
        if model_type in ["multimodal", "image", "text"]:
            self.model_type = model_type
        else:
            raise ValueError(f"Unknown model_type {model_type!r}")

        # set up fusion
        if fusion_type == "cross":
            self.fusion_layers = nn.ModuleList([
                CrossModalFusion(img_dim, txt_dim, joint_dim, num_heads, use_cls_only).to(device)
                for _ in range(num_fusion_layers)
            ])
        else:
            raise ValueError(f"Unknown fusion_type {fusion_type!r}")
        
        self.self_attn = nn.MultiheadAttention(embed_dim=joint_dim, num_heads=num_heads, batch_first=True)

        # LayerNorm and FFN for residual connections and normalization
        self.norm1_layers = nn.ModuleList([
            nn.LayerNorm(joint_dim, eps=1e-5) for _ in range(num_fusion_layers)
        ])
        self.norm2_layers = nn.ModuleList([
            nn.LayerNorm(joint_dim, eps=1e-5) for _ in range(num_fusion_layers)
        ])

        self.alpha = nn.Parameter(torch.ones(1)) # learnable scaling factor for residual connections
        self.dropout = nn.Dropout(0.1)
        self.pos_encoder = PositionalEncoding(joint_dim, txt_dim)

        # Feed-forward network for each fusion layer
        if self.use_shared_ffn:
            self.shared_ffn = MultiHeadMLP(joint_dim, num_heads)
            self.ffn = None
        else:
            self.ffn = nn.ModuleList([
                MultiHeadMLP(joint_dim, num_heads)
                for _ in range(num_fusion_layers)
            ])
            self.shared_ffn = None

        # Stochastic depth for residual connections
        self.drop_path_layers = nn.ModuleList([
            StochasticDepth(0.1) for _ in range(num_fusion_layers)
        ])

        # Projection for image and text embeddings only case
        self.img_proj = nn.Linear(img_dim, joint_dim).to(device)
        self.txt_proj = nn.Linear(txt_dim, joint_dim).to(device)

        # adapters for each fusion layer
        self.adapters = nn.ModuleList([
            nn.Sequential(
                nn.Linear(joint_dim, joint_dim // 2),
                nn.GELU(),
                nn.Linear(joint_dim // 2, joint_dim)
            ) for _ in range(num_fusion_layers)
        ])

        # classification head on the joint embedding
        self.classifier = nn.Sequential(
            nn.Linear(joint_dim, joint_dim * 4),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(joint_dim * 4, num_classes),
            nn.Dropout(0.1)
        ).to(device)

        self.use_cls_only = use_cls_only

        #  Retrieval engine (offline index of embeddings)
        if not training:
            if checkpoint_path:
                state = torch.load(checkpoint_path, map_location=device)
                self.load_state_dict(state)
                self.to(device)
                self.eval()
            else:
                raise ValueError("checkpoint_path must be provided for inference")
            
            if retriever:
                self.retriever = retriever
            else:
                features_path = EMBEDDINGS_DIR / "val_joint_embeddings.npy"
                ids_path      = EMBEDDINGS_DIR / "val_ids.json"
                if not features_path.exists() or not ids_path.exists():
                    raise FileNotFoundError(
                        f"Expected embeddings at {features_path} and IDs at {ids_path}"
                    )
                import warnings
                warnings.warn("No retrieval engine provided. Using default DLS with val embeddings.")

                self.retriever = make_retrieval_engine(
                    features_path=str(features_path),
                    ids_path=str(ids_path),
                    method="dls",
                    link_threshold=0.5,
                    max_links=10
                )

            # set up explanation 
            self.explainer   = None
            self.ig_steps    = 100
            self.image_size  = (224,224)

        else:
            # during training, we can use a dummy path
            features_path = EMBEDDINGS_DIR / "dummy_embeddings.npy"
            ids_path      = EMBEDDINGS_DIR / "dummy_ids.json"
            if not features_path.exists() or not ids_path.exists():
                np.save(features_path, np.zeros((1, joint_dim), dtype=np.float32))
                with open(ids_path, "w") as f:
                    json.dump(["dummy_id"], f)
            
            self.retriever = None

    def set_retriever(self, retriever: RetrievalEngine):
        self.retriever = retriever

    def forward(self, image, input_ids, attention_mask, return_attention=False):
        """
        Inputs:
        image (torch.Tensor): (B, C, H, W)
        input_ids (torch.Tensor): (B, L)
        attention_mask (torch.Tensor): (B, L)

        Returns:
        dict with keys:
            - "joint_emb": (B, joint_dim)
            - "img_emb":   (B, joint_dim) or None
            - "txt_emb":   (B, joint_dim) or None
            - "logits":    (B, num_classes)
            - "attn":      dict of attention maps (if return_attention=True), else None
        """
        attn_weights = {}
        joint_emb = None

        # --- Backbones ---
        (img_global, img_patches), txt_feats = self.backbones(
            image.to(self.device) if image is not None else None,
            input_ids.to(self.device) if input_ids is not None else None,
            attention_mask.to(self.device) if attention_mask is not None else None
        )

        img_emb = self.img_proj(img_global) if img_global is not None else None
        if txt_feats is not None:
            if self.use_cls_only:
                txt_emb = txt_feats[:, 0, :]
            else:
                txt_emb = txt_feats.mean(dim=1)
            txt_emb = self.txt_proj(txt_emb)
        else:
            txt_emb = None
        
        if self.model_type == "multimodal":
            for i, fusion in enumerate(self.fusion_layers):
                # fusion may return either pooled (B, D) or sequence (B, L, D)
                fused_out, attn_from_fusion = fusion(
                    img_global,
                    img_patches,
                    txt_feats,
                    return_attention=return_attention
                )

                # Handling fuse output 
                if fused_out is None:
                    raise RuntimeError("Fusion returned None fused_out")
                if fused_out.dim() == 3:
                    seq = fused_out  # (B, L, D)
                elif fused_out.dim() == 2:
                    seq = fused_out.unsqueeze(1)  # (B, 1, D)
                else:
                    raise RuntimeError(f"Unexpected fused_out shape: {fused_out.shape}")

                # dropout + positional encoding (seq: B,L,D)
                seq = self.dropout(seq)
                seq = self.pos_encoder(seq)

                seq_out, comb_attn_weights = self.self_attn(seq, seq, seq)

                # store comb & fusion attn weights (detach -> cpu to avoid keeping graph)
                if return_attention:
                    try:
                        attn_weights[f"layer_{i}_comb"] = comb_attn_weights.detach().cpu()
                    except Exception:
                        # comb_attn_weights may be None or not a tensor (guard)
                        attn_weights[f"layer_{i}_comb"] = comb_attn_weights

                    # fusion produced cross-attention dict (txt2img/img2txt) — store if present
                    if attn_from_fusion:
                        if "txt2img" in attn_from_fusion and attn_from_fusion["txt2img"] is not None:
                            try:
                                attn_weights[f"layer_{i}_txt2img"] = attn_from_fusion["txt2img"].detach().cpu()
                            except Exception:
                                attn_weights[f"layer_{i}_txt2img"] = attn_from_fusion["txt2img"]
                        if "img2txt" in attn_from_fusion and attn_from_fusion["img2txt"] is not None:
                            try:
                                attn_weights[f"layer_{i}_img2txt"] = attn_from_fusion["img2txt"].detach().cpu()
                            except Exception:
                                attn_weights[f"layer_{i}_img2txt"] = attn_from_fusion["img2txt"]
                        # optional extras (patch_avg etc.)
                        if "patch_avg" in attn_from_fusion:
                            try:
                                attn_weights[f"layer_{i}_patch_avg"] = attn_from_fusion["patch_avg"].detach().cpu()
                            except Exception:
                                attn_weights[f"layer_{i}_patch_avg"] = attn_from_fusion["patch_avg"]

                if self.use_cls_only:
                    fused = fused_out[:, 0, :]
                else:
                    fused = seq_out.mean(dim=1)

                # Residual
                if i == 0:
                    x = fused
                else:
                    x = self.norm1_layers[i](joint_emb)
                    x = self.drop_path_layers[i](x, self.alpha * fused)

                # FFN + Adapter
                x_ffn = self.norm2_layers[i](x)
                if self.use_shared_ffn:
                    x = x + self.shared_ffn(x_ffn)
                else:
                    x = x + self.ffn[i](x_ffn)
                x = x + self.adapters[i](x)

                # Update joint_emb
                joint_emb = x

            # After loop, ensure joint_emb is (B, D)
            if joint_emb is None:
                raise RuntimeError("joint_emb is still None after forward")
            if joint_emb.dim() == 3:
                # if singleton seq dim (B,1,D) -> squeeze; otherwise mean-pool tokens
                if joint_emb.size(1) == 1:
                    joint_emb = joint_emb.squeeze(1)
                else:
                    joint_emb = joint_emb.mean(dim=1)

        # --- Image-only ---
        elif self.model_type == "image":
            x = img_global  # (B, D_img)
            x = self.img_proj(x)  # nn.Linear(img_dim, joint_dim)
            x = self.shared_ffn(x) if self.use_shared_ffn else self.ffn[0](x)
            joint_emb = x

        # --- Text-only ---
        elif self.model_type == "text":
            if self.use_cls_only:
                x = txt_feats[:, 0, :]  # CLS
            else:
                x = txt_feats.mean(dim=1)  # mean pool
            x = self.txt_proj(x)  # nn.Linear(txt_dim, joint_dim)
            x = self.shared_ffn(x) if self.use_shared_ffn else self.ffn[0](x)
            joint_emb = x
        
        logits = self.classifier(joint_emb)

        return {
            "joint_emb": joint_emb,
            "img_emb": img_emb,
            "txt_emb": txt_emb,
            "logits": logits,
            "attn": (attn_weights if return_attention else None)
        }

    def predict(self,
                image: torch.Tensor,
                input_ids: torch.Tensor,
                attention_mask: torch.Tensor,
                threshold=0.5,
                K: int = 5,
                explain: bool = False
                ) -> dict:
        
        joint_emb, logits, attn_weights = self.forward(
            image, input_ids, attention_mask, return_attention=explain
        )

        # probabilities 
        probs = torch.sigmoid(logits)               # (B, num_classes)
        preds = (probs >= threshold).int()
        topk_vals, topk_idx = probs.topk(K, dim=-1)     # (B, K)

        q_emb = joint_emb.detach().cpu().numpy()          # (B, D)

        # ---- Assemble output ----
        output = {
            'probs':            probs.detach().cpu().numpy(),
            'preds':            preds.detach().cpu().numpy(),
            'topk_idx':         topk_idx.detach().cpu().tolist(),
            'topk_vals':        topk_vals.detach().cpu().tolist(),
        }

        if explain:
            targets = topk_idx[0].tolist()

            if self.model_type == "multimodal":
                expl = self.explain(
                    image, input_ids, attention_mask,
                    joint_emb, attn_weights,
                    targets=targets,
                    K=K
                )
            elif self.model_type == "image":
                # no attention/text branch
                (img_global, img_patches), _ = self.backbones(
                    image.to(self.device),
                    None, None
                )
                expl = self.explain_image_only(img_global, img_patches, targets)
                # add retrieval to keep schema consistent
                retr_ids, retr_dists = self.retriever.retrieve(q_emb, K=K)
                expl.update({
                    "retrieval_ids": retr_ids,
                    "retrieval_dists": retr_dists,
                    "attention_map": None,   # not applicable
                })
            elif self.model_type == "text":
                _, txt_feats = self.backbones(
                    None,
                    input_ids.to(self.device),
                    attention_mask.to(self.device)
                )
                expl = self.explain_text_only(txt_feats, targets)
                retr_ids, retr_dists = self.retriever.retrieve(q_emb, K=K)
                expl.update({
                    "retrieval_ids": retr_ids,
                    "retrieval_dists": retr_dists,
                    "attention_map": None,   # not applicable
                    "gradcam_maps": None,    # not applicable
                })
            else:
                raise ValueError(f"Unsupported model_type: {self.model_type}")

            output.update(expl)

        return output

    def explain(
        self,
        image: torch.Tensor,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        joint_emb: torch.Tensor,
        attn_weights: dict,
        targets: list,
        K: int = 5
    ) -> dict:
        # Case-based retrieval again as part of explanation
        q_emb = joint_emb.detach().cpu().numpy()
        retr_ids, retr_dists = self.retriever.retrieve(q_emb, K=K)

        # Lazy-init explainer
        if self.explainer is None:
            self.explainer = ExplanationEngine(
                fusion_model=self.fusion_layers[-1],
                classifier_head=self.classifier,
                image_size=self.image_size,
                ig_steps=self.ig_steps,
                device=self.device
            )

        # Extract features for heatmap methods
        (img_global, img_patches), txt_feats = self.backbones(
            image.to(self.device),
            input_ids.to(self.device),
            attention_mask.to(self.device)
        )

        # Extract attention maps
        last_idx = len(self.fusion_layers) - 1
        attn_weights_expl = {
            "txt2img": attn_weights.get(f"layer_{last_idx}_txt2img"),
            "img2txt": attn_weights.get(f"layer_{last_idx}_img2txt"),
            "comb":    attn_weights.get(f"layer_{last_idx}_comb", None)
        }

        # Get attention maps explained
        maps = self.explainer.explain(
            img_global=img_global,
            img_patches=img_patches,
            txt_feats=txt_feats,
            attn_weights=attn_weights_expl,
            targets=targets
        )

        return {
            'retrieval_ids':   retr_ids,
            'retrieval_dists': retr_dists,
            'attention_map':   maps['attention_map'],
            'ig_maps':         maps['ig_maps'],
            'gradcam_maps':      maps['gradcam_maps']
        }
    
    def get_explain_score(
        self,
        image: torch.Tensor,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        targets: list,
    ) -> dict:
        """
        Computes explanation maps for a single input using three methods.

        Args:
            image: torch.Tensor of shape (B, C, H, W)
            input_ids: torch.Tensor of shape (B, T)
            attention_mask: torch.Tensor of shape (B, T)
            targets: single int or list of ints for the class indices to explain

        Returns:
            dict with three keys:
                - 'attention_map': (H, W) attention map over image patches
                - 'ig_maps':       dict of target to (H, W) IG map over image patches
                - 'gradcam_maps':  dict of target to (H, W) Grad-CAM map over image patches
        """
        # Lazy-init explainer
        if self.explainer is None:
            self.explainer = ExplanationEngine(
                fusion_model=self.fusion_layers[-1],
                classifier_head=self.classifier,
                image_size=self.image_size,
                ig_steps=self.ig_steps,
                device=self.device
            )

        _, _, attn_weights = self.forward(
            image, input_ids, attention_mask, return_attention=True
        )

        # Extract features for heatmap methods
        (img_global, img_patches), txt_feats = self.backbones(
            image.to(self.device),
            input_ids.to(self.device),
            attention_mask.to(self.device)
        )

        # Extract attention maps
        last_idx = len(self.fusion_layers) - 1
        attn_weights_expl = {
            "txt2img": attn_weights.get(f"layer_{last_idx}_txt2img"),
            "img2txt": attn_weights.get(f"layer_{last_idx}_img2txt"),
            "comb":    attn_weights.get(f"layer_{last_idx}_comb", None)
        }

        maps = self.explainer.explain(
            img_global=img_global,
            img_patches=img_patches,
            txt_feats=txt_feats,
            attn_weights=attn_weights_expl,
            targets=targets
        )

        return {
            'attention_map':   maps['attention_map'],
            'ig_maps':         maps['ig_maps'],
            'gradcam_maps':      maps['gradcam_maps']
        }
    
    def explain_image_only(self, img_global, img_patches, targets, K=5):
        if self.explainer is None:
            self.explainer = ExplanationEngine(
                fusion_model=None,            # no fusion in image-only
                classifier_head=self.classifier,
                image_size=self.image_size,
                ig_steps=self.ig_steps,
                device=self.device
            )

        # Get explanation maps for image branch only
        maps = self.explainer.explain_image_only(
            img_global=img_global,
            img_patches=img_patches,
            targets=targets
        )

        return {
            "attention_map": None,        # not applicable in single modality
            "ig_maps": maps["ig_maps"],   # from ExplanationEngine
            "gradcam_maps": maps["gradcam_maps"]
        }

    def explain_text_only(self, txt_feats, targets, K=5):
        if self.explainer is None:
            self.explainer = ExplanationEngine(
                fusion_model=None,             # no fusion in text-only
                classifier_head=self.classifier,
                image_size=self.image_size,
                ig_steps=self.ig_steps,
                device=self.device
            )

        # Get explanation maps for text branch only
        maps = self.explainer.explain_text_only(
            txt_feats=txt_feats,
            targets=targets
        )

        return {
            "attention_map": None,        # not applicable
            "ig_maps": maps["ig_maps"],   # token-level attribution
            "gradcam_maps": maps["gradcam_maps"] # token-level Grad-CAM
        }

end src\Model\model.py

src\Model\__init__.py
from .model import MultiModalRetrievalModel
from .fusion import CrossModalFusion, Backbones
from .explain import ExplanationEngine

__all__ = [
    "MultiModalRetrievalModel",
    "CrossModalFusion",
    "Backbones", 
    "ExplanationEngine"
]
end src\Model\__init__.py

src\Retrieval\retrieval.py
import abc
import numpy as np
import json
from typing import Tuple, List, Optional, Dict, Any, Set
from sklearn.metrics.pairwise import cosine_similarity
import heapq
from pathlib import Path
import pickle
import pandas as pd

# Base dirs
BASE_DIR        = Path(__file__).resolve().parent.parent.parent
EMBEDDINGS_DIR  = BASE_DIR / "embeddings"
FEATURES_PATH   = BASE_DIR / "featureDBs"
KG_DIR_DEFAULT  = BASE_DIR / "knowledge_graph"
LABELS_CSV_DEF  = BASE_DIR / "outputs" / "openi_labels_final.csv"

# ---------------------------
# Reranker class (KG + label + embedding)
# ---------------------------
class Reranker:
    """
    Lightweight reranker that combines:
      - embedding cosine (from joint embeddings),
      - label Jaccard (from outputs/openi_labels_final.csv),
      - KG-based cosine (from knowledge_graph node embeddings).
    Usage:
      rer = Reranker(kg_dir=..., labels_csv=...)
      rer.rerank(query_id, candidate_ids, candidate_embs)
    """

    def __init__(
        self,
        kg_dir: Optional[Path] = None,
        labels_csv: Optional[Path] = None,
        alpha: float = 0.6,
        beta: float = 0.25,
        gamma: float = 0.15,
        preload_record_kg: bool = True,
    ):
        self.kg_dir = Path(kg_dir) if kg_dir else KG_DIR_DEFAULT
        self.labels_csv = Path(labels_csv) if labels_csv else LABELS_CSV_DEF
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma

        # load KG artifacts
        self.kg = self._load_kg(self.kg_dir)
        # load labels
        self.labels_df = pd.read_csv(self.labels_csv, index_col="id")
        self.labels_df.index = self.labels_df.index.astype(str)

        # optional precompute record_kg_vectors if matching embeddings id list exists
        self.record_kg_vectors = None
        self.record_kg_id2idx = None
        if preload_record_kg:
            try:
                self._try_precompute_record_kg()
            except Exception:
                # non-fatal; we can compute on the fly per record
                self.record_kg_vectors = None
                self.record_kg_id2idx = None

    def _load_kg(self, kg_dir: Path) -> Dict[str, Any]:
        node2id_path = kg_dir / "node2id.json"
        if not node2id_path.exists():
            raise FileNotFoundError(f"KG node2id.json not found at {node2id_path}")
        node2id = json.load(open(node2id_path, "r", encoding="utf8"))

        # prefer best checkpoint
        best_files = sorted(kg_dir.glob("node_embeddings_best.npy"))
        if best_files:
            node_emb_file = best_files[-1]
        else:
            # fall back to epoch checkpoints
            node_emb_files = sorted(kg_dir.glob("node_embeddings_epoch*.npy"))
            if not node_emb_files:
                node_emb_files = sorted(kg_dir.glob("node_embeddings*.npy"))
            if not node_emb_files:
                raise FileNotFoundError("No .npy embeddings found in KG dir")
            node_emb_file = node_emb_files[-1]

        node_emb = np.load(node_emb_file)
        node_emb = node_emb / (np.linalg.norm(node_emb, axis=1, keepdims=True) + 1e-12)

        # build id->node list
        max_idx = max(node2id.values())
        id2node = [None] * (max_idx + 1)
        for k, v in node2id.items():
            id2node[v] = k

        print(f"[Reranker] loaded KG embeddings from {node_emb_file}")
        return {"node2id": node2id, "id2node": id2node, "node_emb": node_emb}

    # -------------------------
    # helpers
    # -------------------------
    @staticmethod
    def safe_cos(a: np.ndarray, b: np.ndarray) -> float:
        if a is None or b is None:
            return 0.0
        na = np.linalg.norm(a)
        nb = np.linalg.norm(b)
        if na == 0 or nb == 0:
            return 0.0
        return float(np.dot(a, b) / (na * nb))

    @staticmethod
    def jaccard_sets(a: Set[str], b: Set[str]) -> float:
        if not a and not b:
            return 0.0
        inter = len(a & b); uni = len(a | b)
        return 0.0 if uni == 0 else inter / uni

    @staticmethod
    def minmax_scale_list(x_list: List[float]) -> List[float]:
        arr = np.array(x_list, dtype=float)
        if arr.size == 0:
            return arr.tolist()
        lo = float(np.nanmin(arr)); hi = float(np.nanmax(arr))
        if hi - lo == 0:
            return [0.0] * len(arr)
        return ((arr - lo) / (hi - lo)).tolist()

    def get_record_label_set(self, rec_id: str) -> Set[str]:
        if str(rec_id) not in self.labels_df.index:
            return set()
        row = self.labels_df.loc[str(rec_id)]
        return set([c for c, v in row.items() if int(v) == 1])

    def get_record_kg_vec(self, rec_id: str) -> np.ndarray:
        """
        Try in order:
          1) node key "report:<id>"
          2) raw id as key
          3) average label node embeddings (fallback)
        """
        node2id = self.kg["node2id"]; node_emb = self.kg["node_emb"]
        k1 = f"report:{rec_id}"
        if k1 in node2id:
            return node_emb[node2id[k1]]
        if str(rec_id) in node2id:
            return node_emb[node2id[str(rec_id)]]

        # fallback: average label node embeddings
        labels = self.get_record_label_set(rec_id)
        if not labels:
            return np.zeros(node_emb.shape[1], dtype=float)
        vecs = []
        for lab in labels:
            cands = [f"label:{lab}", lab, lab.lower(), lab.replace(" ", "_")]
            for ck in cands:
                if ck in node2id:
                    vecs.append(node_emb[node2id[ck]])
                    break
        if not vecs:
            return np.zeros(node_emb.shape[1], dtype=float)
        return np.mean(np.stack(vecs, axis=0), axis=0)

    def _try_precompute_record_kg(self):
        """
        If a typical 'trainval_ids.json' exists, precompute record-level KG vectors to speed rerank.
        """
        ids_file = EMBEDDINGS_DIR / "trainval_ids.json"
        if not ids_file.exists():
            return
        with open(ids_file, "r") as f:
            ids = json.load(f)
        rec_vecs = np.zeros((len(ids), self.kg["node_emb"].shape[1]), dtype=float)
        for i, rid in enumerate(ids):
            rec_vecs[i] = self.get_record_kg_vec(str(rid))
        # save to KG dir for reuse
        out_np = self.kg_dir / "record_kg_vectors.npy"
        np.save(out_np, rec_vecs)
        self.record_kg_vectors = rec_vecs
        self.record_kg_id2idx = {str(rid): i for i, rid in enumerate(ids)}

    def rerank(
        self,
        query_id: str,
        candidate_ids: List[str],
        candidate_embs: Optional[np.ndarray] = None,
        candidate_emb_lookup: Optional[Dict[str, np.ndarray]] = None,
        topk: Optional[int] = None,
        query_emb: Optional[np.ndarray] = None,   # <-- new optional arg
    ) -> List[Tuple[str, float, float, float, float]]:
        """
        Return list of tuples: (candidate_id, final_score, emb_score, lab_score, kg_score)

        - candidate_embs: np.array shape (N, D) in same order as candidate_ids (fast)
        - OR candidate_emb_lookup: dict id->embedding (may also include the query embedding keyed by query_id)
        - OR pass query_emb explicitly via `query_emb` argument.
        - If none provided for candidates, raises.
        """
        N = len(candidate_ids)
        # 1) get candidate embeddings (build from lookup if needed)
        if candidate_embs is None:
            if candidate_emb_lookup is not None:
                candidate_embs = np.vstack([candidate_emb_lookup.get(str(cid), 
                                        np.zeros(next(iter(candidate_emb_lookup.values())).shape, dtype=float))
                                        for cid in candidate_ids])
            else:
                raise ValueError("Please provide candidate_embs or candidate_emb_lookup.")

        # basic shape check
        if candidate_embs.shape[0] != N:
            raise ValueError("candidate_embs rows must match candidate_ids length")

        # 2) embedding (cosine) scores
        # Resolve query embedding (multiple fallbacks)
        q_emb = None
        if candidate_emb_lookup is not None and str(query_id) in candidate_emb_lookup:
            q_emb = candidate_emb_lookup[str(query_id)]
        elif query_emb is not None:
            q_emb = query_emb
        else:
            # try find query inside candidate_ids and use that row from candidate_embs
            for i, cid in enumerate(candidate_ids):
                if str(cid) == str(query_id):
                    q_emb = candidate_embs[i]
                    break

        if q_emb is None:
            raise ValueError(
                "Query embedding not found. Provide candidate_emb_lookup keyed by query_id, "
                "or include the query_id in candidate_ids with matching candidate_embs, "
                "or pass query_emb explicitly."
            )

        emb_scores = [self.safe_cos(q_emb, candidate_embs[i]) for i in range(N)]

        # 3) label (Jaccard) scores
        q_labels = self.get_record_label_set(query_id)
        lab_scores = []
        for cid in candidate_ids:
            lab_scores.append(self.jaccard_sets(q_labels, self.get_record_label_set(cid)))

        # 4) KG scores
        kg_scores = []
        if self.record_kg_vectors is not None and self.record_kg_id2idx is not None:
            q_idx = self.record_kg_id2idx.get(str(query_id), None)
            q_kg = self.record_kg_vectors[q_idx] if q_idx is not None else self.get_record_kg_vec(str(query_id))
            for cid in candidate_ids:
                c_idx = self.record_kg_id2idx.get(str(cid), None)
                c_kg = self.record_kg_vectors[c_idx] if c_idx is not None else self.get_record_kg_vec(str(cid))
                kg_scores.append(self.safe_cos(q_kg, c_kg))
        else:
            q_kg = self.get_record_kg_vec(str(query_id))
            for cid in candidate_ids:
                c_kg = self.get_record_kg_vec(str(cid))
                kg_scores.append(self.safe_cos(q_kg, c_kg))

        # 5) normalize and combine
        emb_n = np.array(self.minmax_scale_list(emb_scores))
        lab_n = np.array(self.minmax_scale_list(lab_scores))
        kg_n  = np.array(self.minmax_scale_list(kg_scores))
        final = self.alpha * emb_n + self.beta * lab_n + self.gamma * kg_n

        ranked_idx = np.argsort(final)[::-1]
        if topk:
            ranked_idx = ranked_idx[:topk]
        out = []
        for i in ranked_idx:
            out.append((candidate_ids[i], float(final[i]), float(emb_n[i]), float(lab_n[i]), float(kg_n[i])))
        return out

class RetrievalEngine(abc.ABC):
    """
    Abstract base class for retrieval engines.
    Provides a convenience id->index mapping and embedding access.
    """

    def __init__(self, features_path: str, ids_path: str):
        # load embeddings and IDs once
        self.embs = np.load(features_path).astype("float32")  # shape (N, D)
        with open(ids_path, "r") as f:
            self.ids = json.load(f)                             # length N
        # id->index map for quick lookup (keys as strings)
        self.id2idx = {str(self.ids[i]): i for i in range(len(self.ids))}
        # small sanity
        assert self.embs.shape[0] == len(self.ids), "embeddings count != ids count"

    @abc.abstractmethod
    def retrieve(self, query_emb: np.ndarray, K: int = 5, **kwargs) -> Tuple[List[str], List[float]]:
        """
        Given a query embedding (D,) or (1,D), return top-K IDs and their scores.
        """
        pass

    def get_embeddings_for_ids(self, ids: List[str]) -> np.ndarray:
        """Return embeddings in same order as ids (zeros if missing)."""
        rows = []
        for _id in ids:
            idx = self.id2idx.get(str(_id), None)
            if idx is None:
                rows.append(np.zeros(self.embs.shape[1], dtype=self.embs.dtype))
            else:
                rows.append(self.embs[idx])
        return np.vstack(rows)


class DLSRetrievalEngine(RetrievalEngine):
    """
    DenseLinkSearch implementation:
    - Precomputes a cosine-similarity neighbor graph above a threshold.
    - On query, does a sublinear graph-traversal scan.
    """

    def __init__(
        self,
        features_path: str,
        ids_path: str,
        link_threshold: float = 0.5,
        max_links: int = 10,
        fdb_path: Optional[str] = None,
        name: Optional[str] = None,
    ):
        super().__init__(features_path, ids_path)

        # decide on a single cache path
        if fdb_path:
            self.fdb_path = Path(fdb_path)
        else:
            if name:
                self.fdb_path = FEATURES_PATH / name
            else:
                stem = Path(features_path).stem
                self.fdb_path = FEATURES_PATH / f"{stem}_link_graph.pkl"

        # ensure parent dir exists
        self.fdb_path.parent.mkdir(parents=True, exist_ok=True)

        # try load, rebuild on error or size mismatch
        rebuild = True
        if self.fdb_path.exists():
            try:
                with open(self.fdb_path, "rb") as f:
                    graph = pickle.load(f)
                if len(graph) == self.embs.shape[0]:
                    self.link_graph = graph
                    rebuild = False
            except Exception:
                rebuild = True

        if rebuild:
            self.link_graph = self._build_link_graph(link_threshold, max_links)
            with open(self.fdb_path, "wb") as f:
                pickle.dump(self.link_graph, f)

    def _build_link_graph(self, threshold: float, max_links: int) -> List[List[int]]:
        """
        Build an adjacency list of each node’s top-similarity neighbors above threshold.
        """
        sim = cosine_similarity(self.embs)        # (N, N)
        np.fill_diagonal(sim, -1)                 # ignore self-similarity

        graph = []
        for i in range(sim.shape[0]):
            # sort by descending similarity
            nbrs = np.argsort(sim[i])[::-1]
            # filter and truncate
            selected = [int(j) for j in nbrs if sim[i, j] >= threshold][:max_links]
            graph.append(selected)
        return graph

    def retrieve(
        self,
        query_emb: np.ndarray,
        K: int = 5,
        seed_size: int = 5,
        max_steps: int = 100,
        candidate_multiplier: int = 10,
        reranker: Optional["Reranker"] = None,
        query_id: Optional[str] = None,
        rerank_topk: Optional[int] = None
    ) -> Tuple[List[str], List[float]]:
        """
        Sublinear DLS retrieval via greedy graph-walk using a heap-based candidate pool.

        Optional reranker: pass a Reranker instance and query_id (str) to perform final reranking.
        If reranker is provided, it will be called with the top candidates and their embeddings.
        """
        q = query_emb.astype("float32").reshape(-1)
        N = self.embs.shape[0]

        if len(self.link_graph) != N:
            raise RuntimeError(
                f"Link graph size {len(self.link_graph)} != embeddings {N}. "
                "Rebuild or delete your pickle."
            )

        # --- Seed selection ---
        seeds = np.random.choice(N, size=min(seed_size, N), replace=False).tolist()
        visited = set(seeds)

        # --- Initial scoring: heap stores (-similarity, idx) so heapq gives max-sim first
        heap = []
        q_norm = np.linalg.norm(q) + 1e-6
        for idx in seeds:
            emb = self.embs[idx]
            sim = float(emb @ q / (np.linalg.norm(emb) * q_norm + 1e-12))
            heapq.heappush(heap, (-sim, idx))

        R = max(candidate_multiplier * K, seed_size)
        steps = 0

        # --- Greedy graph walk ---
        while steps < max_steps and heap:
            # Pop best candidate
            neg_sim, best_idx = heapq.heappop(heap)
            best_sim = -neg_sim
            improved = False

            # Expand neighbors
            for nbr in self.link_graph[best_idx]:
                if nbr < 0 or nbr >= N or nbr in visited:
                    continue
                visited.add(nbr)
                nbr_emb = self.embs[nbr]
                nbr_sim = float(nbr_emb @ q / (np.linalg.norm(nbr_emb) * q_norm + 1e-12))
                heapq.heappush(heap, (-nbr_sim, nbr))
                improved = True

            # Keep heap size bounded
            if len(heap) > R:
                heap = heapq.nsmallest(R, heap)  # keeps best R (lowest neg_sim)
                heapq.heapify(heap)

            if not improved:
                break

            steps += 1

        # --- Get top-K sorted by similarity ---
        topk = heapq.nsmallest(K, heap)
        topk = sorted([(-neg_sim, idx) for neg_sim, idx in topk], reverse=True)

        ids = [self.ids[idx] for _, idx in topk]
        scores = [sim for sim, _ in topk]

        cand_embs = self.get_embeddings_for_ids(ids)

        # build lookup that maps id->embedding and also include the query embedding
        cand_lookup = {str(rid): emb for rid, emb in zip(ids, cand_embs)}
        # try to get query embedding from engine if it exists there; else use the query vector q
        if query_id is not None and str(query_id) in self.id2idx:
            cand_lookup[str(query_id)] = self.embs[self.id2idx[str(query_id)]]
        else:
            cand_lookup[str(query_id)] = q 

        # --- optional reranking ---
        if reranker is not None and query_id is not None:
            # fetch candidate embeddings quickly
            cand_embs = self.get_embeddings_for_ids(ids)
            reranked = reranker.rerank(
                query_id=query_id,
                candidate_ids=ids,
                candidate_embs=cand_embs,
                candidate_emb_lookup=cand_lookup,
                topk=rerank_topk or K
            )
            # reranked: list of tuples (id, final_score, emb_score, lab_score, kg_score)
            ids = [t[0] for t in reranked]
            scores = [t[1] for t in reranked]

        return ids, scores

def make_retrieval_engine(
    features_path: str,
    ids_path: str,
    method: str = "dls",
    **kwargs
) -> RetrievalEngine:
    method = method.lower()
    if method == "dls":
        return DLSRetrievalEngine(
            features_path, ids_path,
            link_threshold=kwargs.get("link_threshold", 0.5),
            max_links=kwargs.get("max_links", 10),
            fdb_path=kwargs.get("fdb_path", None),
            name=kwargs.get("name", None)
        )
    else:
        raise ValueError(f"Unknown retrieval method: {method}")

# ---------------------------
# Minimal usage example
# ---------------------------
if __name__ == "__main__":
    # instantiate retrieval engine (uses your embeddings/ids)
    feats = str(EMBEDDINGS_DIR / "trainval_joint_embeddings.npy")
    idsf = str(EMBEDDINGS_DIR / "trainval_ids.json")
    engine = make_retrieval_engine(feats, idsf, method="dls", link_threshold=0.5, max_links=10)

    # instantiate reranker
    rer = Reranker(kg_dir=KG_DIR_DEFAULT, labels_csv=LABELS_CSV_DEF, alpha=0.6, beta=0.25, gamma=0.15)

    # Example: get query embedding (pick first id for demo)
    query_id = engine.ids[0]
    q_emb = engine.embs[0]

    # get top-50 candidates from DLS
    cand_ids, cand_scores = engine.retrieve(q_emb, K=50, seed_size=10, max_steps=200)

    # get embeddings for these candidates (fast)
    cand_embs = engine.get_embeddings_for_ids(cand_ids)

    # rerank using KG+labels
    ranked = rer.rerank(
        query_id=query_id,
        candidate_ids=cand_ids,
        candidate_embs=cand_embs,
        query_emb=q_emb,
        topk=20
    )
    print("Top 5 after rerank:", ranked[:5])

end src\Retrieval\retrieval.py

src\Retrieval\__init__.py
from .retrieval import RetrievalEngine, Reranker, make_retrieval_engine

__all__ = ["RetrievalEngine", "Reranker", "make_retrieval_engine"]
end src\Retrieval\__init__.py

src\Trainner\train.py
from pathlib import Path
import sys
try:
    base = Path(__file__).resolve().parent.parent
except NameError:
    base = Path.cwd().parent
sys.path.append(str(base))
import os
import json
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
from tqdm import tqdm
from sklearn.metrics import average_precision_score, f1_score, precision_recall_curve, precision_score, recall_score, accuracy_score
from transformers import get_cosine_schedule_with_warmup
from DataHandler import parse_openi_xml, build_dataloader
from Model import MultiModalRetrievalModel
from torch.utils.data import WeightedRandomSampler
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
from Helpers import kg_alignment_loss, contrastive_loss, Config, safe_roc_auc, safe_avg_precision
from KnowledgeGraph import KGBuilder, KGTransETrainer 
import wandb
import pandas as pd
from dotenv import load_dotenv
load_dotenv()

# --- Paths ---
BASE_DIR = Path(__file__).resolve().parent.parent.parent
XML_DIR = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
SPLIT_DIR = BASE_DIR / 'splited_data'
MODEL_DIR = BASE_DIR / 'models'
CHECKPOINT_DIR = BASE_DIR / 'checkpoints'
EMBED_SAVE_PATH = BASE_DIR / 'embeddings'
ATTN_DIR = BASE_DIR / 'attention_maps'
CSV_EVAL_SAVE_PATH = BASE_DIR / 'eval_csvs'
CONFIG_DIR = BASE_DIR / 'configs'
CSV_EVAL_SAVE_PATH.mkdir(exist_ok=True)
CHECKPOINT_DIR.mkdir(exist_ok=True)
EMBED_SAVE_PATH.mkdir(exist_ok=True)
ATTN_DIR.mkdir(exist_ok=True)
os.environ['TRANSFORMERS_CACHE'] = str(MODEL_DIR)

# --- Config ---
cfg = Config.load(CONFIG_DIR / 'config.yaml')
EPOCHS = cfg.epochs
PATIENCE = cfg.patience 
BATCH_SIZE = cfg.batch_size
LR = cfg.lr
USE_FOCAL = cfg.use_focal  # Toggle between BCEWithLogits and FocalLoss
USE_HYBRID = cfg.use_hybrid  # Toggle between BCEWithLogits + FocalLoss
FUSION_TYPE = cfg.fusion_type
JOINT_DIM = cfg.joint_dim
KG_MODE = cfg.kg_mode

# --- Loss parameters ---
gamma_FOCAL = cfg.gamma_focal  # Focal loss gamma parameter
FOCAL_RATIO = cfg.focal_ratio  # Ratio of focal loss in hybrid loss (if USE_HYBRID is True), BCE_RATIO = 1 - FOCAL_RATIO

# --- Hyperparameters ---
temperature = cfg.temperature                # temperature for contrastive loss
cls_weight   = cfg.cls_weight                  # focuses on getting the labels right (1.0 is very focus on classification, 0.0 is very focus on contrastive learning)
cont_weight  = cfg.cont_weight                  # focuses on pulling matching (image, text) embeddings closer in the joint space (1.0 is very focus on contrastive learning, 0.0 is very focus on classification)
weight_img_joint = cfg.weight_img_joint
weight_text_joint = cfg.weight_text_joint
kg_weight = cfg.kg_weight
kg_emb_dim = cfg.kg_emb_dim
kg_epochs = cfg.kg_epochs
if cfg.kg_method in ["cosine", "mse"]:
    kg_method = cfg.kg_method
else:
    raise RuntimeError(f"KG method {cfg.kg_method} not supported")
num_heads = cfg.num_heads                     # number of attention heads in the fusion model
num_fusion_layers= cfg.num_fusion_layers
use_shared_ffn = cfg.use_shared_ffn
text_dim = cfg.text_dim
use_cls_only = cfg.use_cls_only
image_backbone = cfg.image_backbone

# --- Wandb ---
project_name = cfg.project_name
run_name = cfg.run_name

def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Setup ---
device = get_device()
if device.type == "cpu":
    raise RuntimeError("No GPU available, run on a CUDA device for better performance")

# --- Focal Loss Class ---
class FocalLoss(nn.Module):
    def __init__(self, gamma=2, alpha=None, reduction='mean'):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction

    def forward(self, logits, targets):
        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        probs = torch.sigmoid(logits).clamp(min=1e-6, max=1-1e-6)
        focal_weight = (1 - probs).clamp(min=1e-6) ** self.gamma
        loss = focal_weight * bce
        if self.alpha is not None:
            loss = self.alpha * loss
        return loss.mean() if self.reduction == 'mean' else loss.sum()

# --- Evaluation ---
def evaluate(model, loader):
    model.eval()
    all_labels, all_logits, all_ids, all_embs,all_attns = [], [], [], [], []

    with torch.no_grad():
        for batch in loader:
            imgs = batch['image'].cuda()
            ids = batch['input_ids'].cuda()
            mask = batch['attn_mask'].cuda()
            labels = batch['labels'].cpu().numpy()
            id_list = batch['id']

            outputs      = model(imgs, ids, mask, return_attention=True)
            joint_emb    = outputs["joint_emb"]
            img_emb      = outputs["img_emb"]
            txt_emb      = outputs["txt_emb"]
            logits       = outputs["logits"]
            attn_weights = outputs["attn"]

            probs = torch.sigmoid(logits).cpu().numpy()

            all_labels.append(labels)
            all_logits.append(probs)
            all_ids.extend(id_list)
            all_embs.append(joint_emb.cpu())
            
            if attn_weights is not None:
                attn_cpu = {k: v.detach().cpu() for k, v in attn_weights.items()}
                all_attns.append(attn_cpu)

    y_true = np.vstack(all_labels)
    y_pred = np.vstack(all_logits)
    embeddings = torch.cat(all_embs).numpy()
    
    return y_true, y_pred, embeddings, all_ids, all_attns

def find_best_thresholds(y_true, y_logits):
    best_ts = []
    for i in range(y_true.shape[1]):
        p, r, t = precision_recall_curve(y_true[:, i], y_logits[:, i])
        f1 = 2 * p * r / (p + r + 1e-8)
        best_ts.append(t[f1.argmax()])
    return np.array(best_ts)

def check_label_consistency(records, labels_df, label_cols):
    mismatches = []

    for rec in records:
        rec_id = rec["id"]
        if rec_id not in labels_df.index:
            mismatches.append((rec_id, "Missing in labels_df"))
            continue
        
        df_vector = labels_df.loc[rec_id, label_cols].tolist()
        record_vector = rec["labels"]

        if list(map(int, df_vector)) != list(map(int, record_vector)):
            mismatches.append((rec_id, df_vector, record_vector))
    
    if mismatches:
        print(f"Found {len(mismatches)} mismatched records!")
        # Print the first few mismatches
        for i, item in enumerate(mismatches[:5]):
            print(f"\nMismatch #{i+1}")
            print("ID:", item[0])
            print("From labels_df:", item[1])
            print("From record:   ", item[2])
    else:
        print(" All label vectors match the CSV.")

    return mismatches

def df_to_records(df):
    return [
        {
            "id": row["id"],
            "report_text": row["report_text"],
            "labels": [row[f] for f in label_cols],
            "dicom_path": row["dicom_path"]
        }
        for _, row in df.iterrows()
    ]

# --- Main ---
if __name__ == '__main__':
    # Used groups
    combined_groups = {**disease_groups, **normal_groups, **finding_groups, **symptom_groups}
    
    # --- Train KG ---
    kg_trainer = KGTransETrainer(
        kg_dir=BASE_DIR/"knowledge_graph",
        emb_dim=cfg.kg_emb_dim,
        joint_dim=cfg.joint_dim
    )

    if not (BASE_DIR/"knowledge_graph"/"node_embeddings.npy").exists():
        print("Training Knowledge Graph embeddings...")
        kg_builder = KGBuilder(out_dir=BASE_DIR/"knowledge_graph", combined_groups=combined_groups)
        
        KGBuilder.ensure_exists(XML_DIR, DICOM_ROOT, mode=KG_MODE)
        kg_trainer.load_triples()   # assumes triples.csv already built
        kg_trainer.train(epochs=cfg.kg_epochs, log_to_wandb=True)
        kg_trainer.save_embeddings()
    else:
        print("Using cached KG embeddings")


    # --- Load best KG embeddings ---
    kg_dir = BASE_DIR / "knowledge_graph"

    # prefer best checkpoint
    best_node_path = kg_dir / "node_embeddings_best.npy"
    best_rel_path  = kg_dir / "rel_embeddings_best.npy"

    if best_node_path.exists():
        node_emb_path = best_node_path
    else:
        # fallback to generic / epoch embeddings
        candidates = sorted(kg_dir.glob("node_embeddings_epoch*.npy"))
        if not candidates:
            candidates = sorted(kg_dir.glob("node_embeddings*.npy"))
        if not candidates:
            raise FileNotFoundError("No node_embeddings found in knowledge_graph/")
        node_emb_path = candidates[-1]

    print(f"[Train] Using KG embeddings from {node_emb_path}")
    kg_embs = np.load(node_emb_path)
    kg_embs = torch.tensor(kg_embs, dtype=torch.float32, device=device)

    # load node2id map
    with open(kg_dir / "node2id.json") as f:
        node2id = json.load(f)

    # --- Load records + split ---
    print("Loading records...")
    parsed_records = parse_openi_xml(XML_DIR, DICOM_ROOT, combined_groups=combined_groups)
    labels_df = pd.read_csv(BASE_DIR / "outputs" / "openi_labels_final.csv").set_index("id")
    label_cols = list(disease_groups.keys()) + list(normal_groups.keys()) + list(finding_groups.keys()) + list(symptom_groups.keys())
    
    records = []
    for rec in parsed_records:
        rec_id = rec["id"]
        if rec_id in labels_df.index:
            label_vec = labels_df.loc[rec_id, label_cols].tolist()
            records.append({
                "id": rec["id"],
                "report_text": rec["report_text"],
                "dicom_path": rec["dicom_path"],
                "labels": label_vec
            })

    mismatches = check_label_consistency(records, labels_df, label_cols)
    if mismatches:
        raise Exception("Mismatched records found in records and labels_df")

    with open(SPLIT_DIR / "train_split_ids.json") as f:
        train_ids = set(json.load(f))
    with open(SPLIT_DIR / "val_split_ids.json") as f:
        val_ids = set(json.load(f))

    train_records = [r for r in records if r["id"] in train_ids]
    val_records   = [r for r in records if r["id"] in val_ids]

    print(f"Train size: {len(train_records)}, Val size: {len(val_records)}")

    # --- compute per‐label frequencies ---
    label_sums = np.zeros(len(label_cols), dtype=float)
    for r in train_records:
        label_sums += np.array(r['labels'], dtype=float)
    pos_freq_np = label_sums / len(train_records)
    
    # --- per‐sample weights for sampler ---
    inv_freq = 1.0 / pos_freq_np.clip(min=1e-3)
    sample_weights = []
    for r in train_records:
        label_vec = np.array(r['labels'], dtype=float)
        if label_vec.sum() > 0:
            weight = (inv_freq * label_vec).max()
        else:
            weight = 1.0  # for all-negative samples
        sample_weights.append(weight)

    sampler = WeightedRandomSampler(
        sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )

    # --- Dataloaders ---
    train_loader = build_dataloader(
        train_records,
        batch_size=BATCH_SIZE,
        mean=0.5, std=0.25,
        sampler=sampler,
        max_length=text_dim
    )

    val_loader = build_dataloader(
        val_records,
        batch_size=BATCH_SIZE,
        mean=0.5, std=0.25,
        max_length=text_dim
    )

    # --- Loss weights & criterion ---
    label_counts = np.array([r['labels'] for r in train_records]).sum(axis=0)
    num_samples  = len(train_records)   
    pos_weight = ((num_samples - torch.tensor(label_counts)) / torch.tensor(label_counts)).to(device)
    pos_weight = torch.clamp(pos_weight, min=1.0)  # Ensure no zero weights
    pos_weight = pos_weight.cuda()

    # Inverse frequency for Focal Loss
    pos_freq_t = torch.tensor(pos_freq_np, dtype=torch.float32).to(device)
    inv_freq = 1.0 / pos_freq_t.clamp(min=1e-3)  
    alpha    = (inv_freq / inv_freq.sum()).to(device)
    alpha = alpha.clamp(min=1e-4)

    if USE_FOCAL:
        alpha = alpha.to(device)
        criterion = FocalLoss(gamma=gamma_FOCAL, alpha=alpha)
    elif USE_HYBRID:
        # --- Hybrid BCE + Focal ---
        bce   = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        alpha = torch.as_tensor(inv_freq, dtype=torch.float32, device=device)
        focal = FocalLoss(gamma=gamma_FOCAL, alpha=alpha)
        criterion = lambda logits, labels: (1 - FOCAL_RATIO) * bce(logits, labels) + FOCAL_RATIO * focal(logits, labels)
    else:
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

    # --- Model, optimizer, scheduler ---
    model = MultiModalRetrievalModel(
        joint_dim=JOINT_DIM,
        num_classes=len(label_cols),
        num_fusion_layers=num_fusion_layers,
        num_heads=num_heads,
        fusion_type=FUSION_TYPE,
        img_backbone=image_backbone,
        swin_ckpt_path=MODEL_DIR / "swin_checkpoint.safetensors",
        bert_local_dir= MODEL_DIR / "clinicalbert_local",
        device=device,
        use_shared_ffn=cfg.use_shared_ffn,
        use_cls_only=cfg.use_cls_only,
        training=True
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    total_steps = EPOCHS * len(train_loader)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),   # 10% warmup
        num_training_steps=total_steps
    )

    # --- wandb init ---
    wandb.init(project=project_name, name = run_name, config={
        "epochs": EPOCHS,
        "lr": LR,
        "batch_size": BATCH_SIZE,
        "loss": "focal" if USE_FOCAL else "BCEWithLogits",
        "fusion": FUSION_TYPE
    })
    wandb.define_metric("epoch")
    wandb.define_metric("val_precision", step_metric="epoch")
    wandb.define_metric("val_recall",    step_metric="epoch")
    wandb.define_metric("val_accuracy",  step_metric="epoch")
    wandb.define_metric("kg/epoch")
    wandb.define_metric("kg/*", step_metric="kg/epoch")
    wandb.define_metric("kg/loss", step_metric="kg/epoch")
    wandb.define_metric("kg/val_mrr", step_metric="kg/epoch")
    wandb.define_metric("kg/val_hits1", step_metric="kg/epoch")
    wandb.define_metric("kg/val_hits10", step_metric="kg/epoch")

    for cn in label_cols:
        wandb.define_metric(f"val_auc_{cn}", step_metric="epoch")
        wandb.define_metric(f"val_f1_{cn}",  step_metric="epoch")
        wandb.define_metric(f"val_prec_{cn}",  step_metric="epoch")
        wandb.define_metric(f"val_rec_{cn}",   step_metric="epoch")

    labels = torch.tensor([r['labels'] for r in train_records], dtype=torch.float32).to(device)
    batch_labels = labels.mean(dim=0).cpu().numpy()
    wandb.log({f"batch_pos_freq/{cn}": float(batch_labels[i]) 
            for i, cn in enumerate(label_cols)}, step=0)

    # --- Early stopping ---
    best_score = -float("inf")
    patience_counter = 0
    
    print("Starting training...")
    # --- Training Loop ---
    for epoch in range(EPOCHS):
        model.train()
        epoch_loss = 0
        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}")):
            imgs = batch['image'].to(device)
            ids = batch['input_ids'].to(device)
            mask = batch['attn_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            # joint_emb: (B, joint_dim), logits: (B, num_classes)
            outputs      = model(imgs, ids, mask, return_attention=True)
            joint_emb    = outputs["joint_emb"]
            img_emb      = outputs["img_emb"]
            txt_emb      = outputs["txt_emb"]
            logits       = outputs["logits"]
            attn_weights = outputs["attn"]

            # Classification loss
            cls_loss = criterion(logits, labels)

            # Contrastive loss
            loss_img_txt   = contrastive_loss(img_emb, txt_emb, temperature)
            loss_img_joint = contrastive_loss(img_emb, joint_emb, temperature)
            loss_txt_joint = contrastive_loss(txt_emb, joint_emb, temperature)

            cont_loss = loss_img_txt + weight_img_joint * loss_img_joint + weight_text_joint * loss_txt_joint
            
            kg_loss = kg_alignment_loss(
                joint_emb,
                batch['id'],
                kg_embs,
                node2id,
                trainer=kg_trainer,
                labels=batch['labels'],
                label_cols=label_cols,
                loss_type=cfg.kg_method
            )

            # Combined loss (InfoNCE term + chosen loss)
            loss = cls_weight * cls_loss + cont_weight * cont_loss + kg_weight * kg_loss

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

            epoch_loss += loss.item()
            wandb.log({
                "batch_train_loss": loss.item(),
                "cls_loss": cls_loss.item(),
                "cont_loss": cont_loss.item(),
                "kg_loss": kg_loss.item(),
                "epoch": epoch + 1,
                "batch_idx": batch_idx
            })

        # --- Validation & threshold tuning ---
        avg_train_loss = epoch_loss / len(train_loader)
        wandb.log({
            "train_loss": avg_train_loss,
            "epoch": epoch + 1
        })

        y_true, y_pred, val_embs, val_ids, val_attns = evaluate(model, val_loader)
        best_ts = find_best_thresholds(y_true, y_pred)
        y_bin = (y_pred > best_ts[None, :]).astype(int)

        for i, cn in enumerate(label_cols):
            wandb.log({f"thresh_{cn}": float(best_ts[i]), "epoch": epoch+1})
        
        # macro metrics (averaged over all classes)
        # per-class AUROC (nan if invalid)
        class_aucs = safe_roc_auc(y_true, y_pred, label_names=label_cols)   # shape (C,)

        # per-class Average Precision (PR-AUC), returns NaN for invalid classes
        class_aps = safe_avg_precision(y_true, y_pred, label_names=label_cols)  # shape (C,)

        # thresholds already computed earlier as `best_ts` (if not, use 0.5 during training)
        y_bin = (y_pred > best_ts[None, :]).astype(int)

        # per-class F1 / precision / recall (these functions handle zero_division)
        class_f1s   = f1_score(y_true, y_bin, average=None, zero_division=0)
        class_precs = precision_score(y_true, y_bin, average=None, zero_division=0)
        class_recs  = recall_score(y_true, y_bin, average=None, zero_division=0)

        # aggregated metrics
        # macro AUC = mean over valid classes (ignore NaNs)
        macro_auc = float(np.nanmean(class_aucs))
        macro_ap  = float(np.nanmean(class_aps))

        # micro PR-AUC (global): wrap in try/except because it can error when no positives at all
        try:
            micro_ap = float(average_precision_score(y_true, y_pred, average='micro'))
        except Exception:
            micro_ap = float("nan")

        # micro F1 / precision / recall (global counts)
        micro_f1   = float(f1_score(y_true, y_bin, average='micro', zero_division=0))
        micro_prec = float(precision_score(y_true, y_bin, average='micro', zero_division=0))
        micro_rec  = float(recall_score(y_true, y_bin, average='micro', zero_division=0))

        # macro F1/prec/rec (still useful)
        macro_f1   = float(f1_score(y_true, y_bin, average='macro'))
        macro_prec = float(precision_score(y_true, y_bin, average='macro', zero_division=0))
        macro_rec  = float(recall_score(y_true, y_bin, average='macro', zero_division=0))

        val_metrics = {
            "val_auc_macro": macro_auc,
            "val_ap_macro": macro_ap,
            "val_ap_micro": micro_ap,
            "val_f1_macro": macro_f1,
            "val_f1_micro": micro_f1,
            "val_prec_macro": macro_prec,
            "val_prec_micro": micro_prec,
            "val_rec_macro": macro_rec,
            "val_rec_micro": micro_rec,
            "val_accuracy": float(accuracy_score(y_true, y_bin)),
            "epoch": epoch + 1
        }
        wandb.log(val_metrics)

        # save CSV for EDA
        df_eval = pd.DataFrame({'id': val_ids})
        for i, cn in enumerate(label_cols):
            df_eval[f'true_{cn}'] = y_true[:,i]
            df_eval[f'pred_{cn}'] = y_pred[:,i]
        df_eval.to_csv(CSV_EVAL_SAVE_PATH/f"eval_epoch_{epoch}.csv", index=False)

        # --- Checkpointing & early stop ---
        torch.save(model.state_dict(), CHECKPOINT_DIR / f"model_epoch_{epoch+1}.pt")

        # --- Early stopping ---
        composite = 0.5 * macro_f1 + 0.5 * macro_auc
        wandb.log({"val_composite": composite, "epoch": epoch + 1})
        if composite > best_score:
            best_score = composite
            patience_counter = 0
            torch.save(model.state_dict(), CHECKPOINT_DIR / "model_best.pt")
            np.save(EMBED_SAVE_PATH / "val_joint_embeddings.npy", val_embs)
            torch.save(val_attns, ATTN_DIR / "val_attn_weights.npy")
            with open(EMBED_SAVE_PATH / "val_ids.json", "w") as f:
                json.dump(val_ids, f)
        else:
            patience_counter += 1
            if patience_counter >= PATIENCE:
                print("Early stopping triggered.")
                break

    # Always save last val embeddings (even if not best)
    y_true, y_pred, val_embs, val_ids, val_attns = evaluate(model, val_loader)
    best_ts = find_best_thresholds(y_true, y_pred)
    y_bin = (y_pred > best_ts[None,:]).astype(int)

    all_class_aucs = safe_roc_auc(y_true, y_pred)
    val_metrics = {
            'val_auc':       np.nanmean(all_class_aucs),
            'val_f1':        f1_score(y_true, y_bin, average='macro'),
            'val_precision': precision_score(y_true, y_bin, average='macro', zero_division=0),
            'val_recall':    recall_score(y_true, y_bin, average='macro', zero_division=0),
            'val_accuracy':  accuracy_score(y_true, y_bin),
            'epoch':         epoch+1
        }

        # per-class metrics
    class_aucs = safe_roc_auc(y_true, y_pred)
    class_f1s  = f1_score(y_true, y_bin, average=None)
    class_precs= precision_score(y_true, y_bin, average=None, zero_division=0)
    class_recs = recall_score(y_true, y_bin, average=None, zero_division=0)

    for i, cn in enumerate(label_cols):
        val_metrics[f'val_auc_{cn}'] = class_aucs[i]
        val_metrics[f'val_f1_{cn}'] = class_f1s[i]
        val_metrics[f'val_prec_{cn}'] = class_precs[i]
        val_metrics[f'val_rec_{cn}'] = class_recs[i]

    df_eval = pd.DataFrame({'id': val_ids})
    for i, cn in enumerate(label_cols):
        df_eval[f'true_{cn}'] = y_true[:, i]
        df_eval[f'pred_{cn}'] = y_pred[:, i]
    df_eval.to_csv(CSV_EVAL_SAVE_PATH / "eval_last.csv", index=False)
    np.save(EMBED_SAVE_PATH / "val_last_embeddings.npy", val_embs)
    torch.save(val_attns, ATTN_DIR / "val_last_attn_weights.npy")
    with open(EMBED_SAVE_PATH / "val_last_ids.json", "w") as f:
        json.dump(val_ids, f)

    print("Training complete.")
    print("Saving train joint embeddings...")

    # Load best model and save train embeddings
    model.load_state_dict(torch.load(CHECKPOINT_DIR / "model_best.pt"))
    model.eval()
    
    y_true, y_pred, train_embs, train_ids, train_attns = evaluate(model, train_loader)
    np.save(EMBED_SAVE_PATH / "train_joint_embeddings.npy", train_embs)
    torch.save(train_attns, ATTN_DIR / "train_attn_weights.npy")

    with open(EMBED_SAVE_PATH / "train_ids.json", "w") as f:
        json.dump(train_ids, f)

    print("Done.")

end src\Trainner\train.py

src\Trainner\__init__.py

end src\Trainner\__init__.py

src\web\app.py
import io
import base64
import sys
import pickle
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent / "src"))

from flask import Flask, render_template, request
import numpy as np
import torch
import time
import matplotlib
matplotlib.use("Agg")  # Use non-interactive backend for rendering images
import matplotlib.pyplot as plt
import pandas as pd
from transformers import AutoTokenizer
import torch.nn.functional as F

from Helpers import Config, compare_maps, heatmap_to_base64_overlay, attention_to_html
from DataHandler import DICOMImagePreprocessor, parse_openi_xml
from Model import MultiModalRetrievalModel
from LabelData import disease_groups, normal_groups, finding_groups, symptom_groups
from Retrieval import make_retrieval_engine

plt.ioff()

# ── Project directories ────────────────────────────────────────────────────────
BASE_DIR       = Path(__file__).resolve().parent.parent
CKPT_DIR       = BASE_DIR / "checkpoints"
MODEL_DIR      = BASE_DIR / "models"
EMBEDDINGS_DIR = BASE_DIR / "embeddings"
OUTPUT_DIR    = BASE_DIR / "outputs"
XML_DIR = BASE_DIR / 'data' / 'openi' / 'xml' / 'NLMCXR_reports' / 'ecgen-radiology'
DICOM_ROOT = BASE_DIR / 'data' / 'openi' / 'dicom'
CONFIG_DIR     = BASE_DIR / "configs"
CKPT_PATH      = BASE_DIR / "checkpoints" / "model_best.pt"
REPORT_CACHE_PATH = OUTPUT_DIR / "openi_reports.pkl"
#────────────────────────────────────────────────────────────────────────────────

cfg = Config.load(CONFIG_DIR / "config.yaml")
app = Flask(__name__, static_folder="static", template_folder="templates")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

label_cols = list(disease_groups.keys()) + list(normal_groups.keys())
labels_df = pd.read_csv(OUTPUT_DIR / "openi_labels_final.csv").set_index("id")

def load_report_lookup_via_parser(xml_dir, dicom_root) -> dict:
    if REPORT_CACHE_PATH.exists():
        print(f"[INFO] Loading cached report lookup from {REPORT_CACHE_PATH.name}")
        with open(REPORT_CACHE_PATH, "rb") as f:
            return pickle.load(f)

    print("[INFO] Parsing reports using parse_openi_xml()...")
    parsed_records = parse_openi_xml(xml_dir, dicom_root)
    id_to_report = {
        rec["id"]: rec["report_text"]
        for rec in parsed_records
        if "id" in rec and "report_text" in rec
    }

    with open(REPORT_CACHE_PATH, "wb") as f:
        pickle.dump(id_to_report, f)
    print(f"[INFO] Cached {len(id_to_report)} reports to {REPORT_CACHE_PATH.name}")
    return id_to_report

def find_dicom_file(rid: str) -> Path:
    primary = list(DICOM_ROOT.rglob(f"{rid}.dcm"))
    if primary:
        return primary[0]

    # Try fallback without leading patient ID
    fallback_id = "_".join(rid.split("_")[1:])
    fallback = list(DICOM_ROOT.rglob(f"{fallback_id}.dcm"))
    if fallback:
        print(f"[INFO] Using fallback DICOM path: {fallback[0].name}")
        return fallback[0]

    raise FileNotFoundError(f"DICOM not found for either {rid}.dcm or {fallback_id}.dcm")

report_lookup = load_report_lookup_via_parser(XML_DIR, DICOM_ROOT)

retriever = make_retrieval_engine(
    features_path=str(EMBEDDINGS_DIR / "train_joint_embeddings.npy"),
    ids_path=str(EMBEDDINGS_DIR / "train_ids.json"),
    method="dls",
    link_threshold=0.5,
    max_links=10
)

model = MultiModalRetrievalModel(
    joint_dim=cfg.joint_dim,
    num_heads=cfg.num_heads,
    num_fusion_layers=cfg.num_fusion_layers,
    num_classes=len(label_cols),
    fusion_type=cfg.fusion_type,
    swin_ckpt_path=MODEL_DIR / "swin_checkpoint.safetensors",
    bert_local_dir= MODEL_DIR / "clinicalbert_local",
    checkpoint_path=str(CKPT_PATH),
    use_shared_ffn=cfg.use_shared_ffn,
    use_cls_only=cfg.use_cls_only,
    device=device,
    retriever=retriever
).to(device)
model.eval()

preproc   = DICOMImagePreprocessor()
tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")

def np_to_base64_img(arr: np.ndarray, cmap="gray") -> str:
    fig, ax = plt.subplots(figsize=(4, 4))

    # Handle (1, H, W) => (H, W)
    if arr.ndim == 3 and arr.shape[0] == 1:
        arr = arr[0]

    if arr.ndim == 2:
        ax.imshow(arr, cmap=cmap)
    else:
        ax.imshow(arr)

    ax.axis("off")
    buf = io.BytesIO()
    plt.tight_layout(pad=0)
    fig.savefig(buf, format="png", bbox_inches="tight", pad_inches=0)
    plt.close(fig)
    buf.seek(0)
    return base64.b64encode(buf.read()).decode("ascii")

def resize_to_match(src_map, target_shape):
    """Resize src_map (H,W) to target_shape (H2,W2) using bilinear interpolation."""
    src_tensor = torch.tensor(src_map, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]
    resized = F.interpolate(src_tensor, size=target_shape, mode="bilinear", align_corners=False)
    return resized.squeeze().numpy()

def to_numpy(x):
    """Return a numpy array whether x is a torch tensor or numpy already."""
    if x is None:
        return None
    if torch.is_tensor(x):
        return x.detach().cpu().numpy()
    return np.asarray(x)

def safe_unpack_topk(topk_any):
    """Normalize topk returned shapes to a flat python list."""
    if topk_any is None:
        return []
    # if nested lists (B x K) and user passed topk for batch, take first row
    if isinstance(topk_any, list) and len(topk_any) > 0 and isinstance(topk_any[0], (list, tuple)):
        return list(topk_any[0])
    # if numpy array
    try:
        return list(np.array(topk_any).tolist())
    except Exception:
        return list(topk_any)

@app.route("/", methods=["GET", "POST"])
def index():
    context = {}
    if request.method == "POST":
        print("[INFO] POST request")
        # --- Load inputs ---
        dcm_bytes  = request.files["dicom_file"].read()
        raw_tensor = preproc(dcm_bytes)                 # shape (C,H,W) or (1,H,W)
        img_tensor = raw_tensor.unsqueeze(0).to(device) # (1,C,H,W)
        threshold = float(request.form.get("threshold", 0.5))
        text_query = request.form.get("text_query", "") or ""
        tokens    = tokenizer(
            text_query,
            return_tensors="pt",
            padding="max_length",
            truncation=True,
            max_length=cfg.text_dim
        )
        txt_ids  = tokens.input_ids.to(device)
        txt_mask = tokens.attention_mask.to(device)
        
        # prepare orig_arr for visualization (H,W) scaled 0..1
        orig_arr = raw_tensor.squeeze().cpu().numpy()
        if orig_arr.dtype != np.float32 and orig_arr.max() > 1.0:
            orig_arr = orig_arr.astype(np.float32) / 255.0

        # --- Predict + Explain for query image ---
        start_time = time.perf_counter()
        out = model.predict(img_tensor, txt_ids, txt_mask, K=5, explain=True)
        predict_time = time.perf_counter() - start_time

        # Get explaining outputs
        att_maps_q = out.get("attention_map", {}) or {}
        valid_keys = ["txt2img", "img2txt", "comb_img", "comb_txt", "final_patch_map", "final_token_map"]
        
        if not any(k in att_maps_q and att_maps_q[k] is not None for k in valid_keys):
            raise RuntimeError("No usable attention maps found in output")
        else:
            print("Attention maps found:", [k for k in valid_keys if k in att_maps_q])

        # IG maps (target-based: class indices)
        ig_maps_q = out.get("ig_maps", {}) or {}
        ig_maps_q = {int(k): to_numpy(v) for k, v in ig_maps_q.items()}
        if not ig_maps_q:  # just check non-empty
            raise RuntimeError("IG maps not found in output")
        else:
            print(f"IG maps found for targets: {list(ig_maps_q.keys())}")

        # Grad-CAM maps (target-based: class indices)
        gradcam_maps_q = out.get("gradcam_maps", {}) or {}
        gradcam_maps_q = {int(k): to_numpy(v) for k, v in gradcam_maps_q.items()}
        if not gradcam_maps_q:
            raise RuntimeError("Grad-CAM maps not found in output")
        else:
            print(f"Grad-CAM maps found for targets: {list(gradcam_maps_q.keys())}")

        # pick which IG target to show: prefer topk[0] if available
        topk_idx = safe_unpack_topk(out.get("topk_idx", []))
        topk_vals = safe_unpack_topk(out.get("topk_vals", []))
        main_target = None
        if topk_idx:
            try:
                main_target = int(topk_idx[0])
            except Exception:
                main_target = None
        if main_target is None and len(ig_maps_q) > 0:
            main_target = list(ig_maps_q.keys())[0]

        # prepare visuals for the query image
        context['img_orig'] = np_to_base64_img(img_tensor.squeeze().cpu().numpy(), cmap="gray")
        context["text_query"] = text_query
        context['attn_map_q_b64'] = None
        context['ig_map_q_b64']   = None
        context['gradcam_map_q_b64'] = None

        if att_maps_q is not None:
            try:
                # --- Attention overlays ---
                context['attn_txt2img_b64']   = heatmap_to_base64_overlay(orig_arr, att_maps_q.get("txt2img"), alpha=0.45)
                context['attn_comb_img_b64']  = heatmap_to_base64_overlay(orig_arr, att_maps_q.get("comb_img"), alpha=0.45)
                context['attn_final_img_b64'] = heatmap_to_base64_overlay(orig_arr, att_maps_q.get("final_patch_map"), alpha=0.45)

                # --- Attention highlights (text) ---
                tokens_decoded = tokenizer.convert_ids_to_tokens(txt_ids[0])
                context['attn_img2txt_html']  = attention_to_html(tokens_decoded, att_maps_q.get("img2txt")) if att_maps_q.get("img2txt") is not None else None
                context['attn_comb_txt_html'] = attention_to_html(tokens_decoded, att_maps_q.get("comb_txt")) if att_maps_q.get("comb_txt") is not None else None
                context['attn_final_txt_html']= attention_to_html(tokens_decoded, att_maps_q.get("final_token_map")) if att_maps_q.get("final_token_map") is not None else None
            except Exception as e:
                print(f"[WARN] query attention overlay failed: {e}")

        if main_target is not None and main_target in ig_maps_q:
            try:
                ig_q = ig_maps_q[main_target]
                context['ig_map_q_b64'] = heatmap_to_base64_overlay(orig_arr, ig_q, alpha=0.45)
            except Exception as e:
                print(f"[WARN] query IG overlay failed: {e}")

        if main_target is not None and main_target in gradcam_maps_q:
            try:
                gc_q = gradcam_maps_q[main_target]
                context['gradcam_map_q_b64'] = heatmap_to_base64_overlay(orig_arr, gc_q, alpha=0.45)
            except Exception as e:
                print(f"[WARN] query Grad-CAM overlay failed: {e}")

        # compute quick numeric comparisons for the query's own att vs ig
        image_attn_keys = ["txt2img", "comb_img", "final_patch_map"]
        context['attn_ig_metrics'] = {}
        if main_target is not None and main_target in ig_maps_q:
            for key in image_attn_keys:
                if att_maps_q.get(key) is not None:
                    try:
                        cm5  = compare_maps(att_maps_q[key], ig_maps_q[main_target], topk_frac=0.05)
                        cm20 = compare_maps(att_maps_q[key], ig_maps_q[main_target], topk_frac=0.20)
                        context['attn_ig_metrics'][key] = {
                            'pearson': round(cm5.get('pearson', 0.0), 4),
                            'spearman': round(cm5.get('spearman', 0.0), 4),
                            'iou_top5pct': round(cm5.get('iou_top5pct', 0.0), 4),
                            'iou_top20pct': round(cm20.get('iou_top20pct', 0.0), 4)
                        }
                    except Exception as e:
                        print(f"[WARN] compare_maps(query-{key}) failed: {e}")

        context['gc_attn_metrics'] = {}
        if main_target is not None and main_target in gradcam_maps_q:
            for key in image_attn_keys:
                if att_maps_q.get(key) is not None:
                    try:
                        cm5  = compare_maps(att_maps_q[key], gradcam_maps_q[main_target], topk_frac=0.05)
                        cm20 = compare_maps(att_maps_q[key], gradcam_maps_q[main_target], topk_frac=0.20)
                        context['gc_attn_metrics'][key] = {
                            'pearson': round(cm5.get('pearson', 0.0), 4),
                            'spearman': round(cm5.get('spearman', 0.0), 4),
                            'iou_top5pct': round(cm5.get('iou_top5pct', 0.0), 4),
                            'iou_top20pct': round(cm20.get('iou_top20pct', 0.0), 4)
                        }
                    except Exception as e:
                        print(f"[WARN] compare_maps(query-{key}) failed: {e}")

        # --- Preds / topk ---
        preds_arr = to_numpy(out.get("preds", None))
        context["preds"] = preds_arr[0] if preds_arr is not None and len(preds_arr) > 0 else None
        context["topk_idx"]  = topk_idx
        context["topk_vals"] = topk_vals
        context["topk_labels"] = [label_cols[i] for i in topk_idx] if topk_idx else []
        context["threshold"] = threshold
        context["pred_labels"] = [label_cols[i] for i, v in enumerate(context["preds"]) if v == 1] if context["preds"] is not None else []

        # --- Retrieval list (returned for the query) ---
        retrieval_pairs = list(zip(out.get("retrieval_ids", []), out.get("retrieval_dists", [])))
        context["retrieval"] = retrieval_pairs

        # Option flags
        show_image = "show_image" in request.form
        show_detail = "show_retrieval_detail" in request.form
        context["show_image"] = show_image
        context["show_retrieval_detail"] = show_detail

        # --- If user wants detailed retrieval: compute visuals & comparisons for each retrieved image ---
        retrieval_detailed = []
        retrieved_att_maps = []
        if show_detail and retrieval_pairs:
            for rid, dist in retrieval_pairs:
                try:
                    # labels & report for retrieved item
                    label_vec = labels_df.loc[rid][label_cols].astype(int).values
                    label_names = [label_cols[i] for i, v in enumerate(label_vec) if v == 1]
                    report_text = report_lookup.get(rid, "No report found")

                    # find dicom and preprocess
                    dcm_path = find_dicom_file(rid)
                    dcm_tensor = preproc(dcm_path)  # (C, H, W)
                    orig_arr_r = dcm_tensor.squeeze().cpu().numpy()
                    if orig_arr_r.dtype != np.float32 and orig_arr_r.max() > 1.0:
                        orig_arr_r = orig_arr_r.astype(np.float32) / 255.0

                    # compute image bytes for UI
                    img_b64 = np_to_base64_img(dcm_tensor.numpy(), cmap="gray")

                    att_maps_r, ig_maps_r, gradcam_maps_r = {}, {}, {}

                    try:
                        out_ret = model.get_explain_score(
                            dcm_tensor.unsqueeze(0).to(device),
                            txt_ids, txt_mask, main_target
                        )

                        # Use same keys/structure as query
                        att_maps_r = out_ret.get("attention_map", {})
                        ig_maps_r      = out_ret.get("ig_maps", {})
                        gradcam_maps_r = out_ret.get("gradcam_maps", {})

                    except Exception as e:
                        print(f"[WARN] failed to compute explanations for retrieved {rid}: {e}")


                    # create base64 overlays for retrieved item
                    attn_txt2img_b64_r, attn_comb_img_b64_r, attn_final_img_b64_r = None, None, None
                    attn_img2txt_html_r, attn_comb_txt_html_r, attn_final_txt_html_r = None, None, None
                    ig_b64_r, gradcam_b64_r = None, None
                    ig_map_r, gradcam_map_r = None, None

                    try:
                        if att_maps_r.get("txt2img") is not None:
                            attn_txt2img_b64_r = heatmap_to_base64_overlay(orig_arr_r, att_maps_r["txt2img"], alpha=0.45)

                        if att_maps_r.get("comb_img") is not None:
                            attn_comb_img_b64_r = heatmap_to_base64_overlay(orig_arr_r, att_maps_r["comb_img"], alpha=0.45)

                        if att_maps_r.get("final_patch_map") is not None:
                            attn_final_img_b64_r = heatmap_to_base64_overlay(orig_arr_r, att_maps_r["final_patch_map"], alpha=0.45)

                        # Text highlights (HTML rendering)
                        tokens_r = tokenizer(
                            report_text,
                            return_tensors="pt",
                            padding="max_length",
                            truncation=True,
                            max_length=cfg.text_dim
                        )
                        tokens_decoded_r = tokenizer.convert_ids_to_tokens(tokens_r.input_ids[0])

                        if att_maps_r.get("img2txt") is not None:
                            attn_img2txt_html_r = attention_to_html(tokens_decoded_r, att_maps_r["img2txt"])

                        if att_maps_r.get("comb_txt") is not None:
                            attn_comb_txt_html_r = attention_to_html(tokens_decoded_r, att_maps_r["comb_txt"])

                        if att_maps_r.get("final_token_map") is not None:
                            attn_final_txt_html_r = attention_to_html(tokens_decoded_r, att_maps_r["final_token_map"])

                    except Exception as e:
                        print(f"[WARN] render retrieved attention failed for {rid}: {e}")

                    if att_maps_r.get("final_patch_map") is not None:
                        retrieved_att_maps.append(att_maps_r["final_patch_map"])

                    if main_target is not None and ig_maps_r.get(main_target) is not None:
                        try:
                            ig_map_r = ig_maps_r[main_target]
                            ig_b64_r = heatmap_to_base64_overlay(orig_arr_r, ig_map_r, alpha=0.45)
                        except Exception as e:
                            print(f"[WARN] render retrieved IG overlay {rid} failed: {e}")

                    if main_target is not None and gradcam_maps_r.get(main_target) is not None:
                        try:
                            gradcam_map_r = gradcam_maps_r[main_target]
                            gradcam_b64_r = heatmap_to_base64_overlay(orig_arr_r, gradcam_map_r, alpha=0.45)
                        except Exception as e:
                            print(f"[WARN] render retrieved Grad-CAM overlay {rid} failed: {e}")

                    # compute cross-image comparisons (query vs retrieved)-
                    cross_metrics = {}
                    try:
                        # --- Attention maps ---
                        for att_type in ["txt2img", "comb_img", "final_patch_map"]:
                            if att_maps_q.get(att_type) is not None and att_maps_r.get(att_type) is not None:
                                cm_5  = compare_maps(att_maps_q[att_type], att_maps_r[att_type], topk_frac=0.05)
                                cm_20 = compare_maps(att_maps_q[att_type], att_maps_r[att_type], topk_frac=0.20)

                                cross_metrics[f"att_{att_type}_pearson_top5pct"]  = round(cm_5.get("pearson", 0.0), 4)
                                cross_metrics[f"att_{att_type}_spearman_top5pct"] = round(cm_5.get("spearman", 0.0), 4)
                                cross_metrics[f"att_{att_type}_iou_top5pct"]      = round(cm_5.get("iou_top5pct", 0.0), 4)
                                cross_metrics[f"att_{att_type}_iou_top20pct"]     = round(cm_20.get("iou_top20pct", 0.0), 4)

                        # --- Integrated Gradients (per target) ---
                        if main_target is not None and main_target in ig_maps_q and ig_map_r is not None:
                            cm_ig_5  = compare_maps(ig_maps_q[main_target], ig_map_r, topk_frac=0.05)
                            cm_ig_20 = compare_maps(ig_maps_q[main_target], ig_map_r, topk_frac=0.20)
                            cross_metrics["ig_pearson_top5pct"]  = round(cm_ig_5.get("pearson", 0.0), 4)
                            cross_metrics["ig_spearman_top5pct"] = round(cm_ig_5.get("spearman", 0.0), 4)
                            cross_metrics["ig_iou_top5pct"]      = round(cm_ig_5.get("iou_top5pct", 0.0), 4)
                            cross_metrics["ig_iou_top20pct"]     = round(cm_ig_20.get("iou_top20pct", 0.0), 4)

                        # --- Grad-CAM (per target) ---
                        if main_target is not None and main_target in gradcam_maps_q and gradcam_map_r is not None:
                            cm_gc_5  = compare_maps(gradcam_maps_q[main_target], gradcam_map_r, topk_frac=0.05)
                            cm_gc_20 = compare_maps(gradcam_maps_q[main_target], gradcam_map_r, topk_frac=0.20)
                            cross_metrics["gradcam_pearson_top5pct"]  = round(cm_gc_5.get("pearson", 0.0), 4)
                            cross_metrics["gradcam_spearman_top5pct"] = round(cm_gc_5.get("spearman", 0.0), 4)
                            cross_metrics["gradcam_iou_top5pct"]      = round(cm_gc_5.get("iou_top5pct", 0.0), 4)
                            cross_metrics["gradcam_iou_top20pct"]     = round(cm_gc_20.get("iou_top20pct", 0.0), 4)

                    except Exception as e:
                        print(f"[WARN] cross-compare failed for {rid}: {e}")

                    except Exception as e:
                        print(f"[WARN] cross-compare failed for {rid}: {e}")

                    retrieval_detailed.append({
                        "id": rid,
                        "dist": dist,
                        "labels": label_names,
                        "report": report_text,
                        "image": img_b64,
                        "attn_txt2img_b64_r": attn_txt2img_b64_r,
                        "attn_comb_img_b64_r": attn_comb_img_b64_r,
                        "attn_final_img_b64_r": attn_final_img_b64_r,
                        "attn_img2txt_html_r": attn_img2txt_html_r,
                        "attn_comb_txt_html_r": attn_comb_txt_html_r,
                        "attn_final_txt_html_r": attn_final_txt_html_r,
                        "ig_b64": ig_b64_r,
                        "gradcam_b64": gradcam_b64_r,
                        "cross_metrics": cross_metrics
                    })

                except Exception as e:
                    print(f"[WARN] Retrieval detail failed for {rid}: {e}")

            if len(retrieved_att_maps) > 1:
                overlaps = []
                for i in range(len(retrieved_att_maps)):
                    for j in range(i + 1, len(retrieved_att_maps)):
                        cm = compare_maps(retrieved_att_maps[i], retrieved_att_maps[j], topk_frac=0.05)
                        overlaps.append(cm['iou_top5pct'])
                avg_overlap = np.mean(overlaps)
                avg_diversity = 1 - avg_overlap
                context['retrieval_diversity_score'] = round(avg_diversity, 4)
            else:
                context['retrieval_diversity_score'] = None

            # --- Compute overlap/diversity restricted to same-class retrieved items ---
            same_class_maps = []
            for item, amap in zip(retrieval_detailed, retrieved_att_maps):
                if main_target is not None and label_cols[main_target] in item["labels"]:
                    same_class_maps.append(amap)

            if len(same_class_maps) > 1:
                overlaps = []
                for i in range(len(same_class_maps)):
                    for j in range(i + 1, len(same_class_maps)):
                        cm = compare_maps(same_class_maps[i], same_class_maps[j], topk_frac=0.05)
                        overlaps.append(cm['iou_top5pct'])
                avg_overlap = np.mean(overlaps)
                avg_diversity = 1 - avg_overlap
                context['retrieval_same_class_overlap'] = round(avg_overlap, 4)
                context['retrieval_same_class_diversity'] = round(avg_diversity, 4)
            else:
                context['retrieval_same_class_overlap'] = None
                context['retrieval_same_class_diversity'] = None
                
        context["retrieval_detailed"] = retrieval_detailed

    # Debug prints
    print("Context keys:", list(context.keys()))
    print("Retrieval pairs:", context.get("retrieval"))

    return render_template("index.html", **context)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=True)

end src\web\app.py

src\web\__init__.py

end src\web\__init__.py

z_old_result\models\en_core_sci_sm\__init__.py
from pathlib import Path
from spacy.util import load_model_from_init_py, get_model_meta



__version__ = get_model_meta(Path(__file__).parent)['version']


def load(**overrides):
    return load_model_from_init_py(__file__, **overrides)

end z_old_result\models\en_core_sci_sm\__init__.py

